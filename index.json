[{"content":" Daily blog posts will be updated. 💻 ","date":null,"permalink":"/posts/","section":"Blog posts","summary":"Daily blog posts will be updated.","title":"Blog posts"},{"content":"","date":null,"permalink":"/tags/feature-engineering/","section":"Tags","summary":"","title":"feature engineering"},{"content":"","date":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"machine learning"},{"content":"This article explains what Feature Engineering is, why it is important, and describes basic Feature Engineering techniques.\nWhat is Feature Engineering? #Feature Engineering is the process of transforming given raw data into features (or variables) that allow a machine learning model to function effectively. This process includes removing unnecessary information, extracting and transforming useful information, and adjusting the data to improve model performance during training.\nThe Importance of Feature Engineering #Feature Engineering can significantly improve the performance of a machine learning model. Good features enable the model to learn patterns in the data better, thus increasing the accuracy of predictions. On the other hand, irrelevant or incorrect features can degrade model performance. Therefore, Feature Engineering is a crucial process for maximizing model performance.\nFeature Engineering Techniques #There are various techniques for Feature Engineering, and below are some of the most basic ones:\nMissing Value Handling: It is important to deal with missing values in the data. Methods include replacing missing values with the mean, median, mode, or removing rows with missing values. Categorical Data Processing: Many models cannot directly process categorical data. Methods such as One-Hot Encoding and Label Encoding can be used to convert categorical data into numerical data. Feature Scaling: Adjusting the scale of various features allows the model to evaluate features fairly. Standardization and Normalization are examples of this. Feature Selection: Important features are selected to reduce model complexity and prevent overfitting. Statistical methods and model-based methods are examples of this. Feature Creation: New features are created by combining or transforming existing features. This helps the model understand the data better. Encoding Conversion #Categorical data refers to data categories represented in text. Since most machine learning algorithms take numerical data as input, converting these categorical data into an appropriate numerical format is essential. Two primary methods are used for encoding conversion.\nOne-Hot Encoding: Converts each category into a separate column, assigning a value of 1 if the category is present and 0 otherwise. This method does not consider the order or importance of categories, allowing the model to treat each category equally. Label Encoding: Converts each category into a numerical value by assigning sequential numbers. For example, \u0026lsquo;red\u0026rsquo;, \u0026lsquo;blue\u0026rsquo;, \u0026lsquo;green\u0026rsquo; can be converted to 0, 1, 2, respectively. While Label Encoding does not increase the dimensionality as much as the number of categories, care must be taken as the magnitude of numbers can affect the model. Scaling #Feature Scaling is the process of standardizing the units or range of data to a uniform scale, ensuring all features equally influence the model. Two primary methods used for scaling are:\nStandardization: Adjusts the data to have a mean of 0 and a standard deviation of 1. This method is useful when the data distribution does not follow a normal distribution and is less sensitive to outliers. Normalization: Adjusts the data values to a range between 0 and 1. The most common method uses the minimum and maximum values, ensuring all data points have the same scale. ","date":"20 February 2024","permalink":"/posts/29/","section":"Blog posts","summary":"\u003cp\u003eThis article explains what Feature Engineering is, why it is important, and describes basic Feature Engineering techniques.\u003c/p\u003e","title":"Machine Learning - Feature Engineering"},{"content":"In this article, we explain the concept of Random Search, one of the hyperparameter tuning methods to maximize the performance of models in machine learning, and introduce an example of implementation using the Scikit-learn library.\nWhat is Random Search? #Random Search is a method that evaluates randomly selected combinations within a given parameter space to find the optimal combination of hyperparameters. Unlike Grid Search, which systematically explores all combinations of specified parameters, Random Search evaluates combinations randomly selected from the search space. This method is particularly useful when the dimension of hyperparameters is high or the search space is large, often yielding similar or better results in less time.\nKey Parameters #Random Search can be implemented through the RandomizedSearchCV class in Scikit-learn. The key parameters are as follows:\nestimator: Specifies the model to optimize. For example, RandomForestClassifier(), SVC(), etc. param_distributions: Specifies the parameter space to explore. You can specify a continuous distribution for each parameter or provide a list. n_iter: Specifies the number of parameter settings to be randomly selected. The larger this value, the more combinations will be explored, but computation time will also increase. scoring: Specifies the criterion for evaluating the model\u0026rsquo;s performance. For example, \u0026lsquo;accuracy\u0026rsquo;, \u0026lsquo;f1\u0026rsquo;, etc. cv: Specifies the strategy for cross-validation splitting. Entering an integer value will perform k-fold cross-validation with that value. random_state: Specifies the seed value for the random number generator to ensure reproducibility of the results. Implementing RandomizedSearchCV #Below is an example of using RandomizedSearchCV to find the optimal hyperparameters for a classifier. In the code below, we use a Support Vector Machine (SVM).\n\u0026gt;\u0026gt;\u0026gt; from sklearn.model_selection import RandomizedSearchCV \u0026gt;\u0026gt;\u0026gt; from sklearn.svm import SVC \u0026gt;\u0026gt;\u0026gt; from sklearn.datasets import load_iris \u0026gt;\u0026gt;\u0026gt; from sklearn.model_selection import train_test_split \u0026gt;\u0026gt;\u0026gt; import scipy.stats as stats # Load the dataset \u0026gt;\u0026gt;\u0026gt; iris = load_iris() \u0026gt;\u0026gt;\u0026gt; X, y = iris.data, iris.target # Split the dataset into training and testing sets \u0026gt;\u0026gt;\u0026gt; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Instantiate the Support Vector Machine \u0026gt;\u0026gt;\u0026gt; svc = SVC() # Define the hyperparameter space to explore \u0026gt;\u0026gt;\u0026gt; param_distributions = { \u0026#39;C\u0026#39;: stats.uniform(0.1, 1000), \u0026#39;gamma\u0026#39;: stats.uniform(0.0001, 0.1), \u0026#39;kernel\u0026#39;: [\u0026#39;linear\u0026#39;, \u0026#39;rbf\u0026#39;] } # Instantiate RandomizedSearchCV \u0026gt;\u0026gt;\u0026gt; random_search.fit(X_train, y_train) # Output the best parameters and highest accuracy \u0026gt;\u0026gt;\u0026gt; print(\u0026#34;Best parameters:\u0026#34;, random_search.best_params_) \u0026gt;\u0026gt;\u0026gt; print(\u0026#34;Best cross-validation score: {:.2f}\u0026#34;.format(random_search.best_score_)) # Evaluate performance on the test set \u0026gt;\u0026gt;\u0026gt; accuracy = random_search.score(X_test, y_test) \u0026gt;\u0026gt;\u0026gt; print(\u0026#34;Test set accuracy: {:.2f}\u0026#34;.format(accuracy)) This code searches for the optimal combination by exploring random combinations of the specified C, gamma, kernel hyperparameters for the SVC model. RandomizedSearchCV can be a more effective method than grid search, especially when the search space is wide.\n","date":"20 February 2024","permalink":"/posts/28/","section":"Blog posts","summary":"\u003cp\u003eIn this article, we explain the concept of Random Search, one of the hyperparameter tuning methods to maximize the performance of models in machine learning, and introduce an example of implementation using the Scikit-learn library.\u003c/p\u003e","title":"Machine Learning - Random Search"},{"content":"","date":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"python"},{"content":"","date":null,"permalink":"/tags/random-search/","section":"Tags","summary":"","title":"random search"},{"content":" ","date":null,"permalink":"/tags/","section":"Tags","summary":" ","title":"Tags"},{"content":"Welcome to my website! I\u0026rsquo;m really happy you stopped by.\n","date":null,"permalink":"/","section":"Welcome to Congo!","summary":"Welcome to my website!","title":"Welcome to Congo!"},{"content":"","date":null,"permalink":"/tags/grid-search/","section":"Tags","summary":"","title":"grid search"},{"content":"One of the efficient methods to find the optimal parameters of a model in machine learning is Grid Search. This article explains what grid search is, how it works, and when it should be used.\nWhat is Grid Search? #Grid search is one of the methods to optimize hyperparameters of a machine learning model. This method tests all combinations of specified hyperparameters to find the combination that produces the best performance. The performance of each parameter combination is evaluated through cross-validation, and through this process, the optimal model can be selected.\nHow It Works #Grid search first receives a range or list of hyperparameters specified by the user. For example, let\u0026rsquo;s assume we are conducting a grid search for a decision tree classifier. The user sets the range of hyperparameters such as the depth of the tree (depth), the minimum number of samples for splitting (min_samples_split), etc. Grid search trains the model on all possible combinations within this range and evaluates the performance of each combination using cross-validation. Common methods for performance evaluation include accuracy, precision, recall, F1 score, etc. After the evaluation, the parameter combination with the best performance is selected.\nAdvantages and Disadvantages #Advantages:\nEasy to use and understand Because it explores all possible combinations, there is a high possibility of finding the optimal combination Disadvantages:\nVery high computational cost. As the number of parameters and their range increases, the required amount of computation increases exponentially It takes a long time to find the optimal combination When to Use #Grid search is suitable when the range of parameters is relatively small and the model\u0026rsquo;s learning time is short. Also, it is good to use when finding the optimal combination of hyperparameters is important and there are sufficient computational resources. However, for models with a very large parameter space or very long learning time, it is advisable to consider other hyperparameter optimization techniques such as Random Search or Bayesian optimization.\nGridSearchCV #GridSearchCV is a class included in the model selection module of the scikit-learn (sklearn) library, used to search the hyperparameter space of a given model through cross-validation and find the optimal parameters. The main parameters that can be passed to this class\u0026rsquo;s constructor are as follows:\nKey Parameters # estimator: The model to optimize. For example, it could be scikit-learn\u0026rsquo;s estimator objects like RandomForestClassifier(), SVC(), etc. param_grid: A dictionary of parameters to search. For example, you could set {\u0026rsquo;n_estimators\u0026rsquo;: [100, 200], \u0026lsquo;max_features\u0026rsquo;: [\u0026lsquo;auto\u0026rsquo;, \u0026lsquo;sqrt\u0026rsquo;]}, which means to search the values [100, 200] for n_estimators and [\u0026lsquo;auto\u0026rsquo;, \u0026lsquo;sqrt\u0026rsquo;] for max_features parameters. scoring: The criterion for evaluating the model\u0026rsquo;s performance. It is specified as a string, such as \u0026lsquo;accuracy\u0026rsquo;, \u0026lsquo;f1\u0026rsquo;, etc. Other predefined scoring options in scikit-learn can also be used. cv: The strategy for cross-validation splitting. For example, 5 means 5-fold cross-validation. You can also directly pass objects of scikit-learn\u0026rsquo;s splitters like KFold, StratifiedKFold, etc. refit: Determines whether to retrain the model on the entire dataset after finding the optimal parameters. The default value is True, meaning the model is trained on the entire dataset with the optimal parameters. Implementing GridSearchCV #The following is a simple example of using Scikit-learn\u0026rsquo;s GridSearchCV to find the optimal hyperparameters for a classifier. Here, we use a decision tree classifier (DecisionTreeClassifier).\n\u0026gt;\u0026gt;\u0026gt; from sklearn.model_selection import GridSearchCV \u0026gt;\u0026gt;\u0026gt; from sklearn.tree import DecisionTreeClassifier \u0026gt;\u0026gt;\u0026gt; from sklearn.datasets import load_iris \u0026gt;\u0026gt;\u0026gt; from sklearn.model_selection import train_test_split # Load data \u0026gt;\u0026gt;\u0026gt; iris = load_iris() \u0026gt;\u0026gt;\u0026gt; X = iris.data \u0026gt;\u0026gt;\u0026gt; y = iris.target # Split training and testing set \u0026gt;\u0026gt;\u0026gt; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Set up the model \u0026gt;\u0026gt;\u0026gt; estimator = DecisionTreeClassifier() # Set the grid of parameters to search \u0026gt;\u0026gt;\u0026gt; param_grid = { \u0026#39;max_depth\u0026#39;: [None, 2, 4, 6, 8], \u0026#39;min_samples_split\u0026#39;: [2, 5, 10], \u0026#39;min_samples_leaf\u0026#39;: [1, 2, 4] } # Set up GridSearchCV \u0026gt;\u0026gt;\u0026gt; grid_search = GridSearchCV(estimator=estimator, param_grid=param_grid, scoring=\u0026#39;accuracy\u0026#39;, cv=5, refit=True) # Perform grid search \u0026gt;\u0026gt;\u0026gt; grid_search.fit(X_train, y_train) # Print the best parameters and highest score \u0026gt;\u0026gt;\u0026gt; print(\u0026#34;Best parameters:\u0026#34;, grid_search.best_params_) \u0026gt;\u0026gt;\u0026gt; print(\u0026#34;Best score:\u0026#34;, grid_search.best_score_) # Evaluate performance on test data \u0026gt;\u0026gt;\u0026gt; test_accuracy = grid_search.score(X_test, y_test) \u0026gt;\u0026gt;\u0026gt; print(\u0026#34;Test accuracy:\u0026#34;, test_accuracy) ","date":"19 February 2024","permalink":"/posts/27/","section":"Blog posts","summary":"\u003cp\u003eOne of the efficient methods to find the optimal parameters of a model in machine learning is Grid Search. This article explains what grid search is, how it works, and when it should be used.\u003c/p\u003e","title":"Machine Learning - Grid Search"},{"content":"","date":null,"permalink":"/tags/paired-t-test/","section":"Tags","summary":"","title":"Paired t-Test"},{"content":"","date":null,"permalink":"/tags/scipy/","section":"Tags","summary":"","title":"scipy"},{"content":"","date":null,"permalink":"/tags/statistics/","section":"Tags","summary":"","title":"statistics"},{"content":"This article was written to explain the Paired Sample t-Test used in statistics.\nWe will also proceed with the Paired Sample t-Test using the Python Scipy library.\nPaired Sample t-Test #The Paired Sample t-Test is a statistical technique for comparing the means of two related groups. This method is typically applied when there are two measurements for the same group of subjects. The paired sample t-test is used to determine whether the difference in means between two related groups is statistically significant.\n1. Hypothesis Setting # H₀ : 𝜇D = 0 → Null Hypothesis (𝜇𝐷 = 𝜇₁ - 𝜇₂) The difference in means before and after the experiment is 0. H₁ : 𝜇D ≠ 0 → Alternative Hypothesis The difference in means before and after the experiment is not 0. 2. Normality Test #If the sample size of the two groups is less than 30, a normality test must be conducted.\nIf the sample size of the two groups is 30 or more, it is assumed that normality is satisfied due to the Central Limit Theorem.\nIn Scipy, normality testing can be confirmed through the Shapiro-Wilk test. 4. Calculation of Paired Sample t-Statistic #The paired sample t-statistic is calculated using the means and standard deviations of the two groups.\n5. Decision/Conclusion #If the calculated t-statistic exceeds the critical value, the null hypothesis is rejected and the alternative hypothesis is accepted.\nOtherwise, the null hypothesis is not rejected.\nIf there is a statistically significant difference, it is concluded that there is a difference in means between the two groups.\nUsing Python Library Scipy #Below is how to proceed with the Paired Sample t-Test using the Python Scipy library.\nThe data we are dealing with includes, in student A\u0026rsquo;s class, there was a rumor that working out improves concentration, so A decided to compare before and after working out. A made 20 people work out, then had them take concentration measurement tests before and after the training.\nWe want to see if there is a significant difference in concentration before and after working out through a Paired Sample t-Test.\nThe hypothesis is as follows:\nNull Hypothesis : The test averages before and after working out are the same.\nAlternative Hypothesis : The test averages before and after working out are not the same.\nThe significance level is set at 0.05.\nFirst, let\u0026rsquo;s load the data.\n\u0026gt;\u0026gt;\u0026gt; import pandas as pd \u0026gt;\u0026gt;\u0026gt; from scipy import stats \u0026gt;\u0026gt;\u0026gt; df = pd.read_csv(\u0026#34;./data/ch11_training_rel.csv\u0026#34;) \u0026gt;\u0026gt;\u0026gt; df.head() 전 후 0 59 41 1 52 63 2 55 68 3 61 59 4 59 84 Next, let\u0026rsquo;s conduct a normality test.\n\u0026gt;\u0026gt;\u0026gt; a = stats.shapiro(df[\u0026#39;Before\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; b = stats.shapiro(df[\u0026#39;After\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; print(a, b) ShapiroResult(statistic=0.9670045375823975, pvalue=0.690794825553894) ShapiroResult(statistic=0.9786625504493713, pvalue=0.9156817197799683) Both results have a p-value greater than 0.05, indicating normality is satisfied.\nNext, we can calculate the t-statistic and p-value using ttest_rel in the Scipy library.\n\u0026gt;\u0026gt;\u0026gt; t_score, p_value = stats.ttest_rel(df[\u0026#39;Before\u0026#39;], df[\u0026#39;After\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; print(round(t_score, 4), round(p_value, 2)) -2.2042 0.04 Since the p-value is less than the significance level of 0.05, the null hypothesis (the averages before and after working out are the same) is rejected. Therefore, we can conclude that there is a significant difference in the average scores before and after working out.\n","date":"19 January 2024","permalink":"/posts/10/","section":"Blog posts","summary":"\u003cp\u003eThis article was written to explain the Paired Sample t-Test used in statistics.\u003c/p\u003e","title":"Statistics - Paired t-Test"},{"content":"","date":null,"permalink":"/tags/ttest_rel/","section":"Tags","summary":"","title":"ttest_rel"},{"content":"","date":null,"permalink":"/tags/independent-samples-t-test/","section":"Tags","summary":"","title":"Independent Samples t-Test"},{"content":"This article was written to explain the Independent Samples t-Test used in statistics.\nWe will also proceed with the Independent Samples t-Test using the Python Scipy library.\nIndependent Samples t-Test #The Independent Samples t-Test is a statistical analysis technique used to test whether there is a statistically significant difference between the means of two independent samples. It is used to determine whether the difference in means between two groups occurred by chance or if it truly exists.\nThe main steps of the Independent Samples t-Test are as follows:\n1. Hypothesis Setting # H₀ : 𝜇₁ = 𝜇₂ → Null Hypothesis The means of the two groups are the same. H₁ : 𝜇₁ ≠ 𝜇₂ → Alternative Hypothesis The means of the two groups are different. 2. Normality Test #If the sample size of the two groups is less than 30, a normality test must be conducted.\nIf the sample size of the two groups is more than 30, it is assumed that normality is satisfied due to the Central Limit Theorem.\nIn Scipy, normality testing can be confirmed through the Shapiro-Wilk test. 3. Equality of Variances Test #If the data counts of the two groups are the same, it is assumed that the variances are equal.\nIf the data counts of the two groups are different, an equality of variances test can be performed to check if the variances are equal.\nIn Scipy, equality of variances testing can be confirmed through the Levene test. 4. Calculation of Independent Samples t-Statistic #The independent samples t-statistic is calculated using the means and standard deviations of the two groups.\n5. Decision/Conclusion #If the calculated t-statistic exceeds the critical value, the null hypothesis is rejected and the alternative hypothesis is accepted.\nOtherwise, the null hypothesis is not rejected.\nIf there is a statistically significant difference, it is concluded that there is a difference in means between the two groups.\nThe Independent Samples t-Test is useful for comparing the mean difference between two groups and can be applied in situations such as investigating differences between experimental and control groups or checking the effect between two conditions.\nUsing Python Library Scipy #Below is how to proceed with the Independent Samples t-Test using the Python Scipy library.\nThe data we are dealing with includes results from concentration tests received by class B, thinking that if strength training indeed has an effect on improving concentration, there might not be a difference in the average concentration test scores between his class with many humanities students, class A, and class B with many students who regularly do strength training.\nWe want to see if there is a significant difference in concentration between classes A and B through an Independent Samples t-Test.\nThe hypothesis is as follows:\nNull Hypothesis : The means of classes A and B are the same.\nAlternative Hypothesis : The means of classes A and B are not the same.\nThe significance level is set at 0.05.\nLet\u0026rsquo;s first load the data.\n\u0026gt;\u0026gt;\u0026gt; import pandas as pd \u0026gt;\u0026gt;\u0026gt; from scipy import stats \u0026gt;\u0026gt;\u0026gt; df = pd.read_csv(\u0026#34;./data/ch11_training_ind.csv\u0026#34;) \u0026gt;\u0026gt;\u0026gt; df.head() A B 0 47 49 1 50 52 2 37 54 3 60 48 4 39 51 Next, let\u0026rsquo;s conduct a normality test.\n\u0026gt;\u0026gt;\u0026gt; a = stats.shapiro(df[\u0026#39;A\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; b = stats.shapiro(df[\u0026#39;B\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; print(a, b) ShapiroResult(statistic=0.9685943722724915, pvalue=0.7249553203582764) ShapiroResult(statistic=0.9730021357536316, pvalue=0.8165789842605591) Both results have a p-value greater than 0.05, satisfying normality.\nNext, since the data counts are the same for these data, we assume equality of variances, but if the groups\u0026rsquo; data counts differ, equality of variances needs to be tested, which can be confirmed through the Levene test as follows:\n\u0026gt;\u0026gt;\u0026gt; stats.levene(df[\u0026#39;A\u0026#39;], df[\u0026#39;B\u0026#39;]) LeveneResult(statistic=2.061573118077718, pvalue=0.15923550057222613) With a p-value of 0.159, the null hypothesis (the two groups\u0026rsquo; variances are not different) is accepted.\nNext, the t-statistic and p-value can be calculated using ttest_ind in the Scipy library.\n\u0026gt;\u0026gt;\u0026gt; t, p = stats.ttest_ind(df[\u0026#39;A\u0026#39;], df[\u0026#39;B\u0026#39;], equal_var=False) # equal_var=False: Welch\u0026#39;s method \u0026gt;\u0026gt;\u0026gt; t, p (-1.760815724652471, 0.08695731107259362) Since the p-value is greater than the significance level of 0.05, the null hypothesis (the means of classes A and B are the same) is accepted. Therefore, it can be concluded that there is no significant difference in average scores between class A and class B.\n","date":"15 January 2024","permalink":"/posts/9/","section":"Blog posts","summary":"\u003cp\u003eThis article was written to explain the Independent Samples t-Test used in statistics.\u003c/p\u003e","title":"Statistics - Independent Samples t-Test"},{"content":"","date":null,"permalink":"/tags/ttest_ind/","section":"Tags","summary":"","title":"ttest_ind"},{"content":"","date":null,"permalink":"/tags/one-sample-t-test/","section":"Tags","summary":"","title":"one sample t-test"},{"content":"This article was written to explain the one-sample t-test used in statistics.\nFurthermore, we will proceed with the one-sample t-test using the Python Scipy library.\nOne Sample t-Test #The one-sample t-test is one of the hypothesis testing methods used in statistical analysis, utilized to test the mean of a single sample. It is commonly applied to check if the population mean is equal to a specific value.\nThe one-sample t-test consists of the following steps:\n1. Hypothesis Setting # H₀ : 𝜇 = 𝜇₀ → Null Hypothesis The population mean is equal to the sample mean. H₁ : 𝜇 ≠ 𝜇₀ → Alternative Hypothesis The population mean is not equal to the sample mean. 2. Sampling #Extract a sample from the population and calculate the mean of that sample.\n3. Calculation of the Test Statistic #Calculate the t-statistic, which represents the difference between the sample mean and the expected mean based on the hypothesis.\n4. Decision / Conclusion #If the calculated t-statistic falls within the rejection region, reject the null hypothesis and accept the alternative hypothesis.\nOtherwise, do not reject the null hypothesis.\nIn the case of a two-tailed test, the rejection regions are the symmetric ends of the t-distribution. If the null hypothesis is rejected, it is concluded that the sample is different from the population.\nConversely, if it is not rejected, it is concluded to be not statistically significant.\nUsing Python Library Scipy #Next, we will proceed with the one-sample t-test using the Python Scipy library.\nThe data we are dealing with here contains the circumference, height, and volume of 31 trees.\nWe want to see if the mean of this sample is consistent with the population mean through a one-sample t-test. The hypothesis is as follows:\nThe significance level is set at 0.05.\nHypothesis Testing\nNull Hypothesis : The mean is 75.\nAlternative Hypothesis : The mean is not 75.\nLet\u0026rsquo;s first load the data.\n\u0026gt;\u0026gt;\u0026gt; import pandas as pd \u0026gt;\u0026gt;\u0026gt; df = pd.read_csv(\u0026#34;./data/trees.csv\u0026#34;) \u0026gt;\u0026gt;\u0026gt; df.head() Girth Height Volume 0 8.3 70 10.3 1 8.6 65 10.3 2 8.8 63 10.2 3 10.5 72 16.4 4 10.7 81 18.8 Also, let\u0026rsquo;s calculate the mean of \u0026lsquo;Height\u0026rsquo;.\n\u0026gt;\u0026gt;\u0026gt; result = df[\u0026#39;Height\u0026#39;].mean() \u0026gt;\u0026gt;\u0026gt; round(result, 2) # (Rounded to the second decimal place) 76.0 Next, we will load the Scipy library for a one-sample t-test.\n\u0026gt;\u0026gt;\u0026gt; from scipy import stats The one-sample t-test uses ttest_1samp in Scipy.\nReference : https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_1samp.html\nThen, let\u0026rsquo;s calculate the test statistic for hypothesis testing.\n\u0026gt;\u0026gt;\u0026gt; from math import sqrt \u0026gt;\u0026gt;\u0026gt; t_score, p_value = stats.ttest_1samp(df[\u0026#39;Height\u0026#39;], popmean=75) \u0026gt;\u0026gt;\u0026gt; print(round(t_score, 2), round(p_value, 2)) 0.87 0.39 popmean is the same as the mean expected in the null hypothesis.\nAfter calculating the p-value of the above statistics (rounded to the fourth decimal place), let\u0026rsquo;s check whether to reject or not reject the null hypothesis under the significance level of 0.05.\n\u0026gt;\u0026gt;\u0026gt; print(round(p_value, 4)) 0.3892 \u0026gt;\u0026gt;\u0026gt; if p_value \u0026gt;= 0.05: print(\u0026#34;Accept\u0026#34;) else: print(\u0026#34;Reject\u0026#34;) Accept Therefore, we did not reject the null hypothesis that the sample mean is equal to the population mean.\n","date":"13 January 2024","permalink":"/posts/8/","section":"Blog posts","summary":"\u003cp\u003eThis article was written to explain the one-sample t-test used in statistics.\u003c/p\u003e","title":"Statistics - One Sample t-Test"},{"content":"","date":null,"permalink":"/tags/regression-analysis/","section":"Tags","summary":"","title":"regression analysis"},{"content":"This article was written to explain regression analysis in statistics.\nWhat is Regression Analysis? #Regression analysis is a statistical technique that estimates the effect of one or more independent variables on a dependent variable.\nVariables in Regression Analysis #The Dependent Variable (y) is the variable affected, also known as the Response Variable or Outcome Variable. It is the variable predicted in the model, influenced by other variables.\nThe Independent Variable (x) influences the outcome, also referred to as the Explanatory Variable or Predictor Variable. Independent variables affect the dependent variable and are used when constructing the prediction model.\nRegression Analysis Methods Based on the Number of Variables #The approach to regression analysis varies based on the number of variables.\nIf there is one independent variable, it is approached with simple linear regression. If there are two or more independent variables, multiple linear regression analysis is possible.\n1. Simple Linear Regression #This statistical technique estimates the effect of a single independent variable on a dependent variable. The regression line chosen is the one where the difference (residual) between predicted values and actual data is smallest. Among numerous lines, the regression line means the one with a smaller Residual Sum of Squares (RSS).\nThis is achieved through the Ordinary Least Squares method.\nOrdinary Least Squares : A method that creates a sum of squares based on measured values and finds the value that minimizes it, choosing the line with the smallest residual sum of squares.\n2. Multiple Linear Regression #This statistical technique estimates the effect of two or more independent variables on one dependent variable. It\u0026rsquo;s important to determine the significance of regression coefficients in multiple linear regression analysis because the model needs to be confirmed with the combination of selected variables where all regression coefficients are statistically verified.\nThe significance of regression coefficients can be confirmed through the regression coefficient t-statistics.\nMulticollinearity in Multiple Linear Regression #Multicollinearity refers to the phenomenon in regression analysis where there is a strong correlation between independent variables, occurring when one independent variable can be predicted from the others.\nMulticollinearity complicates the accurate estimation of each independent variable\u0026rsquo;s regression coefficient. Furthermore, it prevents the regression coefficients of each independent variable from correctly explaining their impact on the dependent variable.\nMethods to Test for Multicollinearity\nVariance Inflation Factor (VIF):\nThe Variance Inflation Factor indicates how much the variance of each independent variable has increased, judging that multicollinearity has risen if this value is large. It\u0026rsquo;s calculated as the variance ratio of linearly regressing each independent variable against the others. Generally, if the VIF is greater than 4, it\u0026rsquo;s judged that multicollinearity exists, and if greater than 10, it\u0026rsquo;s considered to be a serious problem.\nConsiderations in Regression Analysis #When conducting regression analysis, there are three main points to consider:\n1. Are the regression coefficients significant? #A regression coefficient is considered statistically significant if the p-value of its t-statistic is less than 0.05.This means the coefficient has a significant impact on the dependent variable.\n2. How much explanatory power does the model have? #To check how much explanatory power the model has, the Coefficient of Determination (𝑅²) must be reviewed.\nCoefficient of Determination (𝑅²) #The coefficient of determination is a value between 0 and 1, meaning the model explains the variation in the dependent variable well if it is closer to 1. A high coefficient of determination indicates a high predictive power of the model..\n3. Does the model fit the data well? #To determine if the model fits the data well, residuals are plotted, and regression diagnostics are performed. Residuals, the difference between actual values and the model\u0026rsquo;s predicted values, are visually reviewed to check how well the model fits the data. Ideally, residuals follow a normal distribution without any specific patterns or trends, and homoscedasticity of residuals must also be confirmed. Outliers or influential data points should be reviewed, and if necessary, removed or adjusted to check the model\u0026rsquo;s stability.\nThrough these reviews, the reliability of the regression analysis and the model\u0026rsquo;s fit can be assessed.\n","date":"12 January 2024","permalink":"/posts/7/","section":"Blog posts","summary":"\u003cp\u003eThis article was written to explain regression analysis in statistics.\u003c/p\u003e","title":"Statistics - Regression Analysis"},{"content":"","date":null,"permalink":"/tags/alternative-hypothesis/","section":"Tags","summary":"","title":"alternative hypothesis"},{"content":"","date":null,"permalink":"/tags/hypothesis-testing/","section":"Tags","summary":"","title":"hypothesis testing"},{"content":"","date":null,"permalink":"/tags/null-hypothesis/","section":"Tags","summary":"","title":"null hypothesis"},{"content":"This article was written to explain methods for integrating multiple datasets into one using the pandas library.\nIn statistics, a Hypothesis is a proposition that represents a claim or estimation and signifies an assumption/tentative conclusion about parameters.\nTypes of Hypotheses #Hypotheses can be represented in two forms as follows:\n1. Null Hypothesis (H0) #The null hypothesis represents the hypothesis that there is no change or difference compared to the original, serving as a kind of \u0026lsquo;default\u0026rsquo; hypothesis.\nThe content of the null hypothesis varies depending on the test method. For example, a claim such as \u0026ldquo;the means of two groups are equal\u0026rdquo; can be set as a null hypothesis.\n2. Alternative Hypothesis (H1) #The alternative hypothesis is a claim opposing the null hypothesis, representing a hypothesis that one seeks to prove with concrete evidence through samples. For example, a claim such as \u0026ldquo;the means of two groups are different\u0026rdquo; can be set as an alternative hypothesis.\nThrough statistical testing, it is decided whether to reject the null hypothesis using the given data, or if there is no basis for rejection, the null hypothesis is not rejected. If clear evidence that the alternative hypothesis is true is found in the test results, the null hypothesis is rejected.\nThe process of verifying the validity of such hypotheses in statistics is precisely hypothesis testing.\nHypothesis Testing #1. Setting the Hypothesis #The first step of hypothesis testing is setting the null hypothesis (H0) and the alternative hypothesis (H1) according to the problem being investigated.\n2. Sample Analysis #Next, a sample that can represent a part of the entire population is extracted. Data is collected and analyzed for this sample, thereby securing materials for statistical analysis.\n3. Testing the Validity of the Hypothesis #The hypothesis is tested using the collected data. It is decided whether to reject the null hypothesis or accept it because there is no basis for rejection, considering the level of significance and the test statistic.\nLevel of Significance\nThe level of significance is usually denoted by α(alpha) and represents the criterion probability for rejecting the null hypothesis in experiments or surveys.\nThe commonly used level of significance is 0.05 (5%), but other values such as 0.01 or 0.10 can be used depending on the nature of the experiment or characteristics of the research.\nTest Statistic\nThe test statistic is an indicator that measures how well the collected data matches the hypothesis, a sample statistic necessary for parameter inference. The test statistic plays a crucial role in hypothesis testing and is used to decide on the rejection of the null hypothesis.\nIn the process of testing a hypothesis, there is always a possibility of statistical error, referred to as hypothesis testing error.\nHypothesis Testing Error #1. Type I Error #A Type I Error refers to the error of rejecting the null hypothesis when it is true. The cause of a Type I error is setting the significance level in statistical testing, which accidentally occurs when rejecting the null hypothesis at this level.\nExample: Incorrectly concluding there is an effect when there is actually none\n2. Type II Error #A Type II Error refers to the error of accepting the null hypothesis when the alternative hypothesis is true. The cause of a Type II error is insufficient test power, occurring when the actual effect is not detected due to a lack of power.\nExample: Failing to find the effect in statistical testing and adopting the null hypothesis even though there is actually an effect\n","date":"11 January 2024","permalink":"/posts/6/","section":"Blog posts","summary":"\u003cp\u003eThis article was written to explain methods for integrating multiple datasets into one using the pandas library.\u003c/p\u003e","title":"Statistics - Hypothesis Testing (1)"},{"content":"","date":null,"permalink":"/tags/concat/","section":"Tags","summary":"","title":"concat"},{"content":"","date":null,"permalink":"/tags/join/","section":"Tags","summary":"","title":"join"},{"content":"","date":null,"permalink":"/tags/merge/","section":"Tags","summary":"","title":"merge"},{"content":"","date":null,"permalink":"/tags/pandas/","section":"Tags","summary":"","title":"pandas"},{"content":"This article was written to explain methods for integrating multiple datasets into one using the pandas library.\nThere are several methods for integrating data, but this time we will cover concat, join, merge.\nBefore we explain, let\u0026rsquo;s create two example data frames.\n\u0026gt;\u0026gt;\u0026gt; import pandas as pd \u0026gt;\u0026gt;\u0026gt; df1 = pd.DataFrame({ \u0026#39;Class1\u0026#39; : [95, 92, 98, 100], \u0026#39;Class2\u0026#39; : [91, 93, 97, 99] }) \u0026gt;\u0026gt;\u0026gt; df2 = pd.DataFrame({ \u0026#39;Class1\u0026#39; : [87, 89], \u0026#39;Class2\u0026#39; : [85, 90] }) Output of d1:\nClass1 Class2 0 87 85 1 89 90 Output of d2:\nClass1 Class2 0 95 91 1 92 93 2 98 97 3 100 99 1. concat #The concat function of the pandas library is used to append data frames. This function can append multiple data frames in either row or column direction.\nLet\u0026rsquo;s append df1 and df2 to the result.\n\u0026gt;\u0026gt;\u0026gt; result = pd.concat([df1, df2]) \u0026gt;\u0026gt;\u0026gt; result Class1 Class2 0 95 91.0 1 92 93.0 2 98 97.0 3 100 99.0 4 87 85.0 5 89 90.0 6 96 NaN 7 83 NaN pd.concat([df1, df2]) appends df1 and df2 in the row direction. That is, the two data frames are connected vertically.\nNext, let\u0026rsquo;s integrate d3, which only has the Class1 column, into the result.\n\u0026gt;\u0026gt;\u0026gt; df3 = pd.DataFrame({ \u0026#39;Class1\u0026#39; : [96, 83] }) \u0026gt;\u0026gt;\u0026gt; pd.concat([result, df3], ignore_index=True) Class1 Class2 0 95 91.0 1 92 93.0 2 98 97.0 3 100 99.0 4 87 85.0 5 89 90.0 6 96 NaN 7 83 NaN Since the d3 data does not have the \u0026lsquo;Class2\u0026rsquo; column, it outputs blank values.\n2. join #The join method of the pandas library is used to combine two data frames based on a specific column. It generally performs a role similar to SQL\u0026rsquo;s JOIN operation. Unlike concat, join integrates horizontally.\n\u0026gt;\u0026gt;\u0026gt; df4 = pd.DataFrame({ \u0026#39;Class3\u0026#39; : [93, 91, 95, 98] }) \u0026gt;\u0026gt;\u0026gt; df1.join(df4) Class1 Class2 Class3 a 95 91 93 b 92 93 91 c 98 97 95 d 100 99 98 It is also possible to output by arbitrarily setting the index as follows.\n\u0026gt;\u0026gt;\u0026gt; index_label = [\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;d\u0026#39;] \u0026gt;\u0026gt;\u0026gt; df1a = pd.DataFrame({\u0026#39;Class1\u0026#39;: [95, 92, 98, 100], \u0026#39;Class2\u0026#39;: [91, 93, 97, 99]}, index= index_label) \u0026gt;\u0026gt;\u0026gt; df4a = pd.DataFrame({\u0026#39;Class3\u0026#39;: [93, 91, 95, 98]}, index=index_label) \u0026gt;\u0026gt;\u0026gt; df1a.join(df4a) Class1 Class2 Class3 a 95 91 93 b 92 93 91 c 98 97 95 d 100 99 98 3. merge #The merge function of the pandas library is used to merge (integrate) two data frames based on a specific column. Using the merge function, you can combine data frames based on common columns between them.\n\u0026gt;\u0026gt;\u0026gt; df_A_B = pd.DataFrame({\u0026#39;Sales Month\u0026#39;: [\u0026#39;January\u0026#39;, \u0026#39;February\u0026#39;, \u0026#39;March\u0026#39;, \u0026#39;April\u0026#39;], \u0026#39;Product A\u0026#39;: [100, 150, 200, 130], \u0026#39;Product B\u0026#39;: [90, 110, 140, 170]}) \u0026gt;\u0026gt;\u0026gt; df_C_D = pd.DataFrame({\u0026#39;Sales Month\u0026#39;: [\u0026#39;January\u0026#39;, \u0026#39;February\u0026#39;, \u0026#39;March\u0026#39;, \u0026#39;April\u0026#39;], \u0026#39;Product C\u0026#39;: [112, 141, 203, 134], \u0026#39;Product D\u0026#39;: [90, 110, 140, 170]}) df_A_B\nSales Month Product A Product B 0 January 100 90 1 February 150 110 2 March 200 140 3 April 130 170 df_C_D\nSales Month Product C Product D 0 January 112 90 1 February 141 110 2 March 203 140 3 April 134 170 Use merge to merge the two data frames based on the \u0026lsquo;Sales Month\u0026rsquo; column. As a result, the two data frames are combined based on the \u0026lsquo;Sales Month\u0026rsquo; column, and the data is organized around the common columns.\n\u0026gt;\u0026gt;\u0026gt; df_A_B.merge(df_C_D) Sales Month Product A Product B Product C Product D 0 January 100 90 112 90 1 February 150 110 141 110 2 March 200 140 203 140 3 April 130 170 134 170 Let\u0026rsquo;s implement four different ways to combine two data frames using the merge method.\n\u0026gt;\u0026gt;\u0026gt; df_left = pd.DataFrame({\u0026#39;key\u0026#39;:[\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;C\u0026#39;], \u0026#39;left\u0026#39;: [1, 2, 3]}) \u0026gt;\u0026gt;\u0026gt; df_right = pd.DataFrame({\u0026#39;key\u0026#39;:[\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;D\u0026#39;], \u0026#39;right\u0026#39;: [4, 5, 6]}) 1.\n\u0026gt;\u0026gt;\u0026gt; df_left.merge(df_right, how=\u0026#39;left\u0026#39;, on = \u0026#39;key\u0026#39;) key left right 0 A 1 4.0 1 B 2 5.0 2 C 3 NaN Left join df_left and df_right based on the \u0026lsquo;key\u0026rsquo; column. A left join keeps all rows from the left data frame (df_left) and adds rows from the right data frame (df_right) that have a matching key value. If a matching key value is not present in the right data frame, it is filled with NaN.\n2.\n\u0026gt;\u0026gt;\u0026gt; df_left.merge(df_right, how=\u0026#39;right\u0026#39;, on = \u0026#39;key\u0026#39;) key left right 0 A 1.0 4 1 B 2.0 5 2 D NaN 6 Right join df_left and df_right based on the \u0026lsquo;key\u0026rsquo; column. A right join keeps all rows from the right data frame (df_right) and adds rows from the left data frame (df_left) that have a matching key value. If a matching key value is not present in the left data frame, it is filled with NaN.\n3.\n\u0026gt;\u0026gt;\u0026gt; df_left.merge(df_right, how=\u0026#39;outer\u0026#39;, on = \u0026#39;key\u0026#39;) key left right 0 A 1.0 4.0 1 B 2.0 5.0 2 D 3.0 NaN 3 D NaN 6.0 Outer join df_left and df_right based on the \u0026lsquo;key\u0026rsquo; column. An outer join includes all rows from both data frames, filling with NaN where a match is only present in one of the data frames.\n4.\n\u0026gt;\u0026gt;\u0026gt; df_left.merge(df_right, how=\u0026#39;inner\u0026#39;, on = \u0026#39;key\u0026#39;) key left right 0 A 1 4 1 B 2 5 Inner join df_left and df_right based on the \u0026lsquo;key\u0026rsquo; column. An inner join includes only rows that are common to both data frames. That is, it combines rows from both data frames that have the same \u0026lsquo;key\u0026rsquo; value.\n","date":"8 January 2024","permalink":"/posts/5/","section":"Blog posts","summary":"\u003cp\u003eThis article was written to explain methods for integrating multiple datasets into one using the pandas library.\u003c/p\u003e","title":"Python - pandas Data Integration (concat, join, merge)"},{"content":"","date":null,"permalink":"/tags/matplotlib/","section":"Tags","summary":"","title":"matplotlib"},{"content":"This article was written to explain how to insert text into graphs using the matplotlib library in Python.\nWhen outputting graphs using matplotlib, let\u0026rsquo;s try inserting text onto the graph as shown below. First, let\u0026rsquo;s implement it using arbitrary monthly sales data as shown below.\n\u0026gt;\u0026gt;\u0026gt; import calendar \u0026gt;\u0026gt;\u0026gt; month_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] \u0026gt;\u0026gt;\u0026gt; sold_list = [300, 400, 550, 900, 600, 960, 900, 910, 800, 700, 550, 450] \u0026gt;\u0026gt;\u0026gt; fig, ax = plt.subplots() \u0026gt;\u0026gt;\u0026gt; barcharts = ax.bar(month_list, sold_list) # calendar.month_name[1:13] → Outputs January to December on xlabel \u0026gt;\u0026gt;\u0026gt; ax.set_xticks(month_list, calendar.month_name[1:13], rotation=90) \u0026gt;\u0026gt;\u0026gt; print(barcharts) When you run the code, a graph like the one shown below is output.\nNext, let\u0026rsquo;s insert the corresponding y-value above each bar.\nAfter obtaining the value for each bar and to insert it as text, we use get_height() to output the y-value and ax.text() to input text into the bar.\nReference:\nget_height()\nhttps://matplotlib.org/stable/api/_as_gen/matplotlib.patches.Rectangle.html\nax.text()\nhttps://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.text.html#matplotlib.axes.Axes.text\nThe completed code is as follows:\n\u0026gt;\u0026gt;\u0026gt; import calendar \u0026gt;\u0026gt;\u0026gt; month_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] \u0026gt;\u0026gt;\u0026gt; sold_list = [300, 400, 550, 900, 600, 960, 900, 910, 800, 700, 550, 450] \u0026gt;\u0026gt;\u0026gt; fig, ax = plt.subplots() \u0026gt;\u0026gt;\u0026gt; barcharts = ax.bar(month_list, sold_list) \u0026gt;\u0026gt;\u0026gt; ax.set_xticks(month_list, calendar.month_name[1:13], rotation=90) \u0026gt;\u0026gt;\u0026gt; print(barcharts) \u0026gt;\u0026gt;\u0026gt; for rect in barcharts: height = rect.get_height() ax.text(rect.get_x() + rect.get_width()/2., 1.002*height,\u0026#39;%d\u0026#39; % int(height), ha=\u0026#39;center\u0026#39;, va=\u0026#39;bottom\u0026#39;) \u0026gt;\u0026gt;\u0026gt; plt.show() rect.get_x() + rect.get_width()/2.\nCalculates the midpoint of the x position and width of each bar. This represents the center along the horizontal axis of the bar.\n1.002 * height\nheight is the current height of the bar, and 1.002*height is a correction value to place the text slightly above the height of the bar.\n\u0026rsquo;% d\u0026rsquo; % int(height)\nString formatting, i.e., inserting the height value for each bar (% follows d(integer) for insertion, int(height) converts the height of the bar to an integer)\nha=\u0026lsquo;center\u0026rsquo;\nAligns horizontally (along the x-axis) to the center.\nva=\u0026lsquo;bottom\u0026rsquo;\nAligns vertically (along the y-axis) to the bottom.\nTherefore, when you run the code, it will be output as shown below. ","date":"5 January 2024","permalink":"/posts/4/","section":"Blog posts","summary":"\u003cp\u003eThis article was written to explain how to insert text into graphs using the matplotlib library in Python.\u003c/p\u003e","title":"Python - Matplotlib Text Insertion"},{"content":"","date":null,"permalink":"/tags/date_range/","section":"Tags","summary":"","title":"date_range"},{"content":"","date":null,"permalink":"/tags/iloc/","section":"Tags","summary":"","title":"iloc"},{"content":"","date":null,"permalink":"/tags/loc/","section":"Tags","summary":"","title":"loc"},{"content":"This article was written to explain the date_range() function within the pandas library that can automatically generate dates.\nInstead of manually entering dates in the index of data, it is convenient to use pandas\u0026rsquo; date_range() when there are many values.\ndate_range() can be used as follows:\n\u0026gt;\u0026gt;\u0026gt; pd.date_range(start=\u0026#39;date\u0026#39;, end=\u0026#39;date\u0026#39;, freq=\u0026#39;frequency\u0026#39;) Let\u0026rsquo;s take a look at the following example.\n\u0026gt;\u0026gt;\u0026gt; pd.date_range(start=\u0026#39;2024/01/01\u0026#39;, end=\u0026#39;2024/01/07\u0026#39;) DatetimeIndex([\u0026#39;2024-01-01\u0026#39;, \u0026#39;2024-01-02\u0026#39;, \u0026#39;2024-01-03\u0026#39;, \u0026#39;2024-01-04\u0026#39;, \u0026#39;2024-01-05\u0026#39;, \u0026#39;2024-01-06\u0026#39;, \u0026#39;2024-01-07\u0026#39;], dtype=\u0026#39;datetime64[ns]\u0026#39;, freq=\u0026#39;D\u0026#39;) As seen in the output, you can confirm that it has output from the start date \u0026lsquo;2024/01/01\u0026rsquo; to the end date \u0026lsquo;2024/01/07\u0026rsquo;.\nLet\u0026rsquo;s look at another example.\n\u0026gt;\u0026gt;\u0026gt; pd.date_range(start=\u0026#39;2024-01-01 08:00\u0026#39;, periods = 4, freq = \u0026#39;H\u0026#39;) DatetimeIndex([\u0026#39;2024-01-01 08:00:00\u0026#39;, \u0026#39;2024-01-01 09:00:00\u0026#39;, \u0026#39;2024-01-01 10:00:00\u0026#39;, \u0026#39;2024-01-01 11:00:00\u0026#39;], dtype=\u0026#39;datetime64[ns]\u0026#39;, freq=\u0026#39;H\u0026#39;) From the result, you can see that starting from 08:00 on \u0026lsquo;2024-01-01\u0026rsquo;, four results are produced based on the frequency \u0026lsquo;H\u0026rsquo; (hourly).\nWhen setting the freq (frequency), various outputs are possible by referring to the Offset aliases in the link below.\nReference: https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases\n","date":"4 January 2024","permalink":"/posts/2/","section":"Blog posts","summary":"\u003cp\u003eThis article was written to explain the date_range() function within the pandas library that can automatically generate dates.\u003c/p\u003e","title":"Python - pandas Automatic Date Generation with date_range"},{"content":"This article was written to explain the features and differences between .loc() and .iloc(), which are necessary when handling DataFrames using the pandas library in Python.\nFirst, for the explanation, we will use seaborn to fetch example data (iris).\n\u0026gt;\u0026gt;\u0026gt; import seaborn as sns \u0026gt;\u0026gt;\u0026gt; iris = sns.load_dataset(\u0026#39;iris\u0026#39;) \u0026gt;\u0026gt;\u0026gt; iris.head() First 5 rows of iris data 1. loc #loc is a method that selects data based on labels. It accesses data using the names (Labels) of rows and columns. In other words, it explicitly specifies the names of rows and columns to select data.\n# Select values with \u0026#39;virginica\u0026#39; in the column named \u0026#39;species\u0026#39; \u0026gt;\u0026gt;\u0026gt; iris.loc[iris[\u0026#39;species\u0026#39;] == \u0026#39;virginica\u0026#39;] First 5 rows containing \u0026lsquo;virginica\u0026rsquo; values among \u0026lsquo;species\u0026rsquo; 2. iloc #iloc is a method that selects data using integer-based indexes. It accesses data using the integer positions (indexes) of rows and columns. That is, it explicitly specifies the positions of the data in integers to select it.\n# Select the data in the first row and second column \u0026gt;\u0026gt;\u0026gt; iris.iloc[0, 1] 3.5 Data in \u0026lsquo;sepal_width\u0026rsquo;, which is the first row and second column 3. Differences between loc and iloc # Type of Index:\nSince loc uses labels, the names of rows and columns can be strings or other data types.\nSince iloc uses integers, the indexes of rows and columns must be integers.\nUsage:\nloc focuses on selecting data using explicit labels.\niloc focuses on selecting data using integer positions (indexes).\nExamples:\nExample of loc: df.loc[\u0026lsquo;A\u0026rsquo;, \u0026lsquo;column_name\u0026rsquo;]\nExample of iloc: df.iloc[0, 1]\nWhich method to use depends on the structure of the DataFrame and the user\u0026rsquo;s purpose. loc is useful when labels are clearly defined, while iloc is useful when integer-based indexes are used.\n","date":"4 January 2024","permalink":"/posts/3/","section":"Blog posts","summary":"\u003cp\u003eThis article was written to explain the features and differences between \u003cstrong\u003e.loc()\u003c/strong\u003e and \u003cstrong\u003e.iloc()\u003c/strong\u003e, which are necessary when handling DataFrames using the \u003cem\u003e\u003cstrong\u003epandas\u003c/strong\u003e\u003c/em\u003e library in Python.\u003c/p\u003e","title":"Python - pandas loc vs iloc"},{"content":"This article was written to explain sequence types in Python, which are data types where values are connected continuously.\nWhat are Sequence Types? #Sequence types refer to data types where values are connected in a sequence.\nThe biggest feature of sequence types is that they provide common actions and functionalities.\nList [1, 2, 3, 4, 5] [1, 2, 3, 4, 5] Tuple (1, 2, 3, 4, 5) (1, 2, 3, 4, 5) Range range(5) 0, 1, 2, 3, 4 String \u0026lsquo;Hello\u0026rsquo; H e l l o As shown above, sequence types include lists, tuples, ranges, and strings, and also (bytes, bytearray).\nObjects created with sequence types are called sequence objects, and each value of the object is called an element.\nChecking for a Specific Value in a Sequence Object #To check whether a specific value exists within a sequence object, you can use in or not in as shown below.\n\u0026gt;\u0026gt;\u0026gt; a = \u0026#34;Hello\u0026#34; \u0026gt;\u0026gt;\u0026gt; \u0026#34;H\u0026#34; in a True \u0026gt;\u0026gt;\u0026gt; \u0026#34;A\u0026#34; in a False # not in checks if a specific value does not exist \u0026gt;\u0026gt;\u0026gt; \u0026#34;ell\u0026#34; not in a False \u0026gt;\u0026gt;\u0026gt; \u0026#34;Python\u0026#34; not in a True Using the in operator, if a specific value exists, it returns True, otherwise False. Conversely, using the not in operator, if a specific value does not exist it returns True, otherwise False.\nConnecting Sequence Objects #Sequence objects can be connected using the + operator.\n\u0026gt;\u0026gt;\u0026gt; a = [0, 1, 2, 3] \u0026gt;\u0026gt;\u0026gt; b = [4, 5, 6] \u0026gt;\u0026gt;\u0026gt; a + b [0, 1, 2, 3, 4, 5, 6] However, range cannot be connected using the + operator.\n\u0026gt;\u0026gt;\u0026gt; range(0, 5) + range(5, 10) TypeError Traceback (most recent call last) \u0026lt;ipython-input-7-88e74efcb3c0\u0026gt; in \u0026lt;cell line: 1\u0026gt;() ----\u0026gt; 1 range(0, 5) + range(5, 10) TypeError: unsupported operand type(s) for +: \u0026#39;range\u0026#39; and \u0026#39;range\u0026#39; Therefore, converting range to tuples or lists for connection is possible.\n\u0026gt;\u0026gt;\u0026gt; tuple(range(0, 5)) + tuple(range(5, 10)) (0, 1, 2, 3, 4, 5, 6, 7, 8, 9) \u0026gt;\u0026gt;\u0026gt; list(range(0, 5)) + list(range(5, 10)) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] Repeating Sequence Objects #Sequence objects can be repeated using the * operator.\nRepetition is possible with integer * sequence object or sequence * integer.\n\u0026gt;\u0026gt;\u0026gt; [0, 1, 2, 3] * 3 [0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3] However, similar to the method of connecting sequence objects, ranges cannot be repeated using the * operator.\n\u0026gt;\u0026gt;\u0026gt; range(0,10) * 3 TypeError Traceback (most recent call last) \u0026lt;ipython-input-11-824dcf3cff8f\u0026gt; in \u0026lt;cell line: 1\u0026gt;() ----\u0026gt; 1 range(0,10) * 3 TypeError: unsupported operand type(s) for *: \u0026#39;range\u0026#39; and \u0026#39;int\u0026#39; Therefore, converting to tuples or lists for repetition is possible.\nChecking the Number of Elements in a Sequence Object #The number of elements in a sequence object can be checked using the len function.\n# List \u0026gt;\u0026gt;\u0026gt; a = [1, 2, 3, 4, 5] \u0026gt;\u0026gt;\u0026gt; len(a) 5 # Tuple \u0026gt;\u0026gt;\u0026gt; b = (6, 7, 8, 9, 10) \u0026gt;\u0026gt;\u0026gt; len(b) 5 # Range len(range(0, 5, 2)) # -\u0026gt; Increasing by 2 from 0 to 5 gives 0, 2, 4 3 # String \u0026gt;\u0026gt;\u0026gt; c = \u0026#34;Hello, World\u0026#34; \u0026gt;\u0026gt;\u0026gt; len(c) 12 ","date":"2 January 2024","permalink":"/posts/1/","section":"Blog posts","summary":"\u003cp\u003eThis article was written to explain sequence types in Python, which are data types where values are connected continuously.\u003c/p\u003e","title":"Python - Sequence types"},{"content":"","date":null,"permalink":"/tags/sequence-types/","section":"Tags","summary":"","title":"sequence types"},{"content":"This is the advanced tag. Just like other listing pages in Congo, you can add custom content to individual taxonomy terms and it will be displayed at the top of the term listing. 🚀\nYou can also use these content pages to define Hugo metadata like titles and descriptions that will be used for SEO and other purposes.\n","date":null,"permalink":"/tags/advanced/","section":"Tags","summary":"This is the advanced tag.","title":"advanced"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"This section contains all my current projects.\n","date":null,"permalink":"/projects/","section":"Projects","summary":"This section contains all my current projects.","title":"Projects"}]