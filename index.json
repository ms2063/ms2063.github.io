[{"content":"","date":null,"permalink":"/tags/accuracy/","section":"Tags","summary":"","title":"accuracy"},{"content":" Daily blog posts will be updated. üßëüèª‚Äçüíª ","date":null,"permalink":"/posts/","section":"Blog posts","summary":"Daily blog posts will be updated.","title":"Blog posts"},{"content":"","date":null,"permalink":"/tags/confusion-matrix/","section":"Tags","summary":"","title":"confusion matrix"},{"content":"","date":null,"permalink":"/tags/evaluation-metric/","section":"Tags","summary":"","title":"evaluation metric"},{"content":"","date":null,"permalink":"/tags/f1-score/","section":"Tags","summary":"","title":"f1 score"},{"content":"","date":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"machine learning"},{"content":"This article explains Evaluation Metric on Machine Learning.\nWhat is Evaluation Metric? #Evaluating the performance of machine learning models is crucial for determining their effectiveness in making predictions or classifications. Various metrics have been developed to provide insights into the strengths and weaknesses of models across different tasks and datasets.\nAmong these, Accuracy, Confusion Matrix, Precision, Recall, F1 Score, and ROC AUC are fundamental metrics that data scientists and machine learning engineers frequently rely on. This article will delve into each of these metrics, explaining their importance, how they are calculated, and when they are most appropriately used.\nAccuracy #Accuracy is the simplest and most intuitive performance metric. It is the ratio of correctly predicted observations to the total observations. Although it provides a quick assessment of model performance, accuracy can be misleading, especially in imbalanced datasets where the majority class dominates the predictions.\nConfusion Matrix #The confusion matrix is a more detailed metric that allows the evaluation of the performance of a classification algorithm. It is a table that describes the performance of a classification model on a set of test data for which the true values are known. The matrix compares the actual target values with those predicted by the machine learning model, providing insights into the correct and incorrect predictions across different classes.\nPrecision #Precision measures the ratio of correctly predicted positive observations to the total predicted positives. It is a key metric when the cost of a false positive is high. For instance, in email spam detection, a high precision model would minimize the number of non-spam emails incorrectly marked as spam.\nRecall #Recall (also known as sensitivity) measures the ratio of correctly predicted positive observations to the all observations in actual class. It is crucial when the cost of a false negative is significant. For example, in medical diagnostics, a high recall rate would mean that the model correctly identifies as many patients with the condition as possible.\nF1 Score #The F1 Score is the harmonic mean of Precision and Recall, providing a balance between the two metrics. It is particularly useful when the class distribution is uneven. The F1 Score is a better measure than Accuracy for cases where false positives and false negatives may carry different costs.\nROC AUC #The Receiver Operating Characteristic (ROC) curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The Area Under the Curve (AUC) represents a measure of separability. A higher AUC indicates that the model is better at distinguishing between the positive and negative classes across all thresholds. ROC AUC is highly useful for evaluating models in cases of imbalanced datasets or when the costs of different types of errors vary widely.\nWhen to use each metric # Accuracy: When the dataset is balanced and the costs of false positives and false negatives are roughly equal. Confusion Matrix: For a detailed analysis of model performance, including the types of errors made. Precision/Recall: When false positives or false negatives carry a higher cost, respectively. F1 Score: When seeking a balance between precision and recall, especially with uneven class distribution. ROC AUC: When comparing models based on their performance across all classification thresholds, particularly useful in imbalanced datasets. Choosing the right evaluation metric is pivotal in guiding the development and validation of machine learning models. Each metric provides different insights into the model\u0026rsquo;s performance, catering to various aspects of prediction accuracy and error cost. By understanding and correctly applying these metrics, practitioners can more accurately assess their models, leading to more reliable and effective solutions in the diverse landscape of machine learning applications. In the next articles, we will look into each evaluation metric closely.\n","date":"23 February 2024","permalink":"/posts/32/","section":"Blog posts","summary":"\u003cp\u003eThis article explains Evaluation Metric on Machine Learning.\u003c/p\u003e","title":"Machine Learning - Evaluation Metric"},{"content":"","date":null,"permalink":"/tags/precision/","section":"Tags","summary":"","title":"precision"},{"content":"","date":null,"permalink":"/tags/recall/","section":"Tags","summary":"","title":"recall"},{"content":"","date":null,"permalink":"/tags/roc-auc/","section":"Tags","summary":"","title":"roc auc"},{"content":" ","date":null,"permalink":"/tags/","section":"Tags","summary":" ","title":"Tags"},{"content":"Welcome to my website! I\u0026rsquo;m really happy you stopped by.\n","date":null,"permalink":"/","section":"Welcome to Congo!","summary":"Welcome to my website!","title":"Welcome to Congo!"},{"content":"","date":null,"permalink":"/tags/feature-engineering/","section":"Tags","summary":"","title":"feature engineering"},{"content":"This article explains what Feature Engineering is, why it is important, and describes basic Feature Engineering techniques.\nWhat is Feature Engineering? #Feature Engineering is the process of transforming given raw data into features (or variables) that allow a machine learning model to function effectively. This process includes removing unnecessary information, extracting and transforming useful information, and adjusting the data to improve model performance during training.\nThe Importance of Feature Engineering #Feature Engineering can significantly improve the performance of a machine learning model. Good features enable the model to learn patterns in the data better, thus increasing the accuracy of predictions. On the other hand, irrelevant or incorrect features can degrade model performance. Therefore, Feature Engineering is a crucial process for maximizing model performance.\nFeature Engineering Techniques #There are various techniques for Feature Engineering, and below are some of the most basic ones:\nMissing Value Handling: It is important to deal with missing values in the data. Methods include replacing missing values with the mean, median, mode, or removing rows with missing values. Categorical Data Processing: Many models cannot directly process categorical data. Methods such as One-Hot Encoding and Label Encoding can be used to convert categorical data into numerical data. Feature Scaling: Adjusting the scale of various features allows the model to evaluate features fairly. Standardization and Normalization are examples of this. Feature Selection: Important features are selected to reduce model complexity and prevent overfitting. Statistical methods and model-based methods are examples of this. Feature Creation: New features are created by combining or transforming existing features. This helps the model understand the data better. Encoding Conversion #Categorical data refers to data categories represented in text. Since most machine learning algorithms take numerical data as input, converting these categorical data into an appropriate numerical format is essential. Two primary methods are used for encoding conversion.\nOne-Hot Encoding #Converts each category into a separate column, assigning a value of 1 if the category is present and 0 otherwise. This method does not consider the order or importance of categories, allowing the model to treat each category equally.\nLabel Encoding #Converts each category into a numerical value by assigning sequential numbers. For example, \u0026lsquo;red\u0026rsquo;, \u0026lsquo;blue\u0026rsquo;, \u0026lsquo;green\u0026rsquo; can be converted to 0, 1, 2, respectively. While Label Encoding does not increase the dimensionality as much as the number of categories, care must be taken as the magnitude of numbers can affect the model.\nScaling #Feature Scaling is the process of standardizing the units or range of data to a uniform scale, ensuring all features equally influence the model. Two primary methods used for scaling are:\nStandardization: Adjusts the data to have a mean of 0 and a standard deviation of 1. This method is useful when the data distribution does not follow a normal distribution and is less sensitive to outliers. Normalization: Adjusts the data values to a range between 0 and 1. The most common method uses the minimum and maximum values, ensuring all data points have the same scale. ","date":"22 February 2024","permalink":"/posts/31/","section":"Blog posts","summary":"\u003cp\u003eThis article explains what Feature Engineering is, why it is important, and describes basic Feature Engineering techniques.\u003c/p\u003e","title":"Machine Learning - Feature Engineering"},{"content":"","date":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"python"},{"content":"","date":null,"permalink":"/tags/label-encoding/","section":"Tags","summary":"","title":"label encoding"},{"content":"This article explains Label Encoding on Machine Learning.\nLabel Encoding #Label Encoding converts categorical features into numerical values. For example, product categories like \u0026lsquo;TV\u0026rsquo;, \u0026lsquo;fridge\u0026rsquo;, \u0026lsquo;microwave\u0026rsquo;, \u0026lsquo;computer\u0026rsquo;, \u0026lsquo;fan\u0026rsquo;, \u0026lsquo;mixer\u0026rsquo; are converted into numerical codes such as TV: 1, fridge: 2, and so on. It\u0026rsquo;s crucial to note that even numerical codes like \u0026lsquo;01\u0026rsquo;, \u0026lsquo;02\u0026rsquo; should be converted to numeric values without leading zeros.\nScikit-learn Implementation #Scikit-learn implements Label Encoding through the LabelEncoder class. You create a LabelEncoder object and perform label encoding by calling fit() and transform().\nfrom sklearn.preprocessing import LabelEncoder items = [\u0026#39;TV\u0026#39;, \u0026#39;fridge\u0026#39;, \u0026#39;microwave\u0026#39;, \u0026#39;computer\u0026#39;, \u0026#39;fan\u0026#39;, \u0026#39;fan\u0026#39;, \u0026#39;mixer\u0026#39;, \u0026#39;mixer\u0026#39;] # Create objects with LabelEncoder \u0026amp; # Conduct label encoding with fit( ) and transform( ) encoder = LabelEncoder() encoder.fit(items) labels = encoder.transform(items) print(\u0026#39;Encoded label:\u0026#39;, labels) Encoded label: [0 3 4 1 2 2 5 5] This results in Encoded label: [0 3 4 1 2 2 5 5], where \u0026lsquo;TV\u0026rsquo; is 0, \u0026lsquo;fridge\u0026rsquo; is 3, \u0026lsquo;microwave\u0026rsquo; is 4, \u0026lsquo;computer\u0026rsquo; is 1, \u0026lsquo;fan\u0026rsquo; is 2, and \u0026lsquo;mixer\u0026rsquo; is 5. If it\u0026rsquo;s unclear which string values correspond to which numerical codes, you can check the classes_ attribute of the LabelEncoder object.\nprint(\u0026#39;Encoding Class:\u0026#39;, encoder.classes_) Encoding Class: [\u0026#39;TV\u0026#39; \u0026#39;computer\u0026#39; \u0026#39;fan\u0026#39; \u0026#39;fridge\u0026#39; \u0026#39;microwave\u0026#39; \u0026#39;mixer\u0026#39;] The classes_ attribute holds the original values corresponding to the encoding numbers starting from 0 in order. Therefore, it can be determined that \u0026lsquo;TV\u0026rsquo; is encoded as 0, \u0026lsquo;computer\u0026rsquo; as 1, \u0026lsquo;fan\u0026rsquo; as 2, \u0026lsquo;fridge\u0026rsquo; as 3, \u0026lsquo;microwave\u0026rsquo; as 4, and \u0026lsquo;mixer\u0026rsquo; as 5. For decoding, inverse_transform() can be used to revert the encoded values back to the original strings.\nprint(\u0026#39;Decoding original:\u0026#39;, encoder.inverse_transform([4, 2, 5, 0, 3, 3, 2, 2])) Decoding original: [\u0026#39;microwave\u0026#39; \u0026#39;fan\u0026#39; \u0026#39;mixer\u0026#39; \u0026#39;TV\u0026#39; \u0026#39;fridge\u0026#39; \u0026#39;fridge\u0026#39; \u0026#39;fan\u0026#39; \u0026#39;fan\u0026#39;] When product data consists of two attributes, product category and price, applying label encoding to the product category can transform it as follows.\nOriginal Data Product Category Price TV 1,000,000 fridge 1,500,000 microwave 200,000 computer 800,000 fan 100,000 fan 100,000 mixer 50,000 mixer 50,000 Data with Encoded Product Category Product Category Price 0 1,000,000 3 1,500,000 4 200,000 1 800,000 2 100,000 2 100,000 5 50,000 5 50,000 Label encoding converts string values to numeric category values, simplifying categorical data handling. However, this approach may lead to performance issues in some machine learning algorithms due to the numeric values\u0026rsquo; inherent order or magnitude, potentially influencing algorithm predictions inaccurately.\nFor instance, numerical values might imply an unintended order or importance among categories (e.g., \u0026lsquo;computer\u0026rsquo; encoded as 1 might be deemed less significant than \u0026lsquo;fan\u0026rsquo; encoded as 2). Therefore, label encoding is not recommended for models like linear regression that interpret these magnitudes.\nInstead, it\u0026rsquo;s suitable for tree-based algorithms that don\u0026rsquo;t consider the numeric value\u0026rsquo;s order. One-Hot Encoding is proposed to address these issues, offering an alternative that avoids ordinal implications.\nWe will explore One-Hot Encoding in the next section.\n","date":"21 February 2024","permalink":"/posts/29/","section":"Blog posts","summary":"\u003cp\u003eThis article explains Label Encoding on Machine Learning.\u003c/p\u003e","title":"Machine Learning - Label Encoding"},{"content":"This article explains what One-Hot Encoding is on Machine Learning.\nOne-Hot Encoding #One-Hot Encoding adds new features based on the types of feature values, marking 1 in the column corresponding to the unique value and 0 in all others. This changes the dimensionality from a single feature with various unique values to multiple binary features, transforming row-based unique feature values into columnar form. The process involves converting the original data into a format where each unique value of the feature gets its column, with 1 indicating the presence of the feature value and 0 indicating its absence.\nOriginal Data Product Category TV fridge microwave computer fan fan mixer mixer One-Hot Encoding Product Category_TV Product Category_computer Product Category_fan Product Category_fridge Product Category_microwave Product Category_mixer 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 The original data contains 8 records with 6 unique values: [\u0026lsquo;TV\u0026rsquo;, \u0026lsquo;computer\u0026rsquo;, \u0026lsquo;fan\u0026rsquo;, \u0026lsquo;fridge\u0026rsquo;, \u0026lsquo;microwave\u0026rsquo;, \u0026lsquo;mixer\u0026rsquo;]. From the label encoding example, we know \u0026lsquo;TV\u0026rsquo; is encoded as 0, \u0026lsquo;computer\u0026rsquo; as 1, and so on. For one-hot encoding, each product category is converted into 6 unique features. If a record\u0026rsquo;s category is \u0026lsquo;TV\u0026rsquo;, then \u0026lsquo;Product Category_TV\u0026rsquo; is marked 1, and all others 0. Similarly, if a record\u0026rsquo;s category is \u0026lsquo;fridge\u0026rsquo;, then \u0026lsquo;Product Category_fridge\u0026rsquo; is 1, and others 0. This method, where only one attribute is marked as 1, is named one-hot encoding.\nScikit-learn Implementation #One-Hot Encoding in Scikit-learn can be performed using the OneHotEncoder class. Unlike LabelEncoder, it requires the input data to be in a 2D format. Additionally, the output from OneHotEncoder is a sparse matrix, which should be converted to a dense matrix using the toarray() method for ease of use. This process enables the transformation of categorical data into a format suitable for machine learning algorithms that require numerical input.\nfrom sklearn.preprocessing import OneHotEncoder import numpy as np items=[\u0026#39;TV\u0026#39;, \u0026#39;fridge\u0026#39;, \u0026#39;microwave\u0026#39;, \u0026#39;computer\u0026#39;, \u0026#39;fan\u0026#39;, \u0026#39;fan\u0026#39;, \u0026#39;mixer\u0026#39;, \u0026#39;mixer\u0026#39;] # Converting to 2 dimension ndarray items =np.array(items).reshape(-1 , 1) # Applying One-Hot Encoding oh_encoder = OneHotEncoder() oh_encoder.fit(items) oh_labels = oh_encoder.transform(items) # The result of the conversion using OneHotEncoder is a sparse matrix, so we use toarray() to convert it into a dense matrix. print(\u0026#39;One-Hot Encoded data:\u0026#39;) print(oh_labels.toarray()) print(\u0026#39;Dimensions of One-Hot Encoded data:\u0026#39;) print(oh_labels.shape) One-Hot Encoded data: [[1. 0. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0.] [0. 0. 0. 0. 1. 0.] [0. 1. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0.] [0. 0. 1. 0. 0. 0.] [0. 0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 1.]] Dimensions of One-Hot Encoded data: (8, 6) The original data, consisting of 8 records and 1 column, transforms into a dataset with 8 records and 6 columns through one-hot encoding. This encoding assigns \u0026lsquo;TV\u0026rsquo; as 0, \u0026lsquo;computer\u0026rsquo; as 1, \u0026lsquo;fan\u0026rsquo; as 2, \u0026lsquo;fridge\u0026rsquo; as 3, \u0026lsquo;microwave\u0026rsquo; as 4, and \u0026lsquo;mixer\u0026rsquo; as 5, with each number corresponding to a specific column. Hence, if the original data\u0026rsquo;s first record is \u0026lsquo;TV\u0026rsquo;, in the transformed data, the first column in the first record is 1, and all other columns are 0. This process effectively expands the dataset\u0026rsquo;s dimensionality to more accurately represent categorical data for machine learning models.\nOriginal Data Product Category Price TV 1,000,000 fridge 1,500,000 microwave 200,000 computer 800,000 fan 100,000 fan 100,000 mixer 50,000 mixer 50,000 ‚Üì\nOriginal Data Product Category Price 0 1,000,000 3 1,500,000 4 200,000 1 800,000 2 100,000 2 100,000 5 50,000 5 50,000 ‚Üì\nOne-Hot Encoding Product Category_TV Product Category_computer Product Category_fan Product Category_fridge Product Category_microwave Product Category_mixer Price 1 0 0 0 0 0 1,000,000 0 0 0 1 0 0 1,500,000 0 0 0 0 1 0 200,000 0 1 0 0 0 0 800,000 0 0 1 0 0 0 100,000 0 0 1 0 0 0 100,000 0 0 0 0 0 1 50,000 0 0 0 0 0 1 50,000 Pandas offers an easier API for one-hot encoding through the get_dummies() function. Unlike Scikit-learn\u0026rsquo;s OneHotEncoder, it allows direct conversion of string category values to numeric form without needing to transform them into numbers first, simplifying the encoding process.\nimport pandas as pd df = pd.DataFrame({\u0026#39;item\u0026#39;:[\u0026#39;TV\u0026#39;, \u0026#39;fridge\u0026#39;, \u0026#39;microwave\u0026#39;, \u0026#39;computer\u0026#39;, \u0026#39;fan\u0026#39;, \u0026#39;fan\u0026#39;, \u0026#39;mixer\u0026#39;, \u0026#39;mixer\u0026#39;]}) pd.get_dummies(df) item_TV item_computer item_fan item_fridge item_microwave item_mixer 0 True False False False False False 1 False False False True False False 2 False False False False True False 3 False True False False False False 4 False False True False False False 5 False False True False False False 6 False False False False False True 7 False False False False False True Using get_dummies() allows for direct conversion without needing to first transform string categories into numeric values. This feature simplifies the process of applying one-hot encoding to categorical data in pandas.\n","date":"21 February 2024","permalink":"/posts/30/","section":"Blog posts","summary":"\u003cp\u003eThis article explains what One-Hot Encoding is on Machine Learning.\u003c/p\u003e","title":"Machine Learning - One-Hot Encoding"},{"content":"","date":null,"permalink":"/tags/one-hot-encoding/","section":"Tags","summary":"","title":"one-hot encoding"},{"content":"","date":null,"permalink":"/tags/k-fold-cross-validation/","section":"Tags","summary":"","title":"k-fold cross-validation"},{"content":"This article explains K-Fold Cross-Validation on Machine Learning.\nIn the fast-evolving field of machine learning, the ability to accurately evaluate a model\u0026rsquo;s performance is crucial. One of the most popular and robust methods for model evaluation is K-Fold Cross-Validation. This technique not only helps in assessing the effectiveness of a model but also ensures that it generalizes well to new, unseen data. This article will dive deep into what K-Fold Cross-Validation is, how it works, its benefits, and how to implement it.\nWhat is K-Fold Cross-Validation? #K-Fold Cross-Validation is a resampling procedure used to evaluate machine learning models on a limited data sample. The procedure has a single parameter called K that refers to the number of groups that a given data sample is to be split into. This approach allows multiple testing and training cycles, ensuring the model is trained and validated on all available data for more reliable performance metrics.\nHow Does It Work? #The process of K-Fold Cross-Validation involves the following steps:\nShuffle the Dataset Randomly: This ensures that the data distribution is random and not biased by the initial arrangement of data. Split the Dataset into K Groups: The dataset is divided into K equally (or nearly equally) sized folds or groups. For Each Unique Group: Take the group as a holdout or test data set. Take the remaining groups as a training data set. Fit a model on the training set and evaluate it on the test set. Retain the evaluation score and discard the model. Summarize the Skill of the Model: Use the sample of model evaluation scores from each iteration to estimate the overall performance of the model. Benefits of K-Fold Cross-Validation # Minimizes Data Waste: By using each fold as a test set, it ensures that all data contributes to training and validation, which is particularly beneficial in scenarios with limited datasets. More Accurate Estimates: It provides a more reliable estimate of the model\u0026rsquo;s performance on unseen data compared to a single train-test split. Reduces Bias: The random shuffling and rotation of data for training and testing reduce the bias associated with the random splits. Implementing K-Fold Cross-Validation #Implementing K-Fold Cross-Validation can be straightforward, especially with the use of libraries such as Scikit-Learn in Python. Here is a basic outline of steps to implement K-Fold Cross-Validation:\nPrepare Your Dataset: Ensure your data is clean and ready for modeling. Choose a Model: Select the machine learning algorithm you wish to evaluate. Configure K-Fold: Decide on the number of folds, K, you wish to use. A common choice is K=10, though the optimal number can vary based on your dataset size and characteristics. Execute Cross-Validation: Use the cross-validation feature in Scikit-learn or another ML library to automate the splitting, training, and evaluation process. Analyze Results: Once the cross-validation is complete, analyze the scores to understand the model\u0026rsquo;s performance and stability. Conclusion #K-Fold Cross-Validation is a powerful tool in the machine learning toolkit, offering a balanced approach to model evaluation and validation. By understanding and applying this technique, data scientists and machine learning engineers can ensure their models are both accurate and robust, ready to tackle real-world challenges with confidence. Whether you\u0026rsquo;re working on small datasets or large-scale projects, K-Fold Cross-Validation is an indispensable method for achieving reliable and generalizable model performance.\n","date":"20 February 2024","permalink":"/posts/25/","section":"Blog posts","summary":"\u003cp\u003eThis article explains K-Fold Cross-Validation on Machine Learning.\u003c/p\u003e","title":"Machine Learning - K-Fold Cross-Validation"},{"content":"This article explains the concept of Random Search, one of the hyperparameter tuning methods to maximize the performance of models in machine learning, and introduce an example of implementation using the Scikit-learn library.\nWhat is Random Search? #Random Search is a method that evaluates randomly selected combinations within a given parameter space to find the optimal combination of hyperparameters. Unlike Grid Search, which systematically explores all combinations of specified parameters, Random Search evaluates combinations randomly selected from the search space. This method is particularly useful when the dimension of hyperparameters is high or the search space is large, often yielding similar or better results in less time.\nKey Parameters #Random Search can be implemented through the RandomizedSearchCV class in Scikit-learn. The key parameters are as follows:\nestimator: Specifies the model to optimize. For example, RandomForestClassifier(), SVC(), etc. param_distributions: Specifies the parameter space to explore. You can specify a continuous distribution for each parameter or provide a list. n_iter: Specifies the number of parameter settings to be randomly selected. The larger this value, the more combinations will be explored, but computation time will also increase. scoring: Specifies the criterion for evaluating the model\u0026rsquo;s performance. For example, \u0026lsquo;accuracy\u0026rsquo;, \u0026lsquo;f1\u0026rsquo;, etc. cv: Specifies the strategy for cross-validation splitting. Entering an integer value will perform k-fold cross-validation with that value. random_state: Specifies the seed value for the random number generator to ensure reproducibility of the results. Implementing RandomizedSearchCV #Below is an example of using RandomizedSearchCV to find the optimal hyperparameters for a classifier. In the code below, we use a Support Vector Machine (SVM).\n\u0026gt;\u0026gt;\u0026gt; from sklearn.model_selection import RandomizedSearchCV \u0026gt;\u0026gt;\u0026gt; from sklearn.svm import SVC \u0026gt;\u0026gt;\u0026gt; from sklearn.datasets import load_iris \u0026gt;\u0026gt;\u0026gt; from sklearn.model_selection import train_test_split \u0026gt;\u0026gt;\u0026gt; import scipy.stats as stats # Load the dataset \u0026gt;\u0026gt;\u0026gt; iris = load_iris() \u0026gt;\u0026gt;\u0026gt; X, y = iris.data, iris.target # Split the dataset into training and testing sets \u0026gt;\u0026gt;\u0026gt; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Instantiate the Support Vector Machine \u0026gt;\u0026gt;\u0026gt; svc = SVC() # Define the hyperparameter space to explore \u0026gt;\u0026gt;\u0026gt; param_distributions = { \u0026#39;C\u0026#39;: stats.uniform(0.1, 1000), \u0026#39;gamma\u0026#39;: stats.uniform(0.0001, 0.1), \u0026#39;kernel\u0026#39;: [\u0026#39;linear\u0026#39;, \u0026#39;rbf\u0026#39;] } # Instantiate RandomizedSearchCV \u0026gt;\u0026gt;\u0026gt; random_search.fit(X_train, y_train) # Output the best parameters and highest accuracy \u0026gt;\u0026gt;\u0026gt; print(\u0026#34;Best parameters:\u0026#34;, random_search.best_params_) \u0026gt;\u0026gt;\u0026gt; print(\u0026#34;Best cross-validation score: {:.2f}\u0026#34;.format(random_search.best_score_)) # Evaluate performance on the test set \u0026gt;\u0026gt;\u0026gt; accuracy = random_search.score(X_test, y_test) \u0026gt;\u0026gt;\u0026gt; print(\u0026#34;Test set accuracy: {:.2f}\u0026#34;.format(accuracy)) This code searches for the optimal combination by exploring random combinations of the specified C, gamma, kernel hyperparameters for the SVC model. RandomizedSearchCV can be a more effective method than grid search, especially when the search space is wide.\n","date":"20 February 2024","permalink":"/posts/28/","section":"Blog posts","summary":"\u003cp\u003eThis article explains the concept of Random Search, one of the hyperparameter tuning methods to maximize the performance of models in machine learning, and introduce an example of implementation using the Scikit-learn library.\u003c/p\u003e","title":"Machine Learning - Random Search"},{"content":"This article explains what Stratified K-Fold Cross-Validation is on Machine Learning.\nWhat is Stratified K-Fold Cross-Validation? #Stratified K-Fold Cross-Validation is an extension of the traditional K-Fold technique, designed to ensure that each fold of the dataset contains approximately the same percentage of samples of each target class as the complete set. In other words, it stratifies the data, preserving the class distribution within each fold. This method is crucial for evaluating models on imbalanced datasets, where the conventional K-Fold Cross-Validation might not provide a representative analysis of the model\u0026rsquo;s performance.\nHow It Works #The Stratified K-Fold Cross-Validation process involves several key steps, similar to the standard K-Fold but with a critical distinction in the data splitting strategy:\nStratify the Dataset: Before splitting, the dataset is stratified, ensuring that each fold is a good representative of the whole. Split the Dataset into K Folds: The stratified dataset is divided into K folds, maintaining the proportion of the classes in each fold as much as possible. Model Training and Evaluation: For each fold: Use the current fold as the test set, and the remaining folds as the training set. Train the model on the training set and evaluate it on the test set. Record the evaluation metrics for later analysis. Aggregate Results: After iterating through all folds, aggregate the evaluation metrics to provide a comprehensive assessment of the model\u0026rsquo;s performance. Benefits of Stratified K-Fold Cross-Validation # Improved Bias and Variance Handling: By preserving the original distribution of classes, Stratified K-Fold minimizes bias and variance in the model evaluation process, offering a more accurate performance estimate. Better for Imbalanced Data: It is particularly advantageous for imbalanced datasets, where conventional cross-validation techniques might fail to provide an accurate reflection of a model\u0026rsquo;s ability to generalize. Enhanced Model Evaluation: Stratified K-Fold provides a deeper insight into how well a model can perform across different subsets of the data, making it a more robust evaluation technique. Implementing Stratified K-Fold Cross-Validation #Implementing Stratified K-Fold Cross-Validation is straightforward with modern data science libraries. Here\u0026rsquo;s a simplified approach using Python\u0026rsquo;s Scikit-Learn library:\nPrepare Your Dataset: Clean your dataset, ensuring it\u0026rsquo;s ready for modeling, and identify your target variable. Select a Model: Choose the machine learning model you wish to evaluate. Configure Stratified K-Fold: Utilize Scikit-Learn\u0026rsquo;s StratifiedKFold class to set up your cross-validation. Choose the number of folds, K, according to your dataset size and the level of performance detail you need. Execute Cross-Validation: Apply the StratifiedKFold object to split your dataset, ensuring each fold maintains the class proportion. Train and evaluate your model on each fold. Analyze the Results: Once the process is complete, analyze the aggregated results to gain insights into your model\u0026rsquo;s performance and stability. Conclusion #Stratified K-Fold Cross-Validation stands out as a superior approach for evaluating machine learning models, especially when dealing with imbalanced datasets. By ensuring that each fold mirrors the class distribution of the entire dataset, it provides a more accurate and reliable assessment of model performance. This technique is essential for data scientists and machine learning engineers who aim to develop robust, generalizable models capable of performing well across diverse data scenarios. Whether you\u0026rsquo;re working on classification tasks with imbalanced classes or striving for the most accurate model evaluation, Stratified K-Fold Cross-Validation is an invaluable tool in your machine learning arsenal.\n","date":"20 February 2024","permalink":"/posts/26/","section":"Blog posts","summary":"\u003cp\u003eThis article explains what Stratified K-Fold Cross-Validation is on Machine Learning.\u003c/p\u003e","title":"Machine Learning - Stratified K-Fold Cross-Validation"},{"content":"","date":null,"permalink":"/tags/random-search/","section":"Tags","summary":"","title":"random search"},{"content":"","date":null,"permalink":"/tags/stratified-k-fold-cross-validation/","section":"Tags","summary":"","title":"stratified k-fold cross-validation"},{"content":"","date":null,"permalink":"/tags/grid-search/","section":"Tags","summary":"","title":"grid search"},{"content":"One of the efficient methods to find the optimal parameters of a model in machine learning is Grid Search. This article explains what grid search is, how it works, and when it should be used.\nWhat is Grid Search? #Grid search is one of the methods to optimize hyperparameters of a machine learning model. This method tests all combinations of specified hyperparameters to find the combination that produces the best performance. The performance of each parameter combination is evaluated through cross-validation, and through this process, the optimal model can be selected.\nHow It Works #Grid search first receives a range or list of hyperparameters specified by the user. For example, let\u0026rsquo;s assume we are conducting a grid search for a decision tree classifier. The user sets the range of hyperparameters such as the depth of the tree (depth), the minimum number of samples for splitting (min_samples_split), etc. Grid search trains the model on all possible combinations within this range and evaluates the performance of each combination using cross-validation. Common methods for performance evaluation include accuracy, precision, recall, F1 score, etc. After the evaluation, the parameter combination with the best performance is selected.\nAdvantages and Disadvantages #Advantages:\nEasy to use and understand Because it explores all possible combinations, there is a high possibility of finding the optimal combination Disadvantages:\nVery high computational cost. As the number of parameters and their range increases, the required amount of computation increases exponentially It takes a long time to find the optimal combination When to Use #Grid search is suitable when the range of parameters is relatively small and the model\u0026rsquo;s learning time is short. Also, it is good to use when finding the optimal combination of hyperparameters is important and there are sufficient computational resources. However, for models with a very large parameter space or very long learning time, it is advisable to consider other hyperparameter optimization techniques such as Random Search or Bayesian optimization.\nGridSearchCV #GridSearchCV is a class included in the model selection module of the scikit-learn (sklearn) library, used to search the hyperparameter space of a given model through cross-validation and find the optimal parameters. The main parameters that can be passed to this class\u0026rsquo;s constructor are as follows:\nKey Parameters # estimator: The model to optimize. For example, it could be scikit-learn\u0026rsquo;s estimator objects like RandomForestClassifier(), SVC(), etc. param_grid: A dictionary of parameters to search. For example, you could set {\u0026rsquo;n_estimators\u0026rsquo;: [100, 200], \u0026lsquo;max_features\u0026rsquo;: [\u0026lsquo;auto\u0026rsquo;, \u0026lsquo;sqrt\u0026rsquo;]}, which means to search the values [100, 200] for n_estimators and [\u0026lsquo;auto\u0026rsquo;, \u0026lsquo;sqrt\u0026rsquo;] for max_features parameters. scoring: The criterion for evaluating the model\u0026rsquo;s performance. It is specified as a string, such as \u0026lsquo;accuracy\u0026rsquo;, \u0026lsquo;f1\u0026rsquo;, etc. Other predefined scoring options in scikit-learn can also be used. cv: The strategy for cross-validation splitting. For example, 5 means 5-fold cross-validation. You can also directly pass objects of scikit-learn\u0026rsquo;s splitters like KFold, StratifiedKFold, etc. refit: Determines whether to retrain the model on the entire dataset after finding the optimal parameters. The default value is True, meaning the model is trained on the entire dataset with the optimal parameters. Implementing GridSearchCV #The following is a simple example of using Scikit-learn\u0026rsquo;s GridSearchCV to find the optimal hyperparameters for a classifier. Here, we use a decision tree classifier (DecisionTreeClassifier).\n\u0026gt;\u0026gt;\u0026gt; from sklearn.model_selection import GridSearchCV \u0026gt;\u0026gt;\u0026gt; from sklearn.tree import DecisionTreeClassifier \u0026gt;\u0026gt;\u0026gt; from sklearn.datasets import load_iris \u0026gt;\u0026gt;\u0026gt; from sklearn.model_selection import train_test_split # Load data \u0026gt;\u0026gt;\u0026gt; iris = load_iris() \u0026gt;\u0026gt;\u0026gt; X = iris.data \u0026gt;\u0026gt;\u0026gt; y = iris.target # Split training and testing set \u0026gt;\u0026gt;\u0026gt; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Set up the model \u0026gt;\u0026gt;\u0026gt; estimator = DecisionTreeClassifier() # Set the grid of parameters to search \u0026gt;\u0026gt;\u0026gt; param_grid = { \u0026#39;max_depth\u0026#39;: [None, 2, 4, 6, 8], \u0026#39;min_samples_split\u0026#39;: [2, 5, 10], \u0026#39;min_samples_leaf\u0026#39;: [1, 2, 4] } # Set up GridSearchCV \u0026gt;\u0026gt;\u0026gt; grid_search = GridSearchCV(estimator=estimator, param_grid=param_grid, scoring=\u0026#39;accuracy\u0026#39;, cv=5, refit=True) # Perform grid search \u0026gt;\u0026gt;\u0026gt; grid_search.fit(X_train, y_train) # Print the best parameters and highest score \u0026gt;\u0026gt;\u0026gt; print(\u0026#34;Best parameters:\u0026#34;, grid_search.best_params_) \u0026gt;\u0026gt;\u0026gt; print(\u0026#34;Best score:\u0026#34;, grid_search.best_score_) # Evaluate performance on test data \u0026gt;\u0026gt;\u0026gt; test_accuracy = grid_search.score(X_test, y_test) \u0026gt;\u0026gt;\u0026gt; print(\u0026#34;Test accuracy:\u0026#34;, test_accuracy) ","date":"19 February 2024","permalink":"/posts/27/","section":"Blog posts","summary":"\u003cp\u003eOne of the efficient methods to find the optimal parameters of a model in machine learning is Grid Search. This article explains what grid search is, how it works, and when it should be used.\u003c/p\u003e","title":"Machine Learning - Grid Search"},{"content":"","date":null,"permalink":"/tags/cost-function/","section":"Tags","summary":"","title":"cost function"},{"content":"","date":null,"permalink":"/tags/gradient-descent/","section":"Tags","summary":"","title":"gradient descent"},{"content":"","date":null,"permalink":"/tags/linear-regression-model/","section":"Tags","summary":"","title":"linear regression model"},{"content":"This article explains what Cost Function is for Machine Learning.\nCost Function #In machine learning, the cost function plays a pivotal role. It is a mathematical formula that measures the performance of a machine learning model for given data. The cost function quantifies the error between predicted values and expected values and presents it in the form of a single real number.\nDepending on the problem, this could be the difference between the predicted class and the actual class in classification problems, or the difference between estimated values and actual values in regression problems.\nWhen training a model, our goal is to find the best parameters (coefficients or weights) that minimize the cost function.\nLet\u0026rsquo;s take a look at the following example:\nsize in feet\\(^2\\) (x) price $1000‚Äôs (y) 2104 460 1416 232 1534 315 852 178 ‚Ä¶ ‚Ä¶ The chosen model for our training set is a simple linear regression model defined as \\(f_{w,b}(x) = wx+b\\) .\nHere, \\(w,b\\) are the parameters of the model, which we will adjust to fit our model to the data best. These parameters are crucial as they will determine the slope of the line (weight \\(w\\)) and the point where the line crosses the y-axis (bias \\(b\\)).\nThe objective during training is to find the optimal values for these parameters so that our model\u0026rsquo;s predictions, \\( \\hat{y} \\), are as close as possible to the actual outcomes, \\(y\\), for all provided training examples \\( (x^{(i)}, y^{(i)}) \\).\nTo achieve this, we introduce a cost function, more specifically, the Squared Error Cost Function, which is represented as:\n\\(J(w,b) = \\frac{1}{2m} \\displaystyle\\sum_{i=1}^m(f_{w,b}(x^{(i)})-y^{(i)})^2 \\)\nWhere:\n\\(J(w,b)\\) is the cost function. \\(m\\) is the number of training examples. \\( \\hat{y}^{(i)} \\) is the prediction of the model for the \\(i^{th}\\) example. \\( y^{(i)}\\) is the actual value for the \\(i^{th}\\) example. The factor of \\(\\frac{1}{2}\\) is used to simplify the derivative calculation of the cost function. Our goal in training is to minimize \\(J(w,b)\\), which will result in the best possible values for \\(w\\) and \\(b\\).\nThe charts below show the possible linear models given different values of \\(w\\) and \\(b\\):\nBy adjusting \\(w\\) and \\(b\\), we can observe how the line fits the data points. The better the fit, the lower our cost function value, and hence the more accurate predictions our model will make.\n","date":"18 February 2024","permalink":"/posts/20/","section":"Blog posts","summary":"\u003cp\u003eThis article explains what Cost Function is for Machine Learning.\u003c/p\u003e","title":"Machine Learning - Cost Function"},{"content":"This article explains what Gradient Descent is on Machine Learning.\nWhat is Gradient Descent? #Gradient Descent is arguably the backbone of machine learning. It\u0026rsquo;s a first-order iterative optimization algorithm used to find the minimum value of a function. Simply put, it\u0026rsquo;s a method for finding the lowest point of a curve. But in the context of machine learning, it\u0026rsquo;s how we train our models to make accurate predictions. Let\u0026rsquo;s break this down.\nThe Intuition #Imagine you\u0026rsquo;re blindfolded on a mountain, and your goal is to get to the bottom as quickly as possible. You can feel the slope of the mountain under your feet and decide your next step based on the steepest descent. This is what Gradient Descent does in a mathematical landscape.\nThe Cost Function #The mountain in our scenario is the cost function, a measure of \u0026ldquo;how bad\u0026rdquo; the model is based on its parameters. For a linear regression model, it\u0026rsquo;s the mean squared error over all training examples. The goal is to adjust the parameters to minimize this error.\nThe Gradients #The slope you feel under your feet are the gradients. In mathematical terms, a gradient is a partial derivative with respect to its inputs. In simpler terms, it tells you how much the cost function will change if you change the parameters slightly.\nThe Descent #Here\u0026rsquo;s how it works:\nInitialize Parameters: Start with random values for the parameters. Calculate Gradient: Determine the gradient of the cost function at the current position. Update Parameters: Adjust the parameters in the opposite direction of the gradient. Repeat: Perform steps 2 and 3 until the cost function stops changing significantly. This iterative process is the descent down the mountain, and it\u0026rsquo;s done over several iterations or epochs until we reach convergence.\nLearning Rate #The size of the steps you take is called the learning rate. If it\u0026rsquo;s too small, you\u0026rsquo;ll eventually reach the bottom, but it might take a long time. If it\u0026rsquo;s too large, you might overshoot and never reach the bottom. Finding the right learning rate is crucial and often requires some experimentation.\nTypes of Gradient Descent #There are a few different flavors of Gradient Descent:\nBatch Gradient Descent: Uses the entire training set to calculate the gradient at each step. Stochastic Gradient Descent (SGD): Uses a single training example at each step. It\u0026rsquo;s much faster and can escape local minima, but it\u0026rsquo;s also noisier. Mini-batch Gradient Descent: Strikes a balance between Batch and SGD by using a subset of the training data for each step. Challenges #Gradient Descent isn\u0026rsquo;t without its challenges:\nLocal Minima: These are \u0026ldquo;valleys\u0026rdquo; in the cost function that aren\u0026rsquo;t the absolute lowest point. Plateaus: Flat areas where the gradient is close to zero can slow down the descent. Choosing the Right Learning Rate: As mentioned, this is crucial and can be tricky. Conclusion #Gradient Descent is a fundamental algorithm in machine learning for optimizing models. It\u0026rsquo;s how we \u0026ldquo;train\u0026rdquo; our models to make better predictions. Understanding how it works is key to understanding how machine learning algorithms learn from data.\nRemember, like many methods in machine learning, Gradient Descent is more of an art than a science. It requires intuition, experimentation, and practice to master.\n","date":"18 February 2024","permalink":"/posts/21/","section":"Blog posts","summary":"\u003cp\u003eThis article explains what Gradient Descent is on Machine Learning.\u003c/p\u003e","title":"Machine Learning - Gradient Descent"},{"content":"This article explains what Linear Regression Model is for Machine Learning.\nWhat is Linear Regression Model? #Linear Regression Model is a type of machine learning algorithm. It is predominantly used in predictive analysis and forecasting. The core idea behind a Linear Regression Model is to use a training set ‚Äì a set of data used to guide the learning process ‚Äì to train the model.\nNotation # \\(x\\) : feature, input variable\n\\(y\\) : output variable, target variable\n\\(m\\) : number of training examples\n\\((x, y)\\) : single training example\n\\((x^{(i)}, y^{(i)})\\) : \\(i^{th}\\) training example (\\(1^{st}\\), \\(2^{nd}\\), \\(3^{rd}\\), ‚Ä¶)\nflowchart TD A(training set) --\u003e B(learning algorithm) B --\u003e C(f) D(x) --\u003e C C --\u003e E(y hat) x: feature f: model, function y hat : prediction\nHow to represent the model \\(f\\) ?\n\\(f_{w,b}(x)=wx+b\\)\n\\(f(x)=wx+b\\)\nWhen the Linear Regression Model is used in the context of a single variable, it is often referred to as Univariate Linear Regression. This is a special case of the general Linear Regression Model where the output variable depends on a single input variable.\n","date":"18 February 2024","permalink":"/posts/19/","section":"Blog posts","summary":"\u003cp\u003eThis article explains what Linear Regression Model is for Machine Learning.\u003c/p\u003e","title":"Machine Learning - Linear Regression Model"},{"content":"This article explains what Linear Regression with Multiple Variables is on Machine Learning.\nWhat is Linear Regression with Multiple Variables? #Linear Regression with Multiple Variables in Machine Learning involves extending the concept of linear regression from a single variable to multiple features. This approach allows us to model more complex relationships between the input features and the output variable. Here\u0026rsquo;s a clearer explanation:\nFeatures #Consider a dataset with multiple features related to housing prices, such as:\nSize in feet\\(^2\\) \\(x_1\\) Number of bedrooms \\(x_2\\) Number of floors \\(x_3\\) Age of home in years \\(x_4\\) Price ($) in $1000‚Äôs \\(x_5\\) 2104 5 1 45 460 1416 3 2 40 232 1534 3 2 30 315 852 2 1 36 178 ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ Each feature, denoted as \\(x_j = j^{th}\\) contributes to predicting the house price. The number of features is represented by \\(n\\).\nTraining Examples #For each training example \\(i\\), the features are represented as a vector \\( \\vec{x}^{(i)} = \\), with \\(x_j^{(i)} = \\) being the value of feature \\(j\\) in the \\(i^{th}\\) example. For instance, the features of the second training example can be expressed as \\( \\vec{x}^{(2)} = [1416 \\ 3 \\ 2 \\ 40]\\).\nRepresentation of linear regression model for multiple variables #In a simple linear regression model with a single variable, the prediction formula is \\(f_{w,b}(x) = wx+b\\), where \\(w\\) is the weight, and \\(b\\) is the bias. In contrast, the linear regression model for multiple variables is formulated as:\n\\(f_{w,b}(x) = w_1x_1 + w_2x_2 + ‚Ä¶ + w_nx_n + b\\)\nHere, \\( \\vec{w} = [w_1 \\ w_2 \\ w_3 \\ ‚Ä¶ \\ w_n] \\) represents the weights for each feature, and \\(b\\) is the bias term.\nModel Equation #The equation for a linear regression model with multiple variables can be succinctly represented as:\n\\( f_{\\vec{w},b}(\\vec{x}) = \\vec{w} „Éª \\vec{x} + b \\)\nThis equation indicates that the prediction \\( f_{\\vec{w},b}(\\vec{x}) \\) is the dot product of the weights vector \\( \\vec{x} \\) and the features vector \\( \\vec{x} \\), plus the bias \\(b\\). By incorporating multiple features into the linear regression model, we can capture more complex relationships and patterns in the data, leading to more accurate predictions.\n","date":"18 February 2024","permalink":"/posts/22/","section":"Blog posts","summary":"\u003cp\u003eThis article explains what Linear Regression with Multiple Variables is on Machine Learning.\u003c/p\u003e","title":"Machine Learning - Linear Regression with Multiple Variables"},{"content":"This article explains what vectorization is on Machine Learning.\nParameters and Features in Linear Regression #In linear regression with multiple variables, we deal with vectors of parameters and features. Consider the following vectors:\nParameters vector \\( \\vec{w} \\) represents the weights: \\( \\vec{w} = [w_1 \\ w_2 \\ w_3]\\) Features vector \\( \\vec{x} \\) represents the input features: \\( \\vec{x} = [x_1 \\ x_2 \\ x_3]\\) Representation in Code #In programming, especially with Python and libraries like NumPy, we account for the base-0 index. For the vectors above, the code representation is as follows:\nimport numpy as np w = np.array([1.0, 2.5, -3.3]) # Weights b = 4 # Bias x = np.array([10, 20, 30]) # Features Linear Regression Model Without Vectorization #The prediction \\( f_{\\vec{w},b}(\\vec{x}) \\) can be calculated without vectorization, using a straightforward approach:\nf = w[0] * x[0] + w[1] * x[1] + w[2] * x[2] + b Or, for a more generalized form considering any number of features \\(n\\):\nf = 0 n = len(w) # Assuming w and x have the same length for j in range(0, n): f += w[j] * x[j] f += b Linear Regression Model With Vectorization #Vectorization allows for a more efficient calculation by leveraging NumPy\u0026rsquo;s dot product function:\nf = np.dot(w,x) + b Updating Parameters with Gradient Descent #Gradient descent is a method used to update the parameters \\( \\vec{w} \\) n order to minimize the cost function. Consider a gradient vector \\( \\vec{d} \\) and a learning rate of \\(0.1\\).\nWithout Vectorization\nThe update rule for each parameter \\(w_j\\) without vectorization would look like:\nfor j in range(0, 16): # Assuming there are 16 parameters and gradients w[j] = w[j] - 0.1 * d[j] With Vectorization\nVectorization simplifies the update process significantly:\nw = w - 0.1 * d ","date":"18 February 2024","permalink":"/posts/23/","section":"Blog posts","summary":"\u003cp\u003eThis article explains what vectorization is on Machine Learning.\u003c/p\u003e","title":"Machine Learning - Vectorization"},{"content":"","date":null,"permalink":"/tags/multiple-linear-regression/","section":"Tags","summary":"","title":"multiple linear regression"},{"content":"","date":null,"permalink":"/tags/vectorization/","section":"Tags","summary":"","title":"vectorization"},{"content":"This article explains what Supervised Learning is for Machine Learning.\nWhat is Supervised Learning? #Supervised learning is characterized by providing the learning algorithm with examples from which to learn. These examples include the correct outputs or labels \u0026lsquo;y\u0026rsquo; for given inputs \u0026lsquo;x\u0026rsquo;. By observing these correct input-output pairs, the algorithm learns to make reasonably accurate predictions for the output based only on the input.\nTypes of Supervised Learning #There are two main types of supervised learning:\nRegression #: In this type, the goal is to predict a number, and there are infinitely many possible numbers. For example, predicting the price of a house based on features like its size, location, and age.\nClassification #: In this type, the goal is to predict categories, and there are a small number of possible outputs. For example, determining whether an email is spam or not based on its content and sender.\n","date":"17 February 2024","permalink":"/posts/17/","section":"Blog posts","summary":"\u003cp\u003eThis article explains what Supervised Learning is for Machine Learning.\u003c/p\u003e","title":"Machine Learning - Supervised Learning"},{"content":"This article explains what Unsupervised Learning is for Machine Learning.\nWhat is Unsupervised Learning? #Unsupervised learning is a distinct type of machine learning that operates on a unique premise. Unlike supervised learning where data is already labeled, in unsupervised learning, the data only comes with input variables (x), and no corresponding output labels (y). This means the algorithm is not guided by a predefined outcome. Instead, it is tasked with discovering the inherent structure within the data, or to find interesting and insightful patterns in the unlabeled data.\nTypes of Unsupervised Learning #There are several key types of unsupervised learning methodologies that are commonly used:\nClustering #: Clustering is a technique that seeks to group similar data points together. The aim is to identify inherent groupings within the data. For instance, Google News employs clustering to group together similar news articles from different sources, DNA microarray data analysis uses clustering to group genes with similar expression patterns, and businesses use clustering for customer segmentation, grouping customers with similar buying behaviors.\nAnomaly Detection #: Anomaly detection is another important method in unsupervised learning. This technique is used to identify unusual data points in the dataset. These could be outliers or anomalies that deviate significantly from the rest of the data. Anomaly detection is particularly useful in fraud detection, network security, and fault detection.\nDimensionality Reduction #: This is a technique that seeks to compress data by reducing the number of random variables under consideration, using fewer essential variables. This not only helps in data compression, but also in improving the performance of machine learning models by removing noise and redundant data, and hence, making the learning process more efficient.\n","date":"17 February 2024","permalink":"/posts/18/","section":"Blog posts","summary":"\u003cp\u003eThis article explains what Unsupervised Learning is for Machine Learning.\u003c/p\u003e","title":"Machine Learning - Unsupervised Learning"},{"content":"","date":null,"permalink":"/tags/supervised-learning/","section":"Tags","summary":"","title":"supervised learning"},{"content":"","date":null,"permalink":"/tags/unsupervised-learning/","section":"Tags","summary":"","title":"unsupervised learning"},{"content":"","date":null,"permalink":"/tags/paired-t-test/","section":"Tags","summary":"","title":"paired t-test"},{"content":"","date":null,"permalink":"/tags/scipy/","section":"Tags","summary":"","title":"scipy"},{"content":"","date":null,"permalink":"/tags/statistics/","section":"Tags","summary":"","title":"statistics"},{"content":"This article explains the Paired Sample t-Test used in statistics.\nWe will also proceed with the Paired Sample t-Test using the Python Scipy library.\nPaired Sample t-Test #The Paired Sample t-Test is a statistical technique for comparing the means of two related groups. This method is typically applied when there are two measurements for the same group of subjects. The paired sample t-test is used to determine whether the difference in means between two related groups is statistically significant.\n1. Hypothesis Setting # H‚ÇÄ¬†: ùúáD¬†= 0¬†‚Üí Null Hypothesis¬†(ùúáùê∑¬†=¬†ùúá‚ÇÅ - ùúá‚ÇÇ) The difference in means before and after the experiment is 0. H‚ÇÅ¬†:¬†ùúáD¬†‚â† 0¬†‚Üí¬†Alternative Hypothesis The difference in means before and after the experiment is not 0. 2. Normality Test #If the sample size of the two groups is less than 30, a normality test must be conducted.\nIf the sample size of the two groups is 30 or more, it is assumed that normality is satisfied due to the Central Limit Theorem.\nIn Scipy, normality testing can be confirmed through the Shapiro-Wilk test. 4. Calculation of Paired Sample t-Statistic #The paired sample t-statistic is calculated using the means and standard deviations of the two groups.\n5. Decision/Conclusion #If the calculated t-statistic exceeds the critical value, the null hypothesis is rejected and the alternative hypothesis is accepted.\nOtherwise, the null hypothesis is not rejected.\nIf there is a statistically significant difference, it is concluded that there is a difference in means between the two groups.\nUsing Python Library Scipy #Below is how to proceed with the Paired Sample t-Test using the Python Scipy library.\nThe data we are dealing with includes, in student A\u0026rsquo;s class, there was a rumor that working out improves concentration, so A decided to compare before and after working out. A made 20 people work out, then had them take concentration measurement tests before and after the training.\nWe want to see if there is a significant difference in concentration before and after working out through a Paired Sample t-Test.\nThe hypothesis is as follows:\nNull Hypothesis¬†: The test averages before and after working out are the same.\nAlternative Hypothesis¬†:¬†The test averages before and after working out are not the same.\nThe significance level is set at 0.05.\nFirst, let\u0026rsquo;s load the data.\n\u0026gt;\u0026gt;\u0026gt; import pandas as pd \u0026gt;\u0026gt;\u0026gt; from scipy import stats \u0026gt;\u0026gt;\u0026gt; df = pd.read_csv(\u0026#34;./data/ch11_training_rel.csv\u0026#34;) \u0026gt;\u0026gt;\u0026gt; df.head() Ï†Ñ ÌõÑ 0 59 41 1 52 63 2 55 68 3 61 59 4 59 84 Next, let\u0026rsquo;s conduct a normality test.\n\u0026gt;\u0026gt;\u0026gt; a = stats.shapiro(df[\u0026#39;Before\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; b = stats.shapiro(df[\u0026#39;After\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; print(a, b) ShapiroResult(statistic=0.9670045375823975, pvalue=0.690794825553894) ShapiroResult(statistic=0.9786625504493713, pvalue=0.9156817197799683) Both results have a p-value greater than 0.05, indicating normality is satisfied.\nNext, we can calculate the t-statistic and p-value using ttest_rel in the Scipy library.\n\u0026gt;\u0026gt;\u0026gt; t_score, p_value = stats.ttest_rel(df[\u0026#39;Before\u0026#39;], df[\u0026#39;After\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; print(round(t_score, 4), round(p_value, 2)) -2.2042 0.04 Since the p-value is less than the significance level of 0.05, the null hypothesis (the averages before and after working out are the same) is rejected. Therefore, we can conclude that there is a significant difference in the average scores before and after working out.\n","date":"19 January 2024","permalink":"/posts/10/","section":"Blog posts","summary":"\u003cp\u003eThis article explains the Paired Sample t-Test used in statistics.\u003c/p\u003e","title":"Statistics - Paired t-Test"},{"content":"","date":null,"permalink":"/tags/ttest_rel/","section":"Tags","summary":"","title":"ttest_rel"},{"content":"","date":null,"permalink":"/tags/independent-samples-t-test/","section":"Tags","summary":"","title":"independent samples t-test"},{"content":"This article explains the Independent Samples t-Test used in statistics.\nWe will also proceed with the Independent Samples t-Test using the Python Scipy library.\nIndependent Samples t-Test #The Independent Samples t-Test is a statistical analysis technique used to test whether there is a statistically significant difference between the means of two independent samples. It is used to determine whether the difference in means between two groups occurred by chance or if it truly exists.\nThe main steps of the Independent Samples t-Test are as follows:\n1. Hypothesis Setting # H‚ÇÄ¬†:¬†ùúá‚ÇÅ¬†= ùúá‚ÇÇ¬†‚Üí Null Hypothesis The means of the two groups are the same. H‚ÇÅ¬†:¬†ùúá‚ÇÅ¬†‚â† ùúá‚ÇÇ¬†‚Üí Alternative Hypothesis The means of the two groups are different. 2. Normality Test #If the sample size of the two groups is less than 30, a normality test must be conducted.\nIf the sample size of the two groups is more than 30, it is assumed that normality is satisfied due to the Central Limit Theorem.\nIn Scipy, normality testing can be confirmed through the Shapiro-Wilk test. 3. Equality of Variances Test #If the data counts of the two groups are the same, it is assumed that the variances are equal.\nIf the data counts of the two groups are different, an equality of variances test can be performed to check if the variances are equal.\nIn Scipy, equality of variances testing can be confirmed through the Levene test. 4. Calculation of Independent Samples t-Statistic #The independent samples t-statistic is calculated using the means and standard deviations of the two groups.\n5. Decision/Conclusion #If the calculated t-statistic exceeds the critical value, the null hypothesis is rejected and the alternative hypothesis is accepted.\nOtherwise, the null hypothesis is not rejected.\nIf there is a statistically significant difference, it is concluded that there is a difference in means between the two groups.\nThe Independent Samples t-Test is useful for comparing the mean difference between two groups and can be applied in situations such as investigating differences between experimental and control groups or checking the effect between two conditions.\nUsing Python Library Scipy #Below is how to proceed with the Independent Samples t-Test using the Python Scipy library.\nThe data we are dealing with includes results from concentration tests received by class B, thinking that if strength training indeed has an effect on improving concentration, there might not be a difference in the average concentration test scores between his class with many humanities students, class A, and class B with many students who regularly do strength training.\nWe want to see if there is a significant difference in concentration between classes A and B through an Independent Samples t-Test.\nThe hypothesis is as follows:\nNull Hypothesis¬†: The means of classes A and B are the same.\nAlternative Hypothesis¬†:¬†The means of classes A and B are not the same.\nThe significance level is set at 0.05.\nLet\u0026rsquo;s first load the data.\n\u0026gt;\u0026gt;\u0026gt; import pandas as pd \u0026gt;\u0026gt;\u0026gt; from scipy import stats \u0026gt;\u0026gt;\u0026gt; df = pd.read_csv(\u0026#34;./data/ch11_training_ind.csv\u0026#34;) \u0026gt;\u0026gt;\u0026gt; df.head() A B 0 47 49 1 50 52 2 37 54 3 60 48 4 39 51 Next, let\u0026rsquo;s conduct a normality test.\n\u0026gt;\u0026gt;\u0026gt; a = stats.shapiro(df[\u0026#39;A\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; b = stats.shapiro(df[\u0026#39;B\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; print(a, b) ShapiroResult(statistic=0.9685943722724915, pvalue=0.7249553203582764) ShapiroResult(statistic=0.9730021357536316, pvalue=0.8165789842605591) Both results have a p-value greater than 0.05, satisfying normality.\nNext, since the data counts are the same for these data, we assume equality of variances, but if the groups\u0026rsquo; data counts differ, equality of variances needs to be tested, which can be confirmed through the Levene test as follows:\n\u0026gt;\u0026gt;\u0026gt; stats.levene(df[\u0026#39;A\u0026#39;], df[\u0026#39;B\u0026#39;]) LeveneResult(statistic=2.061573118077718, pvalue=0.15923550057222613) With a p-value of 0.159, the null hypothesis (the two groups\u0026rsquo; variances are not different) is accepted.\nNext, the t-statistic and p-value can be calculated using¬†ttest_ind in the Scipy library.\n\u0026gt;\u0026gt;\u0026gt; t, p = stats.ttest_ind(df[\u0026#39;A\u0026#39;], df[\u0026#39;B\u0026#39;], equal_var=False) # equal_var=False: Welch\u0026#39;s method \u0026gt;\u0026gt;\u0026gt; t, p (-1.760815724652471, 0.08695731107259362) Since the p-value is greater than the significance level of 0.05, the null hypothesis (the means of classes A and B are the same) is accepted. Therefore, it can be concluded that there is no significant difference in average scores between class A and class B.\n","date":"15 January 2024","permalink":"/posts/9/","section":"Blog posts","summary":"\u003cp\u003eThis article explains the Independent Samples t-Test used in statistics.\u003c/p\u003e","title":"Statistics - Independent Samples t-Test"},{"content":"","date":null,"permalink":"/tags/ttest_ind/","section":"Tags","summary":"","title":"ttest_ind"},{"content":"","date":null,"permalink":"/tags/one-sample-t-test/","section":"Tags","summary":"","title":"one sample t-test"},{"content":"This article explains the one-sample t-test used in statistics.\nFurthermore, we will proceed with the one-sample t-test using the Python Scipy library.\nOne Sample t-Test #The one-sample t-test is one of the hypothesis testing methods used in statistical analysis, utilized to test the mean of a single sample. It is commonly applied to check if the population mean is equal to a specific value.\nThe one-sample t-test consists of the following steps:\n1. Hypothesis Setting # H‚ÇÄ¬†:¬†ùúá = ùúá‚ÇÄ¬†‚Üí Null Hypothesis The population mean is equal to the sample mean. H‚ÇÅ¬†:¬†ùúá ‚â† ùúá‚ÇÄ¬†‚Üí Alternative Hypothesis The population mean is not equal to the sample mean. 2.¬†Sampling #Extract a sample from the population and calculate the mean of that sample.\n3.¬†Calculation of the Test Statistic #Calculate the t-statistic, which represents the difference between the sample mean and the expected mean based on the hypothesis.\n4.¬†Decision / Conclusion #If the calculated t-statistic falls within the rejection region, reject the null hypothesis and accept the alternative hypothesis.\nOtherwise, do not reject the null hypothesis.\nIn the case of a two-tailed test, the rejection regions are the symmetric ends of the t-distribution. If the null hypothesis is rejected, it is concluded that the sample is different from the population.\nConversely, if it is not rejected, it is concluded to be not statistically significant.\nUsing Python Library Scipy #Next, we will proceed with the one-sample t-test using the Python Scipy library.\nThe data we are dealing with here contains the circumference, height, and volume of 31 trees.\nWe want to see if the mean of this sample is consistent with the population mean through a one-sample t-test. The hypothesis is as follows:\nThe significance level is set at 0.05.\nHypothesis Testing\nNull Hypothesis¬†: The mean is 75.\nAlternative Hypothesis¬†: The mean is not 75.\nLet\u0026rsquo;s first load the data.\n\u0026gt;\u0026gt;\u0026gt; import pandas as pd \u0026gt;\u0026gt;\u0026gt; df = pd.read_csv(\u0026#34;./data/trees.csv\u0026#34;) \u0026gt;\u0026gt;\u0026gt; df.head() Girth Height Volume 0 8.3 70 10.3 1 8.6 65 10.3 2 8.8 63 10.2 3 10.5 72 16.4 4 10.7 81 18.8 Also, let\u0026rsquo;s calculate the mean of \u0026lsquo;Height\u0026rsquo;.\n\u0026gt;\u0026gt;\u0026gt; result = df[\u0026#39;Height\u0026#39;].mean() \u0026gt;\u0026gt;\u0026gt; round(result, 2) # (Rounded to the second decimal place) 76.0 Next, we will load the Scipy library for a one-sample t-test.\n\u0026gt;\u0026gt;\u0026gt; from scipy import stats The one-sample t-test uses ttest_1samp in Scipy.\nReference : https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_1samp.html\nThen, let\u0026rsquo;s calculate the test statistic for hypothesis testing.\n\u0026gt;\u0026gt;\u0026gt; from math import sqrt \u0026gt;\u0026gt;\u0026gt; t_score, p_value = stats.ttest_1samp(df[\u0026#39;Height\u0026#39;], popmean=75) \u0026gt;\u0026gt;\u0026gt; print(round(t_score, 2), round(p_value, 2)) 0.87 0.39 popmean is the same as the mean expected in the null hypothesis.\nAfter calculating the p-value of the above statistics (rounded to the fourth decimal place), let\u0026rsquo;s check whether to reject or not reject the null hypothesis under the significance level of 0.05.\n\u0026gt;\u0026gt;\u0026gt; print(round(p_value, 4)) 0.3892 \u0026gt;\u0026gt;\u0026gt; if p_value \u0026gt;= 0.05: print(\u0026#34;Accept\u0026#34;) else: print(\u0026#34;Reject\u0026#34;) Accept Therefore, we did not reject the null hypothesis that the sample mean is equal to the population mean.\n","date":"13 January 2024","permalink":"/posts/8/","section":"Blog posts","summary":"\u003cp\u003eThis article explains the one-sample t-test used in statistics.\u003c/p\u003e","title":"Statistics - One Sample t-Test"},{"content":"","date":null,"permalink":"/tags/regression-analysis/","section":"Tags","summary":"","title":"regression analysis"},{"content":"This article explains regression analysis in statistics.\nWhat is Regression Analysis? #Regression analysis is a statistical technique that estimates the effect of one or more independent variables on a dependent variable.\nVariables in Regression Analysis #The Dependent Variable (y) is the variable affected, also known as the Response Variable or Outcome Variable. It is the variable predicted in the model, influenced by other variables.\nThe Independent Variable (x) influences the outcome, also referred to as the Explanatory Variable or Predictor Variable. Independent variables affect the dependent variable and are used when constructing the prediction model.\nRegression Analysis Methods Based on the Number of Variables #The approach to regression analysis varies based on the number of variables.\nIf there is one independent variable, it is approached with simple linear regression. If there are two or more independent variables, multiple linear regression analysis is possible.\n1. Simple Linear Regression #This statistical technique estimates the effect of a single independent variable on a dependent variable. The regression line chosen is the one where the difference (residual) between predicted values and actual data is smallest. Among numerous lines, the regression line means the one with a smaller Residual Sum of Squares (RSS).\nThis is achieved through the¬†Ordinary Least Squares method.\nOrdinary Least Squares¬†: A method that creates a sum of squares based on measured values and finds the value that minimizes it, choosing the line with the smallest residual sum of squares.\n2. Multiple Linear Regression #This statistical technique estimates the effect of two or more independent variables on one dependent variable. It\u0026rsquo;s important to determine the¬†significance of regression coefficients in multiple linear regression analysis because the model needs to be confirmed with the combination of selected variables where all regression coefficients are statistically verified.\nThe significance of regression coefficients can be confirmed through the¬†regression coefficient t-statistics.\nMulticollinearity in Multiple Linear Regression #Multicollinearity refers to the phenomenon in regression analysis where there is a strong correlation between independent variables, occurring when one independent variable can be predicted from the others.\nMulticollinearity complicates the accurate estimation of each independent variable\u0026rsquo;s regression coefficient. Furthermore, it prevents the regression coefficients of each independent variable from correctly explaining their impact on the dependent variable.\nMethods to Test for Multicollinearity\nVariance Inflation Factor (VIF):\nThe Variance Inflation Factor indicates how much the variance of each independent variable has increased, judging that multicollinearity has risen if this value is large. It\u0026rsquo;s calculated as the variance ratio of linearly regressing each independent variable against the others. Generally, if the VIF is¬†greater than 4, it\u0026rsquo;s judged that¬†multicollinearity exists, and if¬†greater than 10, it\u0026rsquo;s considered to be a¬†serious problem.\nConsiderations in Regression Analysis #When conducting regression analysis, there are three main points to consider:\n1. Are the regression coefficients significant? #A regression coefficient is considered statistically significant if the¬†p-value of its t-statistic is¬†less than 0.05.This means the coefficient has a significant impact¬†on the dependent variable.\n2. How much explanatory power does the model have? #To check how much explanatory power the model has, the¬†Coefficient of Determination (ùëÖ¬≤) must be reviewed.\nCoefficient of Determination (ùëÖ¬≤) #The coefficient of determination is a value between 0 and 1, meaning the model explains the variation in the dependent variable well if it is closer to 1. A high coefficient of determination indicates a high predictive power of the model..\n3. Does the model fit the data well? #To determine if the model fits the data well, residuals are plotted, and regression diagnostics are performed. Residuals, the difference between actual values and the model\u0026rsquo;s predicted values, are visually reviewed to check how well the model fits the data. Ideally, residuals follow a normal distribution without any specific patterns or trends, and homoscedasticity of residuals must also be confirmed. Outliers or influential data points should be reviewed, and if necessary, removed or adjusted to check the model\u0026rsquo;s stability.\nThrough these reviews, the reliability of the regression analysis and the model\u0026rsquo;s fit can be assessed.\n","date":"12 January 2024","permalink":"/posts/7/","section":"Blog posts","summary":"\u003cp\u003eThis article explains regression analysis in statistics.\u003c/p\u003e","title":"Statistics - Regression Analysis"},{"content":"","date":null,"permalink":"/tags/alternative-hypothesis/","section":"Tags","summary":"","title":"alternative hypothesis"},{"content":"","date":null,"permalink":"/tags/hypothesis-testing/","section":"Tags","summary":"","title":"hypothesis testing"},{"content":"","date":null,"permalink":"/tags/null-hypothesis/","section":"Tags","summary":"","title":"null hypothesis"},{"content":"This article explains methods for integrating multiple datasets into one using the pandas library.\nIn statistics, a Hypothesis is a proposition that represents a claim or estimation and signifies an assumption/tentative conclusion about parameters.\nTypes of Hypotheses #Hypotheses can be represented in two forms as follows:\n1. Null Hypothesis (H0) #The null hypothesis represents¬†the hypothesis that there is no change or difference compared to the original, serving as a kind of \u0026lsquo;default\u0026rsquo; hypothesis.\nThe content of the null hypothesis varies depending on the test method. For example, a claim such as \u0026ldquo;the means of two groups are equal\u0026rdquo; can be set as a null hypothesis.\n2. Alternative Hypothesis (H1) #The alternative hypothesis is a claim opposing the null hypothesis, representing a hypothesis that one seeks to prove with concrete evidence through samples. For example, a claim such as \u0026ldquo;the means of two groups are different\u0026rdquo; can be set as an alternative hypothesis.\nThrough statistical testing, it is decided whether to reject the null hypothesis using the given data, or if there is no basis for rejection, the null hypothesis is not rejected. If clear evidence that the alternative hypothesis is true is found in the test results, the null hypothesis is¬†rejected.\nThe process of verifying the validity of such hypotheses in statistics is precisely hypothesis testing.\nHypothesis Testing #1. Setting the Hypothesis #The first step of hypothesis testing is setting the null hypothesis (H0) and the alternative hypothesis (H1) according to the problem being investigated.\n2. Sample Analysis #Next, a sample that can represent a part of the entire population is extracted. Data is collected and analyzed for this sample, thereby securing materials for statistical analysis.\n3. Testing the Validity of the Hypothesis #The hypothesis is tested using the collected data. It is decided whether to reject the null hypothesis or accept it because there is no basis for rejection, considering the level of significance and the test statistic.\nLevel of Significance\nThe level of significance is usually denoted by Œ±(alpha) and represents the criterion probability for rejecting the null hypothesis in experiments or surveys.\nThe commonly used level of significance is 0.05 (5%), but other values such as 0.01 or 0.10 can be used depending on the nature of the experiment or characteristics of the research.\nTest Statistic\nThe test statistic is an indicator that measures how well the collected data matches the hypothesis, a sample statistic necessary for parameter inference. The test statistic plays a crucial role in hypothesis testing and is used to decide on the rejection of the null hypothesis.\nIn the process of testing a hypothesis, there is always a possibility of statistical error, referred to as hypothesis testing error.\nHypothesis Testing Error #1. Type I Error #A Type I Error refers to the error of rejecting the null hypothesis when it is true. The cause of a Type I error is setting the significance level in statistical testing, which accidentally occurs when rejecting the null hypothesis at this level.\nExample: Incorrectly concluding there is an effect when there is actually none\n2. Type II Error #A Type II Error refers to the error of accepting the null hypothesis when the alternative hypothesis is true. The cause of a Type II error is insufficient test power, occurring when the actual effect is not detected due to a lack of power.\nExample: Failing to find the effect in statistical testing and adopting the null hypothesis even though there is actually an effect\n","date":"11 January 2024","permalink":"/posts/6/","section":"Blog posts","summary":"\u003cp\u003eThis article explains methods for integrating multiple datasets into one using the pandas library.\u003c/p\u003e","title":"Statistics - Hypothesis Testing (1)"},{"content":"","date":null,"permalink":"/tags/concat/","section":"Tags","summary":"","title":"concat"},{"content":"","date":null,"permalink":"/tags/join/","section":"Tags","summary":"","title":"join"},{"content":"","date":null,"permalink":"/tags/merge/","section":"Tags","summary":"","title":"merge"},{"content":"","date":null,"permalink":"/tags/pandas/","section":"Tags","summary":"","title":"pandas"},{"content":"This article explains methods for integrating multiple datasets into one using the pandas library.\nThere are several methods for integrating data, but this time we will cover¬†concat, join, merge.\nBefore we explain, let\u0026rsquo;s create two example data frames.\n\u0026gt;\u0026gt;\u0026gt; import pandas as pd \u0026gt;\u0026gt;\u0026gt; df1 = pd.DataFrame({ \u0026#39;Class1\u0026#39; : [95, 92, 98, 100], \u0026#39;Class2\u0026#39; : [91, 93, 97, 99] }) \u0026gt;\u0026gt;\u0026gt; df2 = pd.DataFrame({ \u0026#39;Class1\u0026#39; : [87, 89], \u0026#39;Class2\u0026#39; : [85, 90] }) Output of d1:\nClass1 Class2 0 87 85 1 89 90 Output of d2:\nClass1 Class2 0 95 91 1 92 93 2 98 97 3 100 99 1. concat #The concat¬†function of the pandas library is used to append data frames. This function can append multiple data frames in either row or column direction.\nLet\u0026rsquo;s append df1 and df2 to the result.\n\u0026gt;\u0026gt;\u0026gt; result = pd.concat([df1, df2]) \u0026gt;\u0026gt;\u0026gt; result Class1 Class2 0 95 91.0 1 92 93.0 2 98 97.0 3 100 99.0 4 87 85.0 5 89 90.0 6 96 NaN 7 83 NaN pd.concat([df1, df2]) appends¬†df1 and¬†df2 in the row direction. That is, the two data frames are connected vertically.\nNext, let\u0026rsquo;s integrate d3, which only has the Class1 column, into the result.\n\u0026gt;\u0026gt;\u0026gt; df3 = pd.DataFrame({ \u0026#39;Class1\u0026#39; : [96, 83] }) \u0026gt;\u0026gt;\u0026gt; pd.concat([result, df3], ignore_index=True) Class1 Class2 0 95 91.0 1 92 93.0 2 98 97.0 3 100 99.0 4 87 85.0 5 89 90.0 6 96 NaN 7 83 NaN Since the d3 data does not have the \u0026lsquo;Class2\u0026rsquo; column, it outputs blank values.\n2. join #The join¬†method of the pandas library is used to combine two data frames based on a specific column. It generally performs a role similar to SQL\u0026rsquo;s¬†JOIN¬†operation. Unlike¬†concat,¬†join integrates horizontally.\n\u0026gt;\u0026gt;\u0026gt; df4 = pd.DataFrame({ \u0026#39;Class3\u0026#39; : [93, 91, 95, 98] }) \u0026gt;\u0026gt;\u0026gt; df1.join(df4) Class1 Class2 Class3 a 95 91 93 b 92 93 91 c 98 97 95 d 100 99 98 It is also possible to output by arbitrarily setting the index as follows.\n\u0026gt;\u0026gt;\u0026gt; index_label = [\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;d\u0026#39;] \u0026gt;\u0026gt;\u0026gt; df1a = pd.DataFrame({\u0026#39;Class1\u0026#39;: [95, 92, 98, 100], \u0026#39;Class2\u0026#39;: [91, 93, 97, 99]}, index= index_label) \u0026gt;\u0026gt;\u0026gt; df4a = pd.DataFrame({\u0026#39;Class3\u0026#39;: [93, 91, 95, 98]}, index=index_label) \u0026gt;\u0026gt;\u0026gt; df1a.join(df4a) Class1 Class2 Class3 a 95 91 93 b 92 93 91 c 98 97 95 d 100 99 98 3. merge #The merge function of the pandas library is used to merge (integrate) two data frames based on a specific column. Using the merge function, you can combine data frames based on common columns between them.\n\u0026gt;\u0026gt;\u0026gt; df_A_B = pd.DataFrame({\u0026#39;Sales Month\u0026#39;: [\u0026#39;January\u0026#39;, \u0026#39;February\u0026#39;, \u0026#39;March\u0026#39;, \u0026#39;April\u0026#39;], \u0026#39;Product A\u0026#39;: [100, 150, 200, 130], \u0026#39;Product B\u0026#39;: [90, 110, 140, 170]}) \u0026gt;\u0026gt;\u0026gt; df_C_D = pd.DataFrame({\u0026#39;Sales Month\u0026#39;: [\u0026#39;January\u0026#39;, \u0026#39;February\u0026#39;, \u0026#39;March\u0026#39;, \u0026#39;April\u0026#39;], \u0026#39;Product C\u0026#39;: [112, 141, 203, 134], \u0026#39;Product D\u0026#39;: [90, 110, 140, 170]}) df_A_B\nSales Month Product A Product B 0 January 100 90 1 February 150 110 2 March 200 140 3 April 130 170 df_C_D\nSales Month Product C Product D 0 January 112 90 1 February 141 110 2 March 203 140 3 April 134 170 Use merge to merge the two data frames based on the \u0026lsquo;Sales Month\u0026rsquo; column. As a result, the two data frames are combined based on the \u0026lsquo;Sales Month\u0026rsquo; column, and the data is organized around the common columns.\n\u0026gt;\u0026gt;\u0026gt; df_A_B.merge(df_C_D) Sales Month Product A Product B Product C Product D 0 January 100 90 112 90 1 February 150 110 141 110 2 March 200 140 203 140 3 April 130 170 134 170 Let\u0026rsquo;s implement four different ways to combine two data frames using the merge method.\n\u0026gt;\u0026gt;\u0026gt; df_left = pd.DataFrame({\u0026#39;key\u0026#39;:[\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;C\u0026#39;], \u0026#39;left\u0026#39;: [1, 2, 3]}) \u0026gt;\u0026gt;\u0026gt; df_right = pd.DataFrame({\u0026#39;key\u0026#39;:[\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;D\u0026#39;], \u0026#39;right\u0026#39;: [4, 5, 6]}) 1.\n\u0026gt;\u0026gt;\u0026gt; df_left.merge(df_right, how=\u0026#39;left\u0026#39;, on = \u0026#39;key\u0026#39;) key left right 0 A 1 4.0 1 B 2 5.0 2 C 3 NaN Left join df_left and df_right based on the \u0026lsquo;key\u0026rsquo; column. A left join keeps all rows from the left data frame (df_left) and adds rows from the right data frame (df_right) that have a matching key value. If a matching key value is not present in the right data frame, it is filled with NaN.\n2.\n\u0026gt;\u0026gt;\u0026gt; df_left.merge(df_right, how=\u0026#39;right\u0026#39;, on = \u0026#39;key\u0026#39;) key left right 0 A 1.0 4 1 B 2.0 5 2 D NaN 6 Right join df_left and df_right based on the \u0026lsquo;key\u0026rsquo; column. A right join keeps all rows from the right data frame (df_right) and adds rows from the left data frame (df_left) that have a matching key value. If a matching key value is not present in the left data frame, it is filled with NaN.\n3.\n\u0026gt;\u0026gt;\u0026gt; df_left.merge(df_right, how=\u0026#39;outer\u0026#39;, on = \u0026#39;key\u0026#39;) key left right 0 A 1.0 4.0 1 B 2.0 5.0 2 D 3.0 NaN 3 D NaN 6.0 Outer join df_left and df_right based on the \u0026lsquo;key\u0026rsquo; column. An outer join includes all rows from both data frames, filling with NaN where a match is only present in one of the data frames.\n4.\n\u0026gt;\u0026gt;\u0026gt; df_left.merge(df_right, how=\u0026#39;inner\u0026#39;, on = \u0026#39;key\u0026#39;) key left right 0 A 1 4 1 B 2 5 Inner join df_left and df_right based on the \u0026lsquo;key\u0026rsquo; column. An inner join includes only rows that are common to both data frames. That is, it combines rows from both data frames that have the same \u0026lsquo;key\u0026rsquo; value.\n","date":"8 January 2024","permalink":"/posts/5/","section":"Blog posts","summary":"\u003cp\u003eThis article explains methods for integrating multiple datasets into one using the pandas library.\u003c/p\u003e","title":"Python - pandas Data Integration (concat, join, merge)"},{"content":"","date":null,"permalink":"/tags/matplotlib/","section":"Tags","summary":"","title":"matplotlib"},{"content":"This article explains how to insert text into graphs using the matplotlib library in Python.\nWhen outputting graphs using matplotlib, let\u0026rsquo;s try inserting text onto the graph as shown below. First, let\u0026rsquo;s implement it using arbitrary monthly sales data as shown below.\n\u0026gt;\u0026gt;\u0026gt; import calendar \u0026gt;\u0026gt;\u0026gt; month_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] \u0026gt;\u0026gt;\u0026gt; sold_list = [300, 400, 550, 900, 600, 960, 900, 910, 800, 700, 550, 450] \u0026gt;\u0026gt;\u0026gt; fig, ax = plt.subplots() \u0026gt;\u0026gt;\u0026gt; barcharts = ax.bar(month_list, sold_list) # calendar.month_name[1:13] ‚Üí Outputs January to December on xlabel \u0026gt;\u0026gt;\u0026gt; ax.set_xticks(month_list, calendar.month_name[1:13], rotation=90) \u0026gt;\u0026gt;\u0026gt; print(barcharts) When you run the code, a graph like the one shown below is output.\nNext, let\u0026rsquo;s insert the corresponding y-value above each bar.\nAfter obtaining the value for each bar and to insert it as text, we use get_height() to output the y-value and ax.text() to input text into the bar.\nReference:\nget_height()\nhttps://matplotlib.org/stable/api/_as_gen/matplotlib.patches.Rectangle.html\nax.text()\nhttps://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.text.html#matplotlib.axes.Axes.text\nThe completed code is as follows:\n\u0026gt;\u0026gt;\u0026gt; import calendar \u0026gt;\u0026gt;\u0026gt; month_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] \u0026gt;\u0026gt;\u0026gt; sold_list = [300, 400, 550, 900, 600, 960, 900, 910, 800, 700, 550, 450] \u0026gt;\u0026gt;\u0026gt; fig, ax = plt.subplots() \u0026gt;\u0026gt;\u0026gt; barcharts = ax.bar(month_list, sold_list) \u0026gt;\u0026gt;\u0026gt; ax.set_xticks(month_list, calendar.month_name[1:13], rotation=90) \u0026gt;\u0026gt;\u0026gt; print(barcharts) \u0026gt;\u0026gt;\u0026gt; for rect in barcharts: height = rect.get_height() ax.text(rect.get_x() + rect.get_width()/2., 1.002*height,\u0026#39;%d\u0026#39; % int(height), ha=\u0026#39;center\u0026#39;, va=\u0026#39;bottom\u0026#39;) \u0026gt;\u0026gt;\u0026gt; plt.show() rect.get_x() + rect.get_width()/2.\nCalculates the midpoint of the x position and width of each bar. This represents the center along the horizontal axis of the bar.\n1.002 * height\nheight is the current height of the bar, and 1.002*height is a correction value to place the text slightly above the height of the bar.\n\u0026rsquo;% d\u0026rsquo;¬†%¬†int(height)\nString formatting, i.e., inserting the height value for each bar (% follows d(integer) for insertion, int(height) converts the height of the bar to an integer)\nha=\u0026lsquo;center\u0026rsquo;\nAligns horizontally (along the x-axis) to the center.\nva=\u0026lsquo;bottom\u0026rsquo;\nAligns vertically (along the y-axis) to the bottom.\nTherefore, when you run the code, it will be output as shown below. ","date":"5 January 2024","permalink":"/posts/4/","section":"Blog posts","summary":"\u003cp\u003eThis article explains how to insert text into graphs using the matplotlib library in Python.\u003c/p\u003e","title":"Python - Matplotlib Text Insertion"},{"content":"","date":null,"permalink":"/tags/date_range/","section":"Tags","summary":"","title":"date_range"},{"content":"","date":null,"permalink":"/tags/iloc/","section":"Tags","summary":"","title":"iloc"},{"content":"","date":null,"permalink":"/tags/loc/","section":"Tags","summary":"","title":"loc"},{"content":"This article explains the date_range() function within the pandas library that can automatically generate dates.\nInstead of manually entering dates in the index of data, it is convenient to use pandas\u0026rsquo; date_range() when there are many values.\ndate_range() can be used as follows:\n\u0026gt;\u0026gt;\u0026gt; pd.date_range(start=\u0026#39;date\u0026#39;, end=\u0026#39;date\u0026#39;, freq=\u0026#39;frequency\u0026#39;) Let\u0026rsquo;s take a look at the following example.\n\u0026gt;\u0026gt;\u0026gt; pd.date_range(start=\u0026#39;2024/01/01\u0026#39;, end=\u0026#39;2024/01/07\u0026#39;) DatetimeIndex([\u0026#39;2024-01-01\u0026#39;, \u0026#39;2024-01-02\u0026#39;, \u0026#39;2024-01-03\u0026#39;, \u0026#39;2024-01-04\u0026#39;, \u0026#39;2024-01-05\u0026#39;, \u0026#39;2024-01-06\u0026#39;, \u0026#39;2024-01-07\u0026#39;], dtype=\u0026#39;datetime64[ns]\u0026#39;, freq=\u0026#39;D\u0026#39;) As seen in the output, you can confirm that it has output from the start date \u0026lsquo;2024/01/01\u0026rsquo; to the end date \u0026lsquo;2024/01/07\u0026rsquo;.\nLet\u0026rsquo;s look at another example.\n\u0026gt;\u0026gt;\u0026gt; pd.date_range(start=\u0026#39;2024-01-01 08:00\u0026#39;, periods = 4, freq = \u0026#39;H\u0026#39;) DatetimeIndex([\u0026#39;2024-01-01 08:00:00\u0026#39;, \u0026#39;2024-01-01 09:00:00\u0026#39;, \u0026#39;2024-01-01 10:00:00\u0026#39;, \u0026#39;2024-01-01 11:00:00\u0026#39;], dtype=\u0026#39;datetime64[ns]\u0026#39;, freq=\u0026#39;H\u0026#39;) From the result, you can see that starting from 08:00 on \u0026lsquo;2024-01-01\u0026rsquo;, four results are produced based on the frequency \u0026lsquo;H\u0026rsquo; (hourly).\nWhen setting the freq (frequency), various outputs are possible by referring to the Offset aliases in the link below.\nReference: https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases\n","date":"4 January 2024","permalink":"/posts/2/","section":"Blog posts","summary":"\u003cp\u003eThis article explains the date_range() function within the pandas library that can automatically generate dates.\u003c/p\u003e","title":"Python - pandas Automatic Date Generation with date_range"},{"content":"This article explains the features and differences between .loc() and .iloc(), which are necessary when handling DataFrames using the pandas¬†library in Python.\nFirst, for the explanation, we will use seaborn to fetch example data (iris).\n\u0026gt;\u0026gt;\u0026gt; import seaborn as sns \u0026gt;\u0026gt;\u0026gt; iris = sns.load_dataset(\u0026#39;iris\u0026#39;) \u0026gt;\u0026gt;\u0026gt; iris.head() First 5 rows of iris data 1. loc #loc is a method that selects data based on labels. It accesses data using the names (Labels) of rows and columns. In other words, it explicitly specifies the names of rows and columns to select data.\n# Select values with \u0026#39;virginica\u0026#39; in the column named \u0026#39;species\u0026#39; \u0026gt;\u0026gt;\u0026gt; iris.loc[iris[\u0026#39;species\u0026#39;] == \u0026#39;virginica\u0026#39;] First 5 rows containing \u0026lsquo;virginica\u0026rsquo; values among \u0026lsquo;species\u0026rsquo; 2. iloc #iloc is a method that selects data using integer-based indexes. It accesses data using the integer positions (indexes) of rows and columns. That is, it explicitly specifies the positions of the data in integers to select it.\n# Select the data in the first row and second column \u0026gt;\u0026gt;\u0026gt; iris.iloc[0, 1] 3.5 Data in \u0026lsquo;sepal_width\u0026rsquo;, which is the first row and second column 3. Differences between loc and iloc # Type of Index:\nSince loc uses labels, the names of rows and columns can be strings or other data types.\nSince iloc uses integers, the indexes of rows and columns must be integers.\nUsage:\nloc focuses on selecting data using explicit labels.\niloc focuses on selecting data using integer positions (indexes).\nExamples:\nExample of loc: df.loc[\u0026lsquo;A\u0026rsquo;, \u0026lsquo;column_name\u0026rsquo;]\nExample of iloc: df.iloc[0, 1]\nWhich method to use depends on the structure of the DataFrame and the user\u0026rsquo;s purpose. loc is useful when labels are clearly defined, while iloc is useful when integer-based indexes are used.\n","date":"4 January 2024","permalink":"/posts/3/","section":"Blog posts","summary":"\u003cp\u003eThis article explains the features and differences between \u003cstrong\u003e.loc()\u003c/strong\u003e and \u003cstrong\u003e.iloc()\u003c/strong\u003e, which are necessary when handling DataFrames using the \u003cem\u003e\u003cstrong\u003epandas\u003c/strong\u003e\u003c/em\u003e¬†library in Python.\u003c/p\u003e","title":"Python - pandas loc vs iloc"},{"content":"This article explains sequence types in Python, which are data types where values are connected continuously.\nWhat are Sequence Types? #Sequence types refer to data types where values are connected in a sequence.\nThe biggest feature of sequence types is that they provide common actions and functionalities.\nList [1, 2, 3, 4, 5] [1, 2, 3, 4, 5] Tuple (1, 2, 3, 4, 5) (1, 2, 3, 4, 5) Range range(5) 0, 1, 2, 3, 4 String \u0026lsquo;Hello\u0026rsquo; H e l l o As shown above, sequence types include lists, tuples, ranges, and strings, and also (bytes, bytearray).\nObjects created with sequence types are called sequence objects, and each value of the object is called an element.\nChecking for a Specific Value in a Sequence Object #To check whether a specific value exists within a sequence object, you can use in or not in as shown below.\n\u0026gt;\u0026gt;\u0026gt; a = \u0026#34;Hello\u0026#34; \u0026gt;\u0026gt;\u0026gt; \u0026#34;H\u0026#34; in a True \u0026gt;\u0026gt;\u0026gt; \u0026#34;A\u0026#34; in a False # not in checks if a specific value does not exist \u0026gt;\u0026gt;\u0026gt; \u0026#34;ell\u0026#34; not in a False \u0026gt;\u0026gt;\u0026gt; \u0026#34;Python\u0026#34; not in a True Using the in operator, if a specific value exists, it returns¬†True, otherwise¬†False. Conversely, using the not in operator, if a specific value does not exist it returns¬†True, otherwise¬†False.\nConnecting Sequence Objects #Sequence objects can be connected using the + operator.\n\u0026gt;\u0026gt;\u0026gt; a = [0, 1, 2, 3] \u0026gt;\u0026gt;\u0026gt; b = [4, 5, 6] \u0026gt;\u0026gt;\u0026gt; a + b [0, 1, 2, 3, 4, 5, 6] However, range cannot be connected using the + operator.\n\u0026gt;\u0026gt;\u0026gt; range(0, 5) + range(5, 10) TypeError Traceback (most recent call last) \u0026lt;ipython-input-7-88e74efcb3c0\u0026gt; in \u0026lt;cell line: 1\u0026gt;() ----\u0026gt; 1 range(0, 5) + range(5, 10) TypeError: unsupported operand type(s) for +: \u0026#39;range\u0026#39; and \u0026#39;range\u0026#39; Therefore, converting range to tuples or lists for connection is possible.\n\u0026gt;\u0026gt;\u0026gt; tuple(range(0, 5)) + tuple(range(5, 10)) (0, 1, 2, 3, 4, 5, 6, 7, 8, 9) \u0026gt;\u0026gt;\u0026gt; list(range(0, 5)) + list(range(5, 10)) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] Repeating Sequence Objects #Sequence objects can be repeated using the * operator.\nRepetition is possible with integer * sequence object or sequence * integer.\n\u0026gt;\u0026gt;\u0026gt; [0, 1, 2, 3] * 3 [0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3] However, similar to the method of connecting sequence objects, ranges cannot be repeated using the * operator.\n\u0026gt;\u0026gt;\u0026gt; range(0,10) * 3 TypeError Traceback (most recent call last) \u0026lt;ipython-input-11-824dcf3cff8f\u0026gt; in \u0026lt;cell line: 1\u0026gt;() ----\u0026gt; 1 range(0,10) * 3 TypeError: unsupported operand type(s) for *: \u0026#39;range\u0026#39; and \u0026#39;int\u0026#39; Therefore, converting to tuples or lists for repetition is possible.\nChecking the Number of Elements in a Sequence Object #The number of elements in a sequence object can be checked using the len function.\n# List \u0026gt;\u0026gt;\u0026gt; a = [1, 2, 3, 4, 5] \u0026gt;\u0026gt;\u0026gt; len(a) 5 # Tuple \u0026gt;\u0026gt;\u0026gt; b = (6, 7, 8, 9, 10) \u0026gt;\u0026gt;\u0026gt; len(b) 5 # Range len(range(0, 5, 2)) # -\u0026gt; Increasing by 2 from 0 to 5 gives 0, 2, 4 3 # String \u0026gt;\u0026gt;\u0026gt; c = \u0026#34;Hello, World\u0026#34; \u0026gt;\u0026gt;\u0026gt; len(c) 12 ","date":"2 January 2024","permalink":"/posts/1/","section":"Blog posts","summary":"\u003cp\u003eThis article explains sequence types in Python, which are data types where values are connected continuously.\u003c/p\u003e","title":"Python - Sequence types"},{"content":"","date":null,"permalink":"/tags/sequence-types/","section":"Tags","summary":"","title":"sequence types"},{"content":"This is the advanced tag. Just like other listing pages in Congo, you can add custom content to individual taxonomy terms and it will be displayed at the top of the term listing. üöÄ\nYou can also use these content pages to define Hugo metadata like titles and descriptions that will be used for SEO and other purposes.\n","date":null,"permalink":"/tags/advanced/","section":"Tags","summary":"This is the advanced tag.","title":"advanced"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":" This section contains all my projects. ü§ñ ","date":null,"permalink":"/projects/","section":"Projects","summary":"This section contains all my projects.","title":"Projects"}]