[{"content":"The purpose of this project is to assist these entrepreneurs by providing density and sales forecasts to aid in their business planning.\n","date":"2024년 3월 7일","permalink":"/ko/projects/convenience_insights/convenience_insights/","section":"Projects","summary":"\u003cp\u003eThe purpose of this project is to assist these entrepreneurs by providing density and sales forecasts to aid in their business planning.\u003c/p\u003e","title":"Convenience Insights"},{"content":"The purpose of this project is to develop a dashboard using Seoul real estate data to visualize real estate market trends and assist users in making real estate investment decisions.\n","date":"2024년 2월 8일","permalink":"/ko/projects/seoul_real_estate_insight/seoul_real_estate_insight/","section":"Projects","summary":"\u003cp\u003eThe purpose of this project is to develop a dashboard using Seoul real estate data to visualize real estate market trends and assist users in making real estate investment decisions.\u003c/p\u003e","title":"Seoul Real Estate Insight"},{"content":"","date":null,"permalink":"/ko/projects/","section":"Projects","summary":"","title":"Projects"},{"content":"","date":null,"permalink":"/ko/tags/seoul_real_estate_insight/","section":"Tags","summary":"","title":"Seoul_real_estate_insight"},{"content":" ","date":null,"permalink":"/ko/tags/","section":"Tags","summary":" ","title":"Tags"},{"content":"Welcome to my website! I\u0026rsquo;m really happy you stopped by.\n","date":null,"permalink":"/ko/","section":"Welcome to Congo!","summary":"Welcome to my website!","title":"Welcome to Congo!"},{"content":"","date":null,"permalink":"/ko/tags/feature-engineering/","section":"Tags","summary":"","title":"Feature Engineering"},{"content":"","date":null,"permalink":"/ko/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning"},{"content":"","date":null,"permalink":"/ko/tags/python/","section":"Tags","summary":"","title":"Python"},{"content":"","date":null,"permalink":"/ko/tags/%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D/","section":"Tags","summary":"","title":"머신 러닝"},{"content":"본 글에서는 원시 데이터(raw data)를 모델이 이해할 수 있는 형태로 변환하는 과정인 피처 엔지니어링(Feature Engineering)이 무엇이며, 왜 중요한지, 그리고 기본적인 피처 엔지니어링 기법에 대해 설명합니다.\n피처 엔지니어링이란? #피처 엔지니어링은 주어진 원시 데이터를 머신러닝 모델이 효과적으로 작동할 수 있는 피처(Feature) 혹은 변수로 변환하는 과정입니다. 이 과정에는 불필요한 정보의 제거, 유용한 정보의 추출 및 변환, 그리고 모델의 학습 과정에서 더 잘 작동할 수 있도록 데이터를 조정하는 작업이 포함됩니다.\n피처 엔지니어링의 중요성 #피처 엔지니어링은 머신러닝 모델의 성능을 크게 향상시킬 수 있습니다. 좋은 피처는 모델이 데이터에서 패턴을 더 잘 학습하게 하며, 결과적으로 예측의 정확도를 높입니다. 반면, 관련성이 낮거나 잘못된 피처는 모델의 성능을 저하시킬 수 있습니다. 따라서, 피처 엔지니어링은 모델의 성능을 최대화하는 데 필수적인 과정입니다.\n피처 엔지니어링 기법 #피처 엔지니어링에는 다양한 기법이 있으며, 아래는 가장 기본적인 몇 가지 기법입니다:\n결측치 처리 #데이터에 결측치가 있는 경우, 이를 처리하는 것이 중요합니다. 결측치를 평균, 중앙값, 최빈값 등으로 대체하거나, 결측치가 있는 행을 제거하는 방법이 있습니다.\n범주형 데이터 처리 #모델에 따라 범주형 데이터를 직접 처리할 수 없는 경우가 많습니다. 원-핫 인코딩(One-Hot Encoding), 레이블 인코딩(Label Encoding) 등의 방법을 사용하여 범주형 데이터를 숫자형 데이터로 변환할 수 있습니다.\n피처 스케일링 #다양한 피처의 스케일을 조정하여 모델이 피처를 공정하게 평가할 수 있도록 합니다. 표준화(Standardization)와 정규화(Normalization)가 이에 해당합니다.\n피처 선택(Feature Selection) #모델의 복잡도를 줄이고, 오버피팅을 방지하기 위해 중요한 피처만을 선택합니다. 통계적 방법, 모델 기반 방법 등이 있습니다.\n피처 생성(Feature Creation) #기존 피처를 조합하거나 변형하여 새로운 피처를 생성합니다. 이를 통해 모델이 데이터를 더 잘 이해할 수 있게 됩니다.\n인코딩 변환 #범주형 데이터는 텍스트로 표현된 데이터 카테고리를 의미합니다. 대부분의 머신러닝 알고리즘은 숫자형 데이터를 입력으로 받기 때문에, 이러한 범주형 데이터를 적절한 숫자형 포맷으로 변환하는 과정이 필수적입니다. 인코딩 변환에는 주로 두 가지 방법이 사용됩니다.\n원-핫 인코딩(One-Hot Encoding) #각 카테고리를 하나의 열로 변환하고, 해당 카테고리에 해당하는 경우에만 1의 값을, 그렇지 않은 경우에는 0의 값을 갖습니다. 이 방식은 범주 간의 순서나 중요도를 고려하지 않기 때문에, 모델이 각 범주를 동등하게 취급하도록 합니다.\n레이블 인코딩(Label Encoding) #각 범주형 데이터를 순서대로 번호를 매겨 숫자형으로 변환합니다. 예를 들어, \u0026lsquo;red\u0026rsquo;, \u0026lsquo;blue\u0026rsquo;, \u0026lsquo;green\u0026rsquo;을 각각 0, 1, 2로 변환할 수 있습니다. 레이블 인코딩은 범주의 개수만큼 차원을 증가시키지 않지만, 숫자의 크고 작음이 모델에 영향을 줄 수 있으므로 주의해야 합니다.\n스케일링 #피처 스케일링은 서로 다른 단위 또는 범위를 가진 데이터를 일정한 범위나 스케일로 통일하는 과정입니다. 이를 통해 모든 피처가 동등하게 모델에 영향을 미치도록 합니다. 스케일링에는 주로 두 가지 방법이 사용됩니다.\n표준화(Standardization): 데이터의 평균을 0, 표준편차를 1로 조정합니다. 이 방식은 데이터의 분포가 정규 분포를 따르지 않을 때 유용하며, 이상치에 덜 민감합니다. 정규화(Normalization): 데이터의 값을 0과 1 사이의 범위로 조정합니다. 가장 일반적인 방식은 최소값과 최대값을 사용하는 것이며, 모든 데이터 포인트가 동일한 스케일을 갖게 됩니다. ","date":"2024년 2월 22일","permalink":"/ko/posts/31/","section":"블로그 글","summary":"\u003cp\u003e본 글에서는 원시 데이터(raw data)를 모델이 이해할 수 있는 형태로 변환하는 과정인 피처 엔지니어링(Feature Engineering)이 무엇이며, 왜 중요한지, 그리고 기본적인 피처 엔지니어링 기법에 대해 설명합니다.\u003c/p\u003e","title":"머신러닝 - 피처 엔지니어링(Feature Engineering)"},{"content":" 여러 가지 주제의 블로그 글들을 기재하고 있습니다. 🧑🏻‍💻 ","date":null,"permalink":"/ko/posts/","section":"블로그 글","summary":"여러 가지 주제의 블로그 글들을 기재하고 있습니다.","title":"블로그 글"},{"content":"","date":null,"permalink":"/ko/tags/%ED%8C%8C%EC%9D%B4%EC%8D%AC/","section":"Tags","summary":"","title":"파이썬"},{"content":"","date":null,"permalink":"/ko/tags/%ED%94%BC%EC%B2%98-%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81/","section":"Tags","summary":"","title":"피처 엔지니어링"},{"content":"","date":null,"permalink":"/ko/tags/one-hot-encoding/","section":"Tags","summary":"","title":"One-Hot Encoding"},{"content":"본 글에서는 머신러닝에서 원-핫 인코딩이 무엇인지 설명합니다.\nOne-Hot Encoding #원-핫 인코딩은 피처 값의 유형에 따라 새로운 피처를 추가해 고유 값에 해당하는 칼럼에만 1을 표시하고 나머지 칼럼에는 0을 표시하는 방식입니다. 즉, 행 형태로 돼 있는 피처의 고유 값을 열 형태로 차원을 변환한 뒤, 고유 값에 해당하는 칼럼에만 1을 표시하고 나머지 칼럼에는 0을 표시합니다. 다음 그림에 원본 데이터를 원-핫 인코딩으로 변환하는 모습을 나타냈습니다.\nOriginal Data Product Category TV fridge microwave computer fan fan mixer mixer One-Hot Encoding Product Category_TV Product Category_computer Product Category_fan Product Category_fridge Product Category_microwave Product Category_mixer 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 먼저 원본 데이터는 8개의 레코드로 돼 있으며 고유 값은 [’TV’ ‘computer’ ‘fan’ ‘fridge’ ‘microwave’ ‘mixer’]로 모두 6개입니다. 앞의 레이블 인코딩 예제를 참조하면 TV가 0, computer 1, fan 2, fridge 3, microwave 4, mixer가 5로 인코딩돼 있음을 알 수 있습니다. 0부터 5까지 6개의 상품 분류 고유 값에 따라 상품분류 피처를 6개의 상품 분류 고유 값 피처로 변환합니다.즉, TV를 위한 상품 분류_TV, computer를 위한 상품 분류_computer, fan을 위한 상품 분류_fan, fridge를 위한 상품 분류_fridge, microwave를 위한 상품 분류_microwave, mixer를 위한 상품 분류_mixer 6개의 피처로 변환하는 것입니다. 그리고 해당 레코드의 상품 분류가 TV인 경우는 상품 분류_TV 피처에만 1을 입력하고 나머지 피처는 모두 0입니다. 마찬가지로 해당 레코드의 상품 분류가 fridge라면 상품 분류_fridge 피처에만 1을 입력하고 나머지 피처는 모두 0이 되는 것입니다. 즉, 해당 고유 값에 매칭되는 피처만 1이 되고 나머지 피처는 0을 입력하며, 이러한 특성으로 원-핫(여러 개의 속성 중 단 한 개의 속성만 1로 표시) 인코딩으로 명명하게 됐습니다.\nScikit-learn Implementation #원-핫 인코딩은 사이킷런에서 OneHotEncoder 클래스로 변환이 가능합니다. 단, LabelEncoder와 다르게 약간 주의할 점이 있습니다. 입력값으로 2차원 데이터가 필요하다는 것과, OneHotEncoder를 이용해 변환한 값이 희소 행렬(Sparse Matrix) 형태이므로 이를 다시 toarray( ) 메서드를 이용해 밀집 행렬(Dense Matrix)로 변환해야 한다는 것입니다. OneHotEncoder를 이용해 앞의 데이터를 원-핫 인코딩으로 변환해 보겠습니다.\nfrom sklearn.preprocessing import OneHotEncoder import numpy as np items=[\u0026#39;TV\u0026#39;, \u0026#39;fridge\u0026#39;, \u0026#39;microwave\u0026#39;, \u0026#39;computer\u0026#39;, \u0026#39;fan\u0026#39;, \u0026#39;fan\u0026#39;, \u0026#39;mixer\u0026#39;, \u0026#39;mixer\u0026#39;] # Converting to 2 dimension ndarray items =np.array(items).reshape(-1 , 1) # Applying One-Hot Encoding oh_encoder = OneHotEncoder() oh_encoder.fit(items) oh_labels = oh_encoder.transform(items) # The result of the conversion using OneHotEncoder is a sparse matrix, so we use toarray() to convert it into a dense matrix. print(\u0026#39;One-Hot Encoded data:\u0026#39;) print(oh_labels.toarray()) print(\u0026#39;Dimensions of One-Hot Encoded data:\u0026#39;) print(oh_labels.shape) One-Hot Encoded data: [[1. 0. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0.] [0. 0. 0. 0. 1. 0.] [0. 1. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0.] [0. 0. 1. 0. 0. 0.] [0. 0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 1.]] Dimensions of One-Hot Encoded data: (8, 6) 8개의 레코드와 1개의 칼럼을 가진 원본 데이터가 8개의 레코드와 6개의 칼럼을 가진 데이터로 변환됐습니다. TV가 0, computer 1, fan 2, fridge 3, microwave 4, mixer가 5로 인코딩됐으므로 첫번째칼럼이 TV, 두 번째 칼럼이 computer, 세 번째 칼럼이 fan, 네 번째 칼럼이 fridge, 다섯 번째 칼럼이 microwave, 여섯 번째 칼럼이 mixer를 나타냅니다. 따라서 원본 데이터의 첫 번째 레코드가 TV이므로 변환된 데이터의 첫번째 레코드의 첫번째 칼럼이 1이고 나머지칼럼은 모두 0이됩니다. 위 예제 코드의 변환절차는 다음 그림과 같이 정리할 수 있습니다.\nOriginal Data Product Category Price TV 1,000,000 fridge 1,500,000 microwave 200,000 computer 800,000 fan 100,000 fan 100,000 mixer 50,000 mixer 50,000 ↓\nOriginal Data Product Category Price 0 1,000,000 3 1,500,000 4 200,000 1 800,000 2 100,000 2 100,000 5 50,000 5 50,000 ↓\nOne-Hot Encoding Product Category_TV Product Category_computer Product Category_fan Product Category_fridge Product Category_microwave Product Category_mixer Price 1 0 0 0 0 0 1,000,000 0 0 0 1 0 0 1,500,000 0 0 0 0 1 0 200,000 0 1 0 0 0 0 800,000 0 0 1 0 0 0 100,000 0 0 1 0 0 0 100,000 0 0 0 0 0 1 50,000 0 0 0 0 0 1 50,000 판다스에는 원-핫 인코딩을 더 쉽게 지원하는 API가 있습니다. get_dummies( ) 를 이용하면 됩니다. 사이킷런의 OneHotEncoder와 다르게 문자열 카태고리 값을 숫자 형으로 변환할 필요 없이 바로 변환할 수 있습니다.\nimport pandas as pd df = pd.DataFrame({\u0026#39;item\u0026#39;:[\u0026#39;TV\u0026#39;, \u0026#39;fridge\u0026#39;, \u0026#39;microwave\u0026#39;, \u0026#39;computer\u0026#39;, \u0026#39;fan\u0026#39;, \u0026#39;fan\u0026#39;, \u0026#39;mixer\u0026#39;, \u0026#39;mixer\u0026#39;]}) pd.get_dummies(df) item_TV item_computer item_fan item_fridge item_microwave item_mixer 0 True False False False False False 1 False False False True False False 2 False False False False True False 3 False True False False False False 4 False False True False False False 5 False False True False False False 6 False False False False False True 7 False False False False False True get_dummies( ) 를 이용하면 숫자형 값으로 변환 없이도 바로 변환이 가능함을 알 수 있습니다.\n","date":"2024년 2월 21일","permalink":"/ko/posts/30/","section":"블로그 글","summary":"\u003cp\u003e본 글에서는 머신러닝에서 원-핫 인코딩이 무엇인지 설명합니다.\u003c/p\u003e","title":"머신러닝 - 원-핫 인코딩(One-Hot Encoding)"},{"content":"","date":null,"permalink":"/ko/tags/random-search/","section":"Tags","summary":"","title":"Random Search"},{"content":"","date":null,"permalink":"/ko/tags/%EB%9E%9C%EB%8D%A4-%EC%84%9C%EC%B9%98/","section":"Tags","summary":"","title":"랜덤 서치"},{"content":"본 글에서는 머신러닝에서 모델의 성능을 극대화하기 위한 하이퍼파라미터 튜닝 중 하나인 랜덤 서치(Random Search)의 개념을 설명하고, Scikit-learn 라이브러리를 사용한 구현 예를 함께 소개하겠습니다.\n랜덤 서치란? #랜덤 서치는 하이퍼파라미터의 최적 조합을 찾기 위해 주어진 파라미터 공간에서 무작위로 선택된 조합을 평가하는 방법입니다. 그리드 서치(Grid Search)가 정해진 파라미터의 모든 조합을 체계적으로 탐색하는 것과 달리, 랜덤 서치는 탐색 공간에서 랜덤하게 조합을 선택하여 평가합니다. 이 방법은 특히 하이퍼파라미터의 차원이 높거나, 탐색 공간이 클 때 유용하며, 종종 더 적은 시간 내에 비슷하거나 더 나은 결과를 도출할 수 있습니다.\n주요 파라미터 #Scikit-learn에서 RandomizedSearchCV 클래스를 통해 랜덤 서치를 구현할 수 있습니다. 주요 파라미터는 다음과 같습니다:\nestimator: 최적화할 모델을 지정합니다. 예를 들어, RandomForestClassifier(), SVC() 등입니다. param_distributions: 탐색할 파라미터 공간을 지정합니다. 각 파라미터에 대해 연속 분포를 지정하거나 리스트를 제공할 수 있습니다. n_iter: 랜덤하게 선택할 파라미터 설정의 수를 지정합니다. 이 값이 클수록 더 많은 조합을 탐색하지만, 계산 시간도 증가합니다. scoring: 모델의 성능을 평가할 기준을 지정합니다. 예를 들어, \u0026lsquo;accuracy\u0026rsquo;, \u0026lsquo;f1\u0026rsquo; 등입니다. cv: 교차 검증 분할 전략을 지정합니다. 정수 값을 입력하면 해당 값으로 k-폴드 교차 검증을 수행합니다. random_state: 결과의 재현 가능성을 위해 난수 생성기의 시드 값을 지정합니다. RandomizedSearchCV 구현 코드 #아래는 RandomizedSearchCV를 사용하여 분류기의 최적 하이퍼파라미터를 찾는 예제입니다. 아래 코드에서는 서포트 벡터 머신(SVM)을 사용합니다.\n\u0026gt;\u0026gt;\u0026gt; from sklearn.model_selection import RandomizedSearchCV \u0026gt;\u0026gt;\u0026gt; from sklearn.svm import SVC \u0026gt;\u0026gt;\u0026gt; from sklearn.datasets import load_iris \u0026gt;\u0026gt;\u0026gt; from sklearn.model_selection import train_test_split \u0026gt;\u0026gt;\u0026gt; import scipy.stats as stats # 데이터셋 로드 \u0026gt;\u0026gt;\u0026gt; iris = load_iris() \u0026gt;\u0026gt;\u0026gt; X, y = iris.data, iris.target # 데이터셋을 훈련 세트와 테스트 세트로 분할 \u0026gt;\u0026gt;\u0026gt; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 서포트 벡터 머신 인스턴스화 \u0026gt;\u0026gt;\u0026gt; svc = SVC() # 탐색할 하이퍼파라미터 공간 정의 \u0026gt;\u0026gt;\u0026gt; param_distributions = { \u0026#39;C\u0026#39;: stats.uniform(0.1, 1000), \u0026#39;gamma\u0026#39;: stats.uniform(0.0001, 0.1), \u0026#39;kernel\u0026#39;: [\u0026#39;linear\u0026#39;, \u0026#39;rbf\u0026#39;] } # RandomizedSearchCV 인스턴스화 \u0026gt;\u0026gt;\u0026gt; random_search.fit(X_train, y_train) # 최적의 파라미터와 최고 정확도 출력 \u0026gt;\u0026gt;\u0026gt; print(\u0026#34;Best parameters:\u0026#34;, random_search.best_params_) \u0026gt;\u0026gt;\u0026gt; print(\u0026#34;Best cross-validation score: {:.2f}\u0026#34;.format(random_search.best_score_)) # 테스트 세트에서 성능 평가 \u0026gt;\u0026gt;\u0026gt; accuracy = random_search.score(X_test, y_test) \u0026gt;\u0026gt;\u0026gt; print(\u0026#34;Test set accuracy: {:.2f}\u0026#34;.format(accuracy)) 이 코드는 SVC 모델에 대해 지정된 C, gamma, kernel 하이퍼파라미터의 랜덤 조합을 탐색하여 최적의 조합을 찾습니다. RandomizedSearchCV는 특히 탐색 공간이 넓을 때 그리드 서치보다 더 효과적인 방법일 수 있습니다.\n","date":"2024년 2월 20일","permalink":"/ko/posts/28/","section":"블로그 글","summary":"\u003cp\u003e본 글에서는 머신러닝에서 모델의 성능을 극대화하기 위한 하이퍼파라미터 튜닝 중 하나인 랜덤 서치(Random Search)의 개념을 설명하고, Scikit-learn 라이브러리를 사용한 구현 예를 함께 소개하겠습니다.\u003c/p\u003e","title":"머신러닝 - 랜덤 서치(Random Search)"},{"content":"","date":null,"permalink":"/ko/tags/grid-search/","section":"Tags","summary":"","title":"Grid Search"},{"content":"","date":null,"permalink":"/ko/tags/%EA%B7%B8%EB%A6%AC%EB%93%9C-%EC%84%9C%EC%B9%98/","section":"Tags","summary":"","title":"그리드 서치"},{"content":"머신러닝에서 모델의 최적의 파라미터를 찾기 위한 효율적인 방법 중 하나는 그리드 서치(Grid Search)입니다. 본 글에서는 그리드 서치가 무엇이며, 어떻게 작동하는지, 그리고 언제 사용해야 하는지에 대해 설명합니다.\n그리드 서치란? #그리드 서치는 머신러닝 모델의 하이퍼파라미터를 최적화하기 위한 방법 중 하나입니다. 이 방법은 지정된 하이퍼파라미터의 모든 조합을 시험해보며, 가장 좋은 성능을 내는 파라미터 조합을 찾습니다. 각각의 파라미터 조합에 대해 교차 검증을 수행하여 모델의 성능을 평가하며, 이 과정을 통해 최적의 모델을 선택할 수 있습니다.\n작동 원리 #그리드 서치는 먼저 사용자가 지정한 하이퍼파라미터의 범위나 리스트를 입력 받습니다. 예를 들어, 결정 트리 분류기에 대한 그리드 서치를 수행한다고 가정해봅시다. 사용자는 트리의 깊이(depth), 분할을 위한 최소 샘플 수(min_samples_split) 등의 하이퍼파라미터 범위를 설정합니다. 그리드 서치는 이 범위 내의 모든 가능한 조합에 대해 모델을 학습시키고, 교차 검증을 사용하여 각 조합의 성능을 평가합니다. 성능 평가 방법으로는 보통 정확도, 정밀도, 재현율, F1 점수 등이 사용됩니다. 평가가 끝난 후, 가장 성능이 좋은 파라미터 조합이 선택됩니다.\n장점과 단점 #장점:\n사용하기 쉽고 이해하기 쉬움 모든 가능한 조합을 탐색하기 때문에 최적의 조합을 찾을 가능성이 높음 단점:\n계산 비용이 매우 높음. 파라미터의 수와 범위가 커질수록 필요한 계산량이 기하급수적으로 증가 최적의 조합을 찾는데 시간이 오래 걸림 사용 용도 #그리드 서치는 파라미터의 범위가 상대적으로 작고, 모델의 학습 시간이 짧은 경우에 적합합니다. 또한, 최적의 하이퍼파라미터 조합을 찾는 것이 중요하고, 계산 자원이 충분한 경우에 사용하는 것이 좋습니다. 그러나 파라미터 공간이 매우 크거나, 학습 시간이 매우 긴 모델의 경우 랜덤 서치(Random Search), 베이지안 최적화 같은 다른 하이퍼파라미터 최적화 기법을 고려하는 것이 좋습니다.\nGridSearchCV #GridSearchCV는 사이킷런(sklearn) 라이브러리의 모델 선택 모듈에 포함된 클래스로, 주어진 모델의 하이퍼파라미터 공간을 교차 검증을 통해 탐색하며 최적의 파라미터를 찾는 데 사용됩니다. 이 클래스의 생성자에 전달할 수 있는 주요 파라미터는 다음과 같습니다:\n주요 파라미터 설명 # estimator: 최적화할 모델. 예를 들어, RandomForestClassifier(), SVC() 등 사이킷런의 추정기(estimator) 객체가 될 수 있습니다. param_grid: 탐색할 파라미터의 딕셔너리. 예를 들어, {\u0026rsquo;n_estimators\u0026rsquo;: [100, 200], \u0026lsquo;max_features\u0026rsquo;: [\u0026lsquo;auto\u0026rsquo;, \u0026lsquo;sqrt\u0026rsquo;]}과 같이 설정할 수 있으며, 이는 n_estimators와 max_features 파라미터에 대해 각각 [100, 200]과 [\u0026lsquo;auto\u0026rsquo;, \u0026lsquo;sqrt\u0026rsquo;]의 값을 탐색하라는 의미입니다. scoring: 모델의 성능을 평가할 기준. 문자열로 지정되며, 예를 들어 \u0026lsquo;accuracy\u0026rsquo;, \u0026lsquo;f1\u0026rsquo; 등이 사용될 수 있습니다. 사이킷런에서 사전 정의된 다른 scoring 옵션을 사용할 수도 있습니다. cv: 교차 검증 분할 전략. 예를 들어, 5는 5-폴드 교차 검증을 의미합니다. KFold, StratifiedKFold 등의 사이킷런의 분할기(splitter) 객체를 직접 전달할 수도 있습니다. refit: 최적의 파라미터를 찾은 후, 전체 데이터셋에 대해 모델을 다시 학습시킬지 여부를 결정합니다. 기본값은 True로, 최적의 파라미터로 전체 데이터셋에 대해 모델을 학습시킵니다. GridSearchCV 구현 #다음은 Scikit-learn의 GridSearchCV를 사용하여 분류기의 최적의 하이퍼파라미터를 찾는 간단한 예제입니다. 여기서는 결정 트리 분류기(DecisionTreeClassifier)를 사용합니다.\n\u0026gt;\u0026gt;\u0026gt; from sklearn.model_selection import GridSearchCV \u0026gt;\u0026gt;\u0026gt; from sklearn.tree import DecisionTreeClassifier \u0026gt;\u0026gt;\u0026gt; from sklearn.datasets import load_iris \u0026gt;\u0026gt;\u0026gt; from sklearn.model_selection import train_test_split # 데이터 로드 \u0026gt;\u0026gt;\u0026gt; iris = load_iris() \u0026gt;\u0026gt;\u0026gt; X = iris.data \u0026gt;\u0026gt;\u0026gt; y = iris.target # 훈련 세트와 테스트 세트 분리 \u0026gt;\u0026gt;\u0026gt; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 모델 설정 \u0026gt;\u0026gt;\u0026gt; estimator = DecisionTreeClassifier() # 탐색할 파라미터 그리드 설정 \u0026gt;\u0026gt;\u0026gt; param_grid = { \u0026#39;max_depth\u0026#39;: [None, 2, 4, 6, 8], \u0026#39;min_samples_split\u0026#39;: [2, 5, 10], \u0026#39;min_samples_leaf\u0026#39;: [1, 2, 4] } # GridSearchCV 설정 \u0026gt;\u0026gt;\u0026gt; grid_search = GridSearchCV(estimator=estimator, param_grid=param_grid, scoring=\u0026#39;accuracy\u0026#39;, cv=5, refit=True) # 그리드 서치 수행 \u0026gt;\u0026gt;\u0026gt; grid_search.fit(X_train, y_train) # 최적 파라미터와 최고 점수 출력 \u0026gt;\u0026gt;\u0026gt; print(\u0026#34;Best parameters:\u0026#34;, grid_search.best_params_) \u0026gt;\u0026gt;\u0026gt; print(\u0026#34;Best score:\u0026#34;, grid_search.best_score_) # 테스트 데이터로 성능 평가 \u0026gt;\u0026gt;\u0026gt; test_accuracy = grid_search.score(X_test, y_test) \u0026gt;\u0026gt;\u0026gt; print(\u0026#34;Test accuracy:\u0026#34;, test_accuracy) ","date":"2024년 2월 19일","permalink":"/ko/posts/27/","section":"블로그 글","summary":"\u003cp\u003e머신러닝에서 모델의 최적의 파라미터를 찾기 위한 효율적인 방법 중 하나는 그리드 서치(Grid Search)입니다. 본 글에서는 그리드 서치가 무엇이며, 어떻게 작동하는지, 그리고 언제 사용해야 하는지에 대해 설명합니다.\u003c/p\u003e","title":"머신 러닝 - 그리드 서치(Grid Search)"},{"content":"","date":null,"permalink":"/ko/tags/.env/","section":"Tags","summary":"","title":".Env"},{"content":"본 글은 .env 파일을 통해 환경 변수를 숨기는 방법을 알아보도록 하겠습니다.\n저자는 python 내 python-dotenv를 활용하여 .env 파일을 숨기도록 하겠습니다.\npython-dotenv 설치하기 #pip install python-dotenv 코드를 통해 python-dotenv 를 설치합니다.\n각종 환경 변수 정리 #API 키 등과 같이 Github에 업로드하기 민감한 각종 환경 변수를 .env 파일에 정리합니다.\n.env 파일 활성화하기 #from dotenv import load_dotenv import os # .env 파일 활성화 load_dotenv() SERVICE_KEY = os.getenv(\u0026#39;SERVICE_KEY\u0026#39;) .gitignore 파일 확인하기 #보통 .gitignore 파일에 기본적으로 .env 이 있지만 혹시 모르니 Github에 push 하기 전에 확인해봅니다. .env 파일이 정상적으로 있으니 git add -\u0026gt; commit -\u0026gt; push를 진행하면 됩니다.\n만약 .env 파일이 아닌 다른 파일에 변수들을 저장했다면, .gitignore 에 명시해주어야 정상적으로 Github에 업로드되지 않습니다.\ngit push 하기 #git push 한 후, Github를 확인합니다.\n.env 파일이 업로드되지 않은 것이 확인 가능하면, 정상적으로 처리가 된 것입니다.\n","date":"2024년 2월 1일","permalink":"/ko/posts/12/","section":"블로그 글","summary":"\u003cp\u003e본 글은 \u003ccode\u003e.env\u003c/code\u003e 파일을 통해 환경 변수를 숨기는 방법을 알아보도록 하겠습니다.\u003c/p\u003e","title":".env로 환경 변수 숨기기"},{"content":"","date":null,"permalink":"/ko/tags/%ED%99%98%EA%B2%BD%EB%B3%80%EC%88%98/","section":"Tags","summary":"","title":"환경변수"},{"content":"","date":null,"permalink":"/ko/tags/django/","section":"Tags","summary":"","title":"Django"},{"content":"본 글은 django를 활용하여 여러가지 항목들을 구현하기 위해 작성되었습니다.\n본인의 깃허브로 접속합니다.\nhttps://github.com/{사용자 아이디} 다음으로, \u0026ldquo;Repositories” 클릭합니다. 다음, \u0026ldquo;New\u0026rdquo; 클릭합니다.\n\u0026ldquo;Repository name\u0026rdquo; 클릭합니다.\n\u0026ldquo;pyburger\u0026rdquo; 작성\n본인은 이미 pyburger를 생성하였기 때문에 위와 같은 메시지가 나오지만,새로 생성할 경우 정상적으로 표시가 될 것입니다.\n\u0026ldquo;Add a README file\u0026rdquo; 체크를 취소합니다.\n.gitignore 파일은 github가 아닌 터미널에서 생성하고 싶기 때문에 \u0026ldquo;.gitignore template: None\u0026quot;로 유지하겠습니다.\n\u0026ldquo;Create repository\u0026rdquo; 클릭하여 생성합니다.\n다음으로는 본인의 컴퓨터의 원하는 디렉토리에 ‘pyburger’ 폴더 생성합니다.\n터미널을 실행 후 pyburger 디렉토리 설정 후 아래의 코드를 모두 작성하여 실행합니다.\n모두 다 작성하면 아래와 같이 나옵니다.\n다음으로는 .gitignore 파일을 생성하도록 하겠습니다.\n우선, https://www.toptal.com/developers/gitignore 에 접속한 후, \u0026ldquo;Python\u0026rdquo; 입력합니다.\n\u0026ldquo;Create\u0026rdquo; 클릭합니다.\n연결된 페이지에 있는 전체를 복사합니다.\n선호하는 IDE에 접속한 후 .gitignore 파일을 생성한 후 전부 붙여넣기하여 저장합니다.\n다음으로는 가상환경을 설정한 후 django를 설치하도록 하겠습니다.\n가상환경 설정은 virtualenv를 활용했습니다.\nvirtualenv venv #venv 대신 원하는 이름으로 변경 가능 source venv/bin/activate #생성한 가상환경 활성화 가상환경에 접속이 완료가 되었다면, django 설치를 하도록 하겠습니다.\npip install ‘django\u0026lt;5’ django-admin --version → 4.2.9 # 설치된 django 버전 확인 django-admin startproject config . # 새로운 프로젝트 생성 ls # 파일이 생성되었는지 확인 생성한 모든 파일을 github에 푸시하도록 하겠습니다.\ngit add . git commit -m “UPLOADED: new files” git push 정상적으로 작성이 완료되었으면 아래와 같이 출력이 됩니다.\n다음 글에 이어집니다.\n","date":"2024년 1월 24일","permalink":"/ko/posts/11/","section":"블로그 글","summary":"\u003cp\u003e본 글은 django를 활용하여 여러가지 항목들을 구현하기 위해 작성되었습니다.\u003c/p\u003e","title":"Django - pyburger 프로젝트 (1)"},{"content":"","date":null,"permalink":"/ko/tags/git/","section":"Tags","summary":"","title":"Git"},{"content":"","date":null,"permalink":"/ko/tags/github/","section":"Tags","summary":"","title":"Github"},{"content":"","date":null,"permalink":"/ko/tags/paired-t-test/","section":"Tags","summary":"","title":"Paired T-Test"},{"content":"","date":null,"permalink":"/ko/tags/scipy/","section":"Tags","summary":"","title":"Scipy"},{"content":"","date":null,"permalink":"/ko/tags/ttest_rel/","section":"Tags","summary":"","title":"Ttest_rel"},{"content":"","date":null,"permalink":"/ko/tags/%EB%8C%80%EC%9D%91%ED%91%9C%EB%B3%B8-t-%EA%B2%80%EC%A0%95/","section":"Tags","summary":"","title":"대응표본 T 검정"},{"content":"","date":null,"permalink":"/ko/tags/%ED%86%B5%EA%B3%84/","section":"Tags","summary":"","title":"통계"},{"content":"본 글은 통계학에서 사용되는 대응표본 T 검정을 설명하기 위해 작성되었습니다.\n금번에도 파이썬 라이브러리 Scipy를 활용하여 대응표본 T 검정을 진행하도록 하겠습니다.\n대응표본 T 검정 #대응표본 T 검정이란 두 관련된 집단 간의 평균을 비교하는 통계적 기법입니다. 이 방법은 일반적으로 동일한 피험자 집단에 대해 두 가지 측정값이 있는 경우에 적용됩니다. 대응표본 t-검정은 두 관련된 집단 간의 평균 차이가 통계적으로 유의한지 여부를 판단하는 데 사용됩니다.\n1. 가설 설정 # H₀ : 𝜇D = 0 → 귀무가설 (𝜇𝐷 = 𝜇₁ - 𝜇₂) 실험 전과 후의 평균의 차이는 0이다. H₁ : 𝜇D ≠ 0 → 대립가설 실험 전과 후의 평균의 차이는 0이 아니다. 2. 정규성 검정 #두 그룹의 표본 수가 30개 이하일 경우, 정규성 검정을 실시해야 합니다.\n두 그룹의 표본 수가 30개 이상일 경우, 중심극한정리에 의해 정규성을 만족했다고 가정합니다.\nScipy에서 정규성 검정은 Shapiro-Wilk 검정을 통해서 확인 가능합니다. 4. 대응표본 T 통계량 계산 #두 그룹의 평균과 표준편차를 사용하여 대응표본 T 통계량을 계산합니다.\n5. 결정/결론 #계산된 T 통계량이 임계값을 초과하면 귀무가설을 기각하고 대립가설을 채택합니다.\n그렇지 않으면 귀무가설을 기각하지 않습니다.\n통계적으로 유의한 차이가 있다면, 두 그룹 간에 평균 차이가 존재한다고 결론 내립니다.\n파이썬 라이브러리 Scipy 활용 방법 #다음은 파이썬 Scipy 라이브러리를 활용하여 대응표본 T 검정을 진행하도록 하겠습니다. 책 누구나 파이썬 통계분석의 챕터 11의 데이터 파일 ch11_training_ind.csv을 사용하였습니다.\n책은 아래 링크를 통해 구입 가능합니다.\nhttps://www.aladin.co.kr/shop/wproduct.aspx?ItemId=237744461\u0026start=slayer\n이번에 다루는 데이터에는 A의 학급에서는 근력운동을 하면 집중력이 향상된다는 이야기가 돌아 A가 실제로 근력운동을 시작하기 전과 후를 비교하기로 하였습니다. A는 20명에게 근력운동을 하게 한 후, 운동 전과 후에 집중력 측정 테스트를 받게 한 결과입니다.\n근력운동 전과 후 집중력에 유의한 차이가 있는지 대응표본 T 검정을 통해 알아보고자 합니다.\n가설은 아래와 같습니다.\n귀무가설 : 근력운동 전과 후의 테스트 평균은 같다.\n대립가설 : 근력운동 전과 후의 테스트 평균은 같지 않다.\n유의수준은 0.05로 설정하도록 하겠습니다.\n우선 데이터를 불러오도록 하겠습니다.데이터 파일은 하기 링크를 통해 다운로드 가능합니다.\nhttps://www.hanbit.co.kr/support/supplement_list.html\n\u0026gt;\u0026gt;\u0026gt; import pandas as pd \u0026gt;\u0026gt;\u0026gt; from scipy import stats \u0026gt;\u0026gt;\u0026gt; df = pd.read_csv(\u0026#34;./data/ch11_training_rel.csv\u0026#34;) \u0026gt;\u0026gt;\u0026gt; df.head() 전 후 0 59 41 1 52 63 2 55 68 3 61 59 4 59 84 다음은 정규성 검정을 하도록 하겠습니다.\n\u0026gt;\u0026gt;\u0026gt; a = stats.shapiro(df[\u0026#39;전\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; b = stats.shapiro(df[\u0026#39;후\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; print(a, b) ShapiroResult(statistic=0.9670045375823975, pvalue=0.690794825553894) ShapiroResult(statistic=0.9786625504493713, pvalue=0.9156817197799683) 결과는 모두 p-value가 0.05보다 커서 정규성을 만족합니다.\n다음으로는, Scipy 라이브러리 내에 있는 ttest_rel를 통해 t통계량과 p-value를 구할 수 있습니다.\n\u0026gt;\u0026gt; t_score, p_value = stats.ttest_rel(df[\u0026#39;전\u0026#39;], df[\u0026#39;후\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; print(round(t_score, 4), round(p_value, 2)) -2.2042 0.04 p-value가 유의수준인 0.05보다 작기 때문에 귀무가설(근력운동 전과 후의 평균이 같다)이 기각되었습니다. 따라서 근력운동 전과 후의 평균 점수에 유의한 차이가 있다고 말할수 있다는 결론을 내릴 수 있습니다.\n","date":"2024년 1월 19일","permalink":"/ko/posts/10/","section":"블로그 글","summary":"\u003cp\u003e본 글은 통계학에서 사용되는 대응표본 T 검정을 설명하기 위해 작성되었습니다.\u003c/p\u003e","title":"통계 - 대응표본 T 검정(Paired t-Test)"},{"content":"","date":null,"permalink":"/ko/tags/independent-samples-t-test/","section":"Tags","summary":"","title":"Independent Samples T-Test"},{"content":"","date":null,"permalink":"/ko/tags/ttest_ind/","section":"Tags","summary":"","title":"Ttest_ind"},{"content":"","date":null,"permalink":"/ko/tags/%EB%8F%85%EB%A6%BD%ED%91%9C%EB%B3%B8-t-%EA%B2%80%EC%A0%95/","section":"Tags","summary":"","title":"독립표본 T 검정"},{"content":"본 글은 통계학에서 사용되는 독립표본 T 검정(Independent Samples t-Test)을 설명하기 위해 작성되었습니다.\n금번에도 파이썬 라이브러리 Scipy를 활용하여 독립표본 T 검정을 진행하도록 하겠습니다.\n독립표본 T 검정 #독립표본 T 검정이란 두 개의 독립된 표본 간에 평균 차이가 통계적으로 유의미한지를 검정하는 통계 분석 기법입니다. 이는 두 그룹 간의 평균 차이가 우연히 발생한 것인지 아니면 진짜로 존재하는 것인지를 판단하는 데 사용됩니다.\n독립표본 T 검정의 주요 단계는 다음과 같습니다.\n1. 가설 설정 # H₀ : 𝜇₁ = 𝜇₂ → 귀무가설 두 그룹의 평균은 같다. H₁ : 𝜇₁ ≠ 𝜇₂ → 대립가설 두 그룹의 평균은 다르다. 2. 정규성 검정 #두 그룹의 표본 수가 30개 이하일 경우, 정규성 검정을 실시해야 합니다.\n두 그룹의 표본 수가 30개 이상일 경우, 중심극한정리에 의해 정규성을 만족했다고 가정합니다.\nScipy에서 정규성 검정은 Shapiro-Wilk 검정을 통해서 확인 가능합니다. 3. 등분산성 검정 #두 그룹의 데이터 수가 같다면, 분산이 같다고 가정합니다.\n두 그룹의 데이터 수가 다르다면, 분산이 같은지 확인하기 위해 등분산성 검정을 수행할 수 있습니다.\nScipy에서 등분산성 검정은 Levene 검정을 통해서 확인 가능합니다 4. 독립표본 T 통계량 계산 #두 그룹의 평균과 표준편차를 사용하여 독립표본 T 통계량을 계산합니다.\n5. 결정/결론 #계산된 T 통계량이 임계값을 초과하면 귀무가설을 기각하고 대립가설을 채택합니다.\n그렇지 않으면 귀무가설을 기각하지 않습니다.\n통계적으로 유의한 차이가 있다면, 두 그룹 간에 평균 차이가 존재한다고 결론 내립니다.\n독립표본 T 검정은 두 그룹 간의 평균 차이를 비교하는 데 유용하며, 실험 그룹과 대조 그룹 간의 차이를 조사하거나 두 가지 조건 간의 효과를 확인하는 등의 상황에서 적용될 수 있습니다.\n파이썬 라이브러리 Scipy 활용 방법 #다음은 파이썬 Scipy 라이브러리를 활용하여 독립표본 T 검정을 진행하도록 하겠습니다.책 누구나 파이썬 통계분석의 챕터 11의 데이터 파일 ch11_training_ind.csv을 사용하였습니다.\n책은 아래 링크를 통해 구입 가능합니다.\nhttps://www.aladin.co.kr/shop/wproduct.aspx?ItemId=237744461\u0026start=slayer\n이번에 다루는 데이터에는 인문 계열 학생이 많은 A의 학급에서는 근력운동을 하는 학생이 부쩍 늘어 A는 혹시 근력운동이 집중력을 향상히는 효과가 있다면 자신의 학급과 평소에 근력운동을 하고 있는 체육 계열 학생이 많은 B의 학급 사이에 집중력 테스트의 평균에서 차이가 나지 않을까 생각하여, B의 학급에도 집중력 테스트를 받게 한 결과입니다.\nA와 B의 학급의 집중력에 유의한 차이가 있는지 독립표본 T 검정을 통해 알아보고자 합니다.\n가설은 아래와 같습니다.\n귀무가설 : A와 B의 학급의 평균은 같다.\n대립가설 : A와 B의 학급의 평균은 같지 않다.\n유의수준은 0.05로 설정하도록 하겠습니다.\n우선 데이터를 불러오도록 하겠습니다.데이터 파일은 하기 링크를 통해 다운로드 가능합니다.\nhttps://www.hanbit.co.kr/support/supplement_list.html\n\u0026gt;\u0026gt;\u0026gt; import pandas as pd \u0026gt;\u0026gt;\u0026gt; from scipy import stats \u0026gt;\u0026gt;\u0026gt; df = pd.read_csv(\u0026#34;./data/ch11_training_ind.csv\u0026#34;) \u0026gt;\u0026gt;\u0026gt; df.head() A B 0 47 49 1 50 52 2 37 54 3 60 48 4 39 51 다음은 정규성 검정을 하도록 하겠습니다.\n\u0026gt;\u0026gt;\u0026gt; a = stats.shapiro(df[\u0026#39;A\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; b = stats.shapiro(df[\u0026#39;B\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; print(a, b) ShapiroResult(statistic=0.9685943722724915, pvalue=0.7249553203582764) ShapiroResult(statistic=0.9730021357536316, pvalue=0.8165789842605591) 결과는 모두 p-value가 0.05보다 커서 정규성을 만족합니다.\n다음으로 해당 데이터는 데이터의 개수가 같기 때문에 등산이 같다고 가정하지만, 각 그룹의 데이터수가 다르면 분산이 같은지 검정해야 하기 때문에 levene을 통해서 아래와 같이 확인이 가능합니다.\n\u0026gt;\u0026gt;\u0026gt; stats.levene(df[\u0026#39;A\u0026#39;], df[\u0026#39;B\u0026#39;]) LeveneResult(statistic=2.061573118077718, pvalue=0.15923550057222613) p-value가 0.159 이므로 귀무가설(두 그룹의 분산은 차이가 없다)을 채택합니다.\n다음으로는, Scipy 라이브러리 내에 있는 ttest_ind를 통해 t통계량과 p-value를 구할 수 있습니다.\n\u0026gt;\u0026gt;\u0026gt; t, p = stats.ttest_ind(df[\u0026#39;A\u0026#39;], df[\u0026#39;B\u0026#39;], equal_var=False) # equal_var=False : 웰치의 방법 \u0026gt;\u0026gt;\u0026gt; t, p (-1.760815724652471, 0.08695731107259362) p-value가 유의수준인 0.05보다 크기 때문에 귀무가설(A와 B의 학급의 평균이 같다)이 채택되었습니다. 따라서 A의 학급과 B의 학급 사이에는 평균 점수에 유의한 차이가 있다고 말할수 없다는 결론을 내릴 수 있습니다.\n","date":"2024년 1월 15일","permalink":"/ko/posts/9/","section":"블로그 글","summary":"\u003cp\u003e본 글은 통계학에서 사용되는 독립표본 T 검정(Independent Samples t-Test)을 설명하기 위해 작성되었습니다.\u003c/p\u003e","title":"통계 - 독립표본 T 검정(Independent Samples t-Test)"},{"content":"","date":null,"permalink":"/ko/tags/one-sample-t-test/","section":"Tags","summary":"","title":"One Sample T-Test"},{"content":"","date":null,"permalink":"/ko/tags/%EB%8B%A8%EC%9D%BC%ED%91%9C%EB%B3%B8-t-%EA%B2%80%EC%A0%95/","section":"Tags","summary":"","title":"단일표본 T 검정"},{"content":"본 글은 통계학에서 사용되는 단일표본 T 검정을 설명하기 위해 작성되었습니다.\n또한, 파이썬 Scipy 라이브러리를 활용하여 단일표본 T 검정을 진행하도록 하겠습니다.\n단일표본 T 검정 #단일표본 T 검정이란 통계 분석에서 사용되는 가설 검정 방법 중 하나로, 하나의 표본에 대한 평균을 검정하는데 사용됩니다. 주로 모집단의 평균이 어떤 특정 값과 같은지를 확인하고자 할 때 적용됩니다.\n단일표본 T 검정은 다음과 같은 단계로 이루어집니다.\n1. 가설 설정 # H₀ : 𝜇 = 𝜇₀ → 귀무가설 모평균과 표본평균은 같다. H₁ : 𝜇 ≠ 𝜇₀ → 대립가설 모평균과 표본평균은 같지 않다. 2. 표본 추출 #모집단에서 표본을 추출하여 해당 표본의 평균을 계산합니다.\n3. 가설 검정 통계량 계산 #t-통계량을 계산합니다. 이는 표본 평균과 가설에 기반한 예상 평균 간의 차이를 나타냅니다.\n4. 결정 / 결론 #계산된 t-통계량이 기각역 안에 들어가면 귀무가설을 기각하고 대립가설을 채택합니다.\n그렇지 않으면 귀무가설을 기각하지 않습니다.\n양측검정일 경우, 기각역은 양측에 대칭되는 t-분포의 양 끝 부분입니다. 귀무가설을 기각했을 경우, 표본이 모집단과 다르다고 결론내립니다.\n반대로 기각하지 못했을 경우, 통계적으로 유의하지 않다고 결론내립니다.\n파이썬 라이브러리 Scipy 활용 방법 #다음은 파이썬 Scipy 라이브러리를 활용하여 단일표본 T 검정을 진행하도록 하겠습니다.\n이번에 다루는 데이터에는 나무 31그루의 둘레와 높이, 부피가 저장되어 있습니다.\n이 표본의 평균이 모평균과 일치하는지 단일표본 T 검정을 통해 알아보고자 합니다. 가설은 아래와 같습니다.\n유의수준은 0.05로 설정하도록 하겠습니다.\n가설검정\n귀무가설 : 평균은 75이다.\n대립가설 : 평균은 75가 아니다.\n우선 데이터를 불러오도록 하겠습니다.\n\u0026gt;\u0026gt;\u0026gt; import pandas as pd \u0026gt;\u0026gt;\u0026gt; df = pd.read_csv(\u0026#34;./data/trees.csv\u0026#34;) \u0026gt;\u0026gt;\u0026gt; df.head() Girth Height Volume 0 8.3 70 10.3 1 8.6 65 10.3 2 8.8 63 10.2 3 10.5 72 16.4 4 10.7 81 18.8 또한, 표본 평균 \u0026lsquo;Height\u0026rsquo;의 평균을 구해보도록 하겠습니다.\n\u0026gt;\u0026gt;\u0026gt; result = df[\u0026#39;Height\u0026#39;].mean() \u0026gt;\u0026gt;\u0026gt; round(result, 2) # (반올림하여 소숫점 둘째 자리까지 계산) 76.0 다음으로, 단일표본 T 검정을 위해 Scipy 라이브러리를 불러오도록 하겠습니다.\n\u0026gt;\u0026gt;\u0026gt; from scipy import stats 단일표본 T 검정은 Scipy 내 ttest_1samp를 사용하여 검정합니다.\n참조: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_1samp.html\n그 다음으로, 가설 검정 통계량 계산하도록 하겠습니다.\n\u0026gt;\u0026gt;\u0026gt; from math import sqrt \u0026gt;\u0026gt;\u0026gt; t_score, p_value = stats.ttest_1samp(df[\u0026#39;Height\u0026#39;], popmean=75) \u0026gt;\u0026gt;\u0026gt; print(round(t_score, 2), round(p_value, 2)) 0.87 0.39 popmean은 귀무가설에서 예상한 평균과 같습니다.\n위의 통계량에 대한 p-값을 구한 후(반올림하여 소숫점 넷째 자리까지 계산), 유의수준 0.05하에서 가설검정의 결과를 귀무가설을 기각할 것인지, 기각하지 못할 것인지 확인하도록 하겠습니다.\n\u0026gt;\u0026gt;\u0026gt; print(round(p_value, 4)) 0.3892 \u0026gt;\u0026gt;\u0026gt; if p_value \u0026gt;= 0.05: print(\u0026#34;채택\u0026#34;) else: print(\u0026#34;기각\u0026#34;) 채택 따라서, 표본 평균은 모평균과 같다는 귀무가설을 기각하지 못하였습니다.\n","date":"2024년 1월 13일","permalink":"/ko/posts/8/","section":"블로그 글","summary":"\u003cp\u003e본 글은 통계학에서 사용되는 단일표본 T 검정을 설명하기 위해 작성되었습니다.\u003c/p\u003e","title":"통계 - 단일표본 T 검정(One Sample t-Test)"},{"content":"본 글은 통계학에서 회귀 분석을 설명하기 위해 작성되었습니다.\n회귀 분석이란? #회귀 분석이란 하나 또는 그 이상의 독립변수들이 종속변수에 미치는 영향을 추정할 수 있는 통계 기법을 말합니다.\n회귀 분석의 변수 #종속변수(Dependent Variable) y는 영향을 받는 변수로, 반응변수 (Response Variable), 결과변수 (Outcome Variable)이라고도 합니다. 모델에서 예측하려는 변수로, 종속 변수는 다른 변수들에 의해 영향을 받습니다.\n독립변수(Independent Variable) x는 영향을 주는 변수로, 설명변수 (Explanatory Variable), 예측변수 (Predictor Variable)이라고도 합니다. 독립변수는 종속 변수에 영향을 미치는 변수들로, 예측 모델을 구축할 때 사용됩니다.\n변수의 개수에 따른 회귀분석 방법 #회귀분석은 변수의 개수에 따라 접근하는 방법이 상이합니다.\n독립 변수의 개수가 하나이면 단순선형회귀분석으로, 독립 변수의 개수가 두 개 이상이면 다중선형회귀분석 접근이 가능합니다.\n1. 단순선형회귀분석(Simple Linear Regression) #하나의 독립변수가 종속변수에 미치는 영향을 추정할 수 있는 통계기법을 말합니다. 예측값과 실제 데이터의 오차*(= 잔차)*가 가장 작은 직선을 회귀직선으로 선택합니다. 회귀직선은 수많은 직선들 중에서, 데이터에 대해 잔차제곱합(Residual Sum of Squares, RSS) 이 더욱 작은 직선을 뜻합니다.\n이는 최소제곱법(Ordinary Least Square) 을 통해 이뤄집니다.\n최소제곱법 : 측정값을 기초로 제곱합을 만들고 그것이 최소인 값을 구하여 처리하는 방법, 잔차제곱합이 가장 작은 선을 선택합니다.\n2. 다중선형회귀분석(Multiple Linear Regression) #두 개 이상의 독립변수가 하나의 종속변수에 미치는 영향을 추정하는 통계기법을 말합니다. 다중선형회귀분석에는 회귀계수의 유의성을 판단하는 것이 중요합니다. 왜냐하면, 모든 회귀계수의 유의성이 통계적으로 검증되어야 선택된 변수들의 조합으로 모형을 확인할 수 있기 때문입니다.\n회귀계수의 유의성은 단순회귀분석의 회귀계수 유의성 검토와 같이 회귀계수 t-통계량을 통해 확인이 가능합니다.\n다중선형회귀분석다중공선성(Multicollinearity) #다중 공선성은 회귀분석에서 독립변수들 간에 강한 상관관계가 나타나는 현상을 의미합니다. 즉, 하나의 독립변수를 나머지 독립변수들로 예측할 수 있는 경우에 발생합니다.\n다중공선성이 발생하면 각 독립변수의 회귀계수의 정확한 추정이 어려워집니다. 또한, 각 독립변수의 회귀계수가 종속변수에 미치는 영향력을 올바로 설명하지 못하게 됩니다.\n다중 공선성 검사 방법\n분산팽창요인 (VIF, Variance Inflation Factor):\n분산팽창요인은 각 독립변수의 분산이 얼마나 증가했는지를 나타내며, 이 값이 크면 다중공선성이 증가했다고 판단합니다. 이는 각 독립변수를 다른 독립변수들로 선형회귀한 결과의 분산 비율로 계산됩니다. 일반적으로, 분산팽창요인이 4보다 크다면 다중공선성이 존재한다고 판단하며, 10보다 크다면 심각한 문제가 있는 것으로 해석됩니다.\n회귀 분석 시 검토 사항 #회귀 분석을 시행할 경우, 검토해야 하는 사항은 다음과 같이 세 가지 항목이 있습니다.\n1. 회귀계수들이 유의미한가? #해당 계수의 t-통계량의 p-값이 0.05보다 작으면 해당 회귀계수가 통계적으로 유의미하다고 판단합니다. 즉, 계수가 종속변수에 대해 유의미한 영향을 미친다는 것을 의미합니다.\n2. 모형이 얼마나 설명력을 갖는가? #모형이 얼마나 설명력을 갖는지 확인하기 위해서는 결정계수*(𝑅²)*를 확인해야 합니다.\n결정계수(Coefficient of Determination; 𝑅²) #결정계수는 0에서 1 사이의 값으로, 1에 가까울수록 모형이 종속변수의 변동을 잘 설명한다는 것을 의미합니다. 변동을 잘 설명한다는 의미는 회귀선에 얼마나 변동이 되는지 확인이 가능하다는 뜻입니다.\n높은 결정계수는 모형의 예측 능력이 높다는 것을 나타냅니다.\n3. 모형이 데이터를 잘 적합하고 있는가? #모형이 데이터를 잘 적합하는지 판단하기 위해서 잔차를 그래프로 그리고 회귀진단을 수행합니다. 잔차는 실제 값과 모델의 예측 값 간의 차이를 의미하며, 이를 시각적으로 검토하여 모델이 데이터를 얼마나 잘 적합하고 있는지를 확인합니다. 일반적으로 잔차는 정규분포를 따르고, 특정한 패턴이나 추세가 없어야 합니다. 또한, 잔차의 등분산성도 확인되어야 합니다. 이상치나 영향력 있는 데이터 포인트가 있는지도 검토하며, 필요시 이를 제거하거나 조정하여 모델의 안정성을 확인합니다.\n이러한 검토를 통해 회귀 분석의 신뢰성과 모델의 적합성을 평가할 수 있습니다.\n","date":"2024년 1월 12일","permalink":"/ko/posts/7/","section":"블로그 글","summary":"\u003cp\u003e본  글은 통계학에서 회귀 분석을 설명하기 위해 작성되었습니다.\u003c/p\u003e","title":"통계 - 회귀 분석"},{"content":"","date":null,"permalink":"/ko/tags/%ED%9A%8C%EA%B7%80-%EB%B6%84%EC%84%9D/","section":"Tags","summary":"","title":"회귀 분석"},{"content":"","date":null,"permalink":"/ko/tags/%EA%B0%80%EC%84%A4-%EA%B2%80%EC%A0%95/","section":"Tags","summary":"","title":"가설 검정"},{"content":"","date":null,"permalink":"/ko/tags/%EA%B7%80%EB%AC%B4-%EA%B0%80%EC%84%A4/","section":"Tags","summary":"","title":"귀무 가설"},{"content":"","date":null,"permalink":"/ko/tags/%EB%8C%80%EB%A6%BD-%EA%B0%80%EC%84%A4/","section":"Tags","summary":"","title":"대립 가설"},{"content":"본 글은 pandas 라이브러리를 활용하는 데에 있어 여러 개의 데이터를 하나로 통합하는 방법을 설명하기 위해 작성되었습니다.\n통계학에서 가설(Hypothesis)은 어떤 주장이나 추정을 나타내는 명제이며, 모수에 대한 가정/잠정적 결론을 뜻합니다.\n가설의 종류 #가설은 아래와 같이 두 가지 형태로 나타낼 수 있습니다.\n1. 귀무가설 (Null Hypothesis, H0) #귀무가설은 기존과 비교하여 아무런 변화 혹은 차이가 없음을 나타내는 가설로, 일종의 \u0026lsquo;디폴트\u0026rsquo; 가설입니다.\n검정 방법에 따라 귀무가설의 내용이 달라집니다. 예를 들자면, \u0026ldquo;두 그룹의 평균은 같다\u0026quot;와 같은 주장이 귀무가설로 설정될 수 있습니다.\n2. 대립가설 (Alternative Hypothesis, H1) #대립가설은 귀무가설에 대립하는 주장이며, 표본을 통해 확실한 근거를 가지고 입증하고자 하는 가설입니다. 예를 들어, \u0026ldquo;두 그룹의 평균은 다르다\u0026quot;와 같은 주장이 대립가설로 설정될 수 있습니다.\n통계적 검정을 통해 주어진 데이터를 사용하여 귀무가설을 기각할지, 아니면 기각할 근거가 없어서 귀무가설을 기각하지 못한다는 것을 결정하게 됩니다. 검정 결과에서 대립가설이 참이라는 확실한 근거를 발견할 경우, 귀무가설을 기각합니다.\n통계학에서 주장하고 싶은 이러한 가설의 타당성을 검증하는 과정이 바로 가설 검정입니다.\n가설 검정 #1. 가설 설정 #가설 검정의 첫 단계는 조사하고자 하는 문제에 따라 귀무가설(H0)과 대립가설(H1)을 설정하는 것입니다.\n2. 표본 분석 #다음으로는, 전체 모집단에서 일부를 대표할 수 있는 표본을 추출합니다. 이 표본에 대해 데이터를 수집하고 분석합니다. 이로써 통계적 분석에 사용할 자료를 확보합니다.\n3. 가설의 타당성 검정 #수집된 데이터를 사용하여 가설을 검정합니다. 통계적 기법을 활용하여 귀무가설을 기각할지, 아니면 기각할 근거가 없어서 귀무가설을 채택할지를 결정합니다. 이는 유의 수준과 검정통계량을 고려하여 이루어집니다.\n유의 수준 (Level of Significance)\n유의 수준은 주로 α(alpha)로 표기되며, 실험 또는 조사에서 귀무가설을 기각하는 기준 확률을 나타냅니다.\n흔히 사용되는 유의 수준은 0.05(5%)이지만, 실험의 성격이나 연구의 특성에 따라 0.01 또는 0.10 등 다른 값을 사용할 수 있습니다.\n검정통계량\n검정통계량은 수집한 데이터와 가설이 얼마나 일치하는지를 측정하는 지표로, 모수 추론을 하기 위해서 필요한 표본 통계량입니다. 검정통계량은 가설 검정에서 중요한 역할을 하며, 귀무가설의 기각 여부를 결정하는 데 사용됩니다.\n가설을 검정하다 보면, 통계적인 오류가 발생할 경우가 항상 존재하는데 이를 가설 검정 오류라고 합니다.\n가설 검정 오류 #1. 제1종 오류 #제1종 오류란 귀무가설이 참일 때, 귀무가설을 기각하는 오류를 말합니다. 제1종 오류가 발생하는 원인은 통계적 검정에서 유의 수준(significance level)을 설정하게 되는데, 이 수준에서 귀무가설을 기각할 때, 우연히 발생합니다.\n예시) 실제로는 효과가 없는데도, 우리가 효과가 있다고 잘못 결론 내리는 상황\n2. 제2종 오류 #제2종 오류란 대립가설이 참일 때, 귀무가설을 채택하는 오류를 말합니다. 제2종 오류가 발생하는 원인은 검정력(power)이 부족하여 실제로 존재하는 효과를 감지하지 못할 때 발생합니다.\n예시) 실제로는 효과가 있지만, 통계적 검정에서 그 효과를 찾지 못하여 귀무가설을 채택하는 상황\n","date":"2024년 1월 11일","permalink":"/ko/posts/6/","section":"블로그 글","summary":"\u003cp\u003e본 글은 pandas 라이브러리를 활용하는 데에 있어 여러 개의 데이터를 하나로 통합하는 방법을 설명하기 위해 작성되었습니다.\u003c/p\u003e","title":"통계 - 가설 검정 (1)"},{"content":"","date":null,"permalink":"/ko/tags/concat/","section":"Tags","summary":"","title":"Concat"},{"content":"","date":null,"permalink":"/ko/tags/join/","section":"Tags","summary":"","title":"Join"},{"content":"","date":null,"permalink":"/ko/tags/merge/","section":"Tags","summary":"","title":"Merge"},{"content":"","date":null,"permalink":"/ko/tags/pandas/","section":"Tags","summary":"","title":"Pandas"},{"content":"본 글은 pandas 라이브러리를 활용하는 데에 있어 여러 개의 데이터를 하나로 통합하는 방법을 설명하기 위해 작성되었습니다.\n데이터를 통합하는 방법으로는 여러 가지가 있는데 이번에 다루어 볼 내용은 concat, join, merge입니다.\n우선 설명하기에 앞서, 예시로 두 개의 데이터 프레임을 작성하도록 하겠습니다.\n\u0026gt;\u0026gt;\u0026gt; import pandas as pd \u0026gt;\u0026gt;\u0026gt; df1 = pd.DataFrame({ \u0026#39;Class1\u0026#39; : [95, 92, 98, 100], \u0026#39;Class2\u0026#39; : [91, 93, 97, 99] }) \u0026gt;\u0026gt;\u0026gt; df2 = pd.DataFrame({ \u0026#39;Class1\u0026#39; : [87, 89], \u0026#39;Class2\u0026#39; : [85, 90] }) d1 출력값:\nClass1 Class2 0 87 85 1 89 90 d2 출력값:\nClass1 Class2 0 95 91 1 92 93 2 98 97 3 100 99 1. concat #pandas 라이브러리의 concat 함수는 데이터프레임을 이어 붙이는 데 사용됩니다. 이 함수는 여러 데이터프레임을 행 또는 열 방향으로 이어 붙일 수 있습니다.\ndf1과 df2를 result에 붙이도록 하겠습니다.\n\u0026gt;\u0026gt;\u0026gt; result = pd.concat([df1, df2]) \u0026gt;\u0026gt;\u0026gt; result Class1 Class2 0 95 91.0 1 92 93.0 2 98 97.0 3 100 99.0 4 87 85.0 5 89 90.0 6 96 NaN 7 83 NaN **pd.concat([df1, df2])**는 df1과 df2를 행 방향으로 이어 붙입니다. 즉, 두 데이터프레임이 위아래로 연결됩니다.\n다음으로는 Class1 열만 가지고 있는 d3를 result에 통합하도록 하겠습니다.\n\u0026gt;\u0026gt;\u0026gt; df3 = pd.DataFrame({ \u0026#39;Class1\u0026#39; : [96, 83] }) \u0026gt;\u0026gt;\u0026gt; pd.concat([result, df3], ignore_index=True) Class1 Class2 0 95 91.0 1 92 93.0 2 98 97.0 3 100 99.0 4 87 85.0 5 89 90.0 6 96 NaN 7 83 NaN d3 데이터는 \u0026lsquo;Class2\u0026rsquo; 열을 가지고 있지 않기 때문에 공백값을 출력하게 됩니다.\n2. join #pandas 라이브러리의 join 메서드는 두 개의 데이터프레임을 특정 열을 기준으로 결합하는 데 사용됩니다. 일반적으로 SQL의 JOIN 연산과 유사한 역할을 합니다. concat과 달리 join은 가로 방향으로 통합됩니다.\n\u0026gt;\u0026gt;\u0026gt; df4 = pd.DataFrame({ \u0026#39;Class3\u0026#39; : [93, 91, 95, 98] }) \u0026gt;\u0026gt;\u0026gt; df1.join(df4) Class1 Class2 Class3 a 95 91 93 b 92 93 91 c 98 97 95 d 100 99 98 다음과 같이 index를 임의로 설정하여 출력하는 것 또한 가능합니다.\n\u0026gt;\u0026gt;\u0026gt; index_label = [\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;d\u0026#39;] \u0026gt;\u0026gt;\u0026gt; df1a = pd.DataFrame({\u0026#39;Class1\u0026#39;: [95, 92, 98, 100], \u0026#39;Class2\u0026#39;: [91, 93, 97, 99]}, index= index_label) \u0026gt;\u0026gt;\u0026gt; df4a = pd.DataFrame({\u0026#39;Class3\u0026#39;: [93, 91, 95, 98]}, index=index_label) \u0026gt;\u0026gt;\u0026gt; df1a.join(df4a) Class1 Class2 Class3 a 95 91 93 b 92 93 91 c 98 97 95 d 100 99 98 3. merge #pandas 라이브러리의 merge 함수는 두 개의 데이터프레임을 특정 열을 기준으로 병합(merge)하는 데 사용됩니다. merge 함수를 사용하면 데이터프레임 간에 공통된 열을 기준으로 결합할 수 있습니다.\n\u0026gt;\u0026gt;\u0026gt; df_A_B = pd.DataFrame({\u0026#39;판매월\u0026#39;: [\u0026#39;1월\u0026#39;, \u0026#39;2월\u0026#39;, \u0026#39;3월\u0026#39;, \u0026#39;4월\u0026#39;], \u0026#39;제품A\u0026#39;: [100, 150, 200, 130], \u0026#39;제품B\u0026#39;: [90, 110, 140, 170]}) \u0026gt;\u0026gt;\u0026gt; df_C_D = pd.DataFrame({\u0026#39;판매월\u0026#39;: [\u0026#39;1월\u0026#39;, \u0026#39;2월\u0026#39;, \u0026#39;3월\u0026#39;, \u0026#39;4월\u0026#39;], \u0026#39;제품C\u0026#39;: [112, 141, 203, 134], \u0026#39;제품D\u0026#39;: [90, 110, 140, 170]}) df_A_B\n판매월 제품A 제품B 0 1월 100 90 1 2월 150 110 2 3월 200 140 3 4월 130 170 df_C_D\n판매월 제품C 제품D 0 1월 112 90 1 2월 141 110 2 3월 203 140 3 4월 134 170 merge를 사용하여 \u0026lsquo;판매월\u0026rsquo; 열을 기준으로 두 데이터프레임을 병합합니다. 결과적으로 \u0026lsquo;판매월\u0026rsquo; 열을 기준으로 두 데이터프레임이 합쳐지고, 공통된 열을 중심으로 데이터가 정렬됩니다.\n\u0026gt;\u0026gt;\u0026gt; df_A_B.merge(df_C_D) 판매월 제품A 제품B 제품C 제품D 0 1월 100 90 112 90 1 2월 150 110 141 110 2 3월 200 140 203 140 3 4월 130 170 134 170 merge 메서드를 사용하여 두 데이터프레임을 결합하는 네 가지 다른 방법을 구현해 보도록 하겠습니다.\n\u0026gt;\u0026gt;\u0026gt; df_left = pd.DataFrame({\u0026#39;key\u0026#39;:[\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;C\u0026#39;], \u0026#39;left\u0026#39;: [1, 2, 3]}) \u0026gt;\u0026gt;\u0026gt; df_right = pd.DataFrame({\u0026#39;key\u0026#39;:[\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;D\u0026#39;], \u0026#39;right\u0026#39;: [4, 5, 6]}) 1.\n\u0026gt;\u0026gt;\u0026gt; df_left.merge(df_right, how=\u0026#39;left\u0026#39;, on = \u0026#39;key\u0026#39;) key left right 0 A 1 4.0 1 B 2 5.0 2 C 3 NaN \u0026lsquo;key\u0026rsquo; 열을 기준으로 df_left와 df_right를 왼쪽 조인(left join)합니다. 왼쪽 조인은 왼쪽 데이터프레임(df_left)의 모든 행을 유지하고, 오른쪽 데이터프레임(df_right)의 해당 키 값이 있는 행을 추가합니다. 만약 해당 키 값이 오른쪽 데이터프레임에 없으면 NaN으로 채웁니다.\n2.\n\u0026gt;\u0026gt;\u0026gt; df_left.merge(df_right, how=\u0026#39;right\u0026#39;, on = \u0026#39;key\u0026#39;) key left right 0 A 1.0 4 1 B 2.0 5 2 D NaN 6 \u0026lsquo;key\u0026rsquo; 열을 기준으로 df_left와 df_right를 오른쪽 조인(right join)합니다. 오른쪽 조인은 오른쪽 데이터프레임(df_right)의 모든 행을 유지하고, 왼쪽 데이터프레임(df_left)의 해당 키 값이 있는 행을 추가합니다. 만약 해당 키 값이 왼쪽 데이터프레임에 없으면 NaN으로 채웁니다.\n3.\n\u0026gt;\u0026gt;\u0026gt; df_left.merge(df_right, how=\u0026#39;outer\u0026#39;, on = \u0026#39;key\u0026#39;) key left right 0 A 1.0 4.0 1 B 2.0 5.0 2 D 3.0 NaN 3 D NaN 6.0 \u0026lsquo;key\u0026rsquo; 열을 기준으로 df_left와 df_right를 외부 조인(outer join)합니다. 외부 조인은 양쪽 데이터프레임의 모든 행을 포함하며, 양쪽 데이터프레임 중 한쪽에만 해당하는 경우 NaN으로 채웁니다.\n4.\n\u0026gt;\u0026gt;\u0026gt; df_left.merge(df_right, how=\u0026#39;inner\u0026#39;, on = \u0026#39;key\u0026#39;) key left right 0 A 1 4 1 B 2 5 \u0026lsquo;key\u0026rsquo; 열을 기준으로 df_left와 df_right를 내부 조인(inner join)합니다. 내부 조인은 양쪽 데이터프레임에 공통으로 존재하는 행만을 포함합니다. 즉, 양쪽 데이터프레임에서 동일한 \u0026lsquo;key\u0026rsquo; 값을 가지는 행들을 결합합니다.\n","date":"2024년 1월 8일","permalink":"/ko/posts/5/","section":"블로그 글","summary":"\u003cp\u003e본 글은 pandas 라이브러리를 활용하는 데에 있어 여러 개의 데이터를 하나로 통합하는 방법을 설명하기 위해 작성되었습니다.\u003c/p\u003e","title":"파이썬 - pandas 데이터 통합하기 (concat, join, merge)"},{"content":"","date":null,"permalink":"/ko/tags/matplotlib/","section":"Tags","summary":"","title":"Matplotlib"},{"content":"본 글은 파이썬 내 matplotlib 라이브러리 사용 중 그래프에 텍스트를 삽입하기 위한 방법을 설명하기 위해 작성되었습니다.\nmatplotlib을 활용하여 그래프를 출력할 때, 아래와 같이 그래프 위에 텍스트를 삽입해 보도록 하겠습니다.\n우선 아래와 같이 임의의 월별 판매 수량의 데이터를 가지고 구현하도록 하겠습니다.\n\u0026gt;\u0026gt;\u0026gt; import calendar \u0026gt;\u0026gt;\u0026gt; month_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] \u0026gt;\u0026gt;\u0026gt; sold_list = [300, 400, 550, 900, 600, 960, 900, 910, 800, 700, 550, 450] \u0026gt;\u0026gt;\u0026gt; fig, ax = plt.subplots() \u0026gt;\u0026gt;\u0026gt; barcharts = ax.bar(month_list, sold_list) # calendar.month_name[1:13] → xlabel에 January부터 December까지 출력 \u0026gt;\u0026gt;\u0026gt; ax.set_xticks(month_list, calendar.month_name[1:13], rotation=90) \u0026gt;\u0026gt;\u0026gt; print(barcharts) 코드를 실행하면, 아래와 같은 그래프가 출력됩니다.\n다음으로, 각 bar 위에 해당하는 y값을 삽입하도록 하겠습니다.\n각 bar의 값을 구한 뒤, 텍스트로 삽입하기 위해서는 y값을 출력하는 get_height()와 bar에 텍스트를 입력하는 ax.text()를 사용합니다.\n참조:\nget_height()\nhttps://matplotlib.org/stable/api/_as_gen/matplotlib.patches.Rectangle.html\nax.text()\nhttps://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.text.html#matplotlib.axes.Axes.text\n완성된 코드는 다음과 같습니다.\n\u0026gt;\u0026gt;\u0026gt; import calendar \u0026gt;\u0026gt;\u0026gt; month_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] \u0026gt;\u0026gt;\u0026gt; sold_list = [300, 400, 550, 900, 600, 960, 900, 910, 800, 700, 550, 450] \u0026gt;\u0026gt;\u0026gt; fig, ax = plt.subplots() \u0026gt;\u0026gt;\u0026gt; barcharts = ax.bar(month_list, sold_list) \u0026gt;\u0026gt;\u0026gt; ax.set_xticks(month_list, calendar.month_name[1:13], rotation=90) \u0026gt;\u0026gt;\u0026gt; print(barcharts) \u0026gt;\u0026gt;\u0026gt; for rect in barcharts: height = rect.get_height() ax.text(rect.get_x() + rect.get_width()/2., 1.002*height,\u0026#39;%d\u0026#39; % int(height), ha=\u0026#39;center\u0026#39;, va=\u0026#39;bottom\u0026#39;) \u0026gt;\u0026gt;\u0026gt; plt.show() rect.get_x() + rect.get_width()/2.\n각 막대의 x 위치와 너비의 중간 지점을 계산합니다. 이는 막대의 가로축 중심을 나타냅니다.\n1.002 * height\nheight는 현재 막대의 높이이며, 1.002*height는 막대의 높이보다 조금 더 위에 텍스트를 배치하도록 하는 보정 값입니다.\n\u0026rsquo;% d\u0026rsquo; % int(height)\n문자열 포매팅 즉, 각 막대별 높잇값 삽입 (% 뒤에 오는 d(정수)를 삽입, int(height) 막대의 높이를 정수로 변환)\nha=\u0026lsquo;center\u0026rsquo;\n가로 (x 축) 정렬을 중앙에 맞춥니다.\nva=\u0026lsquo;bottom\u0026rsquo;\n세로 (y 축) 정렬을 아래에 맞춥니다.\n따라서, 코드를 실행하면 아래와 같이 출력됩니다. ","date":"2024년 1월 5일","permalink":"/ko/posts/4/","section":"블로그 글","summary":"\u003cp\u003e본 글은 파이썬 내 matplotlib 라이브러리 사용 중 그래프에 텍스트를 삽입하기 위한 방법을 설명하기 위해 작성되었습니다.\u003c/p\u003e","title":"파이썬 - Matplotlib 텍스트 삽입"},{"content":"","date":null,"permalink":"/ko/tags/date_range/","section":"Tags","summary":"","title":"Date_range"},{"content":"","date":null,"permalink":"/ko/tags/iloc/","section":"Tags","summary":"","title":"Iloc"},{"content":"","date":null,"permalink":"/ko/tags/loc/","section":"Tags","summary":"","title":"Loc"},{"content":"본 글은 파이썬 내 pandas 라이브러리를 활용하여 DataFrame을 다룰 때 필요한 .loc() 과 .iloc() 각각의 특징과 차이점을 설명하고자 작성되었습니다.\n우선, 설명을 위해서 seaborn을 활용하여 예시의 데이터(iris)를 가져오도록 하겠습니다.\n\u0026gt;\u0026gt;\u0026gt; import seaborn as sns \u0026gt;\u0026gt;\u0026gt; iris = sns.load_dataset(\u0026#39;iris\u0026#39;) \u0026gt;\u0026gt;\u0026gt; iris.head() iris 데이터 첫 5행열 1. loc #loc은 레이블(Label)을 기반으로 데이터를 선택하는 메서드입니다. 행과 열의 이름(Label)을 사용하여 데이터에 접근합니다. 즉, 행과 열의 이름을 명시적으로 지정하여 데이터를 선택합니다.\n# 열 이름이 \u0026#39;species\u0026#39;인 데이터 중 \u0026#39;virginica\u0026#39;를 가진 값 선택 \u0026gt;\u0026gt;\u0026gt; iris.loc[iris[\u0026#39;species\u0026#39;] == \u0026#39;virginica\u0026#39;] species 중에 virgnica 값을 가지고 있는 첫 5행열 2. iloc #iloc은 정수(Integer) 기반의 인덱스를 사용하여 데이터를 선택하는 메서드입니다. 행과 열의 정수 위치(인덱스)를 이용하여 데이터에 접근합니다. 즉, 데이터의 위치를 정수로 명시하여 선택합니다.\n# 첫 번째 행과 두 번째 열에 있는 데이터 선택 \u0026gt;\u0026gt;\u0026gt; iris.iloc[0, 1] 3.5 첫 번째 행과 두 번째 열인 sepal_width에 있는 데이터 3. loc과 iloc의 차이점 # 인덱스의 타입:\nloc은 레이블(Label)을 사용하므로, 행과 열의 이름이 문자열이나 다른 데이터 타입일 수 있습니다.\niloc은 정수(Integer)를 사용하므로, 행과 열의 인덱스는 정수여야 합니다.\n사용법:\nloc은 명시적으로 레이블을 사용하여 데이터를 선택하는 데에 중점을 둡니다.\niloc은 정수 위치(인덱스)를 사용하여 데이터를 선택하는 데에 중점을 둡니다.\n예시:\nloc 예시: df.loc[\u0026lsquo;A\u0026rsquo;, \u0026lsquo;column_name\u0026rsquo;]\niloc 예시: df.iloc[0, 1]\n언제 어떤 메서드를 사용할지는 데이터프레임의 구조 및 사용자의 목적에 따라 다릅니다.\nloc은 레이블이 명확하게 정의되어 있을 때 유용하며, iloc은 정수 기반의 인덱스가 사용되는 경우에 유용합니다.\n","date":"2024년 1월 4일","permalink":"/ko/posts/3/","section":"블로그 글","summary":"\u003cp\u003e본 글은 파이썬 내 \u003cem\u003e\u003cstrong\u003epandas\u003c/strong\u003e\u003c/em\u003e 라이브러리를 활용하여 DataFrame을 다룰 때 필요한 \u003cstrong\u003e.loc()\u003c/strong\u003e 과 \u003cstrong\u003e.iloc()\u003c/strong\u003e 각각의 특징과 차이점을 설명하고자 작성되었습니다.\u003c/p\u003e","title":"파이썬 - pandas loc vs iloc"},{"content":"본 글은 pandas 라이브러리 내 날짜를 자동으로 생성할 수 있는 date_range() 함수를 설명하기 위해 작성되었습니다.\n데이터 안 index에 날짜를 일일이 기입하는 대신 pandas의 date_range()를 활용하면 값이 많을 때 편리합니다.\ndate_range()는 하기와 같이 사용하면 됩니다.\n\u0026gt;\u0026gt;\u0026gt; pd.date_range(start=\u0026#39;날짜\u0026#39;, end=\u0026#39;날짜\u0026#39;, freq=\u0026#39;주기\u0026#39;) 예시를 들어 설명해 보겠습니다.\n\u0026gt;\u0026gt;\u0026gt; pd.date_range(start=\u0026#39;2024/01/01\u0026#39;, end=\u0026#39;2024/01/07\u0026#39;) DatetimeIndex([\u0026#39;2024-01-01\u0026#39;, \u0026#39;2024-01-02\u0026#39;, \u0026#39;2024-01-03\u0026#39;, \u0026#39;2024-01-04\u0026#39;, \u0026#39;2024-01-05\u0026#39;, \u0026#39;2024-01-06\u0026#39;, \u0026#39;2024-01-07\u0026#39;], dtype=\u0026#39;datetime64[ns]\u0026#39;, freq=\u0026#39;D\u0026#39;) 출력된 결과와 같이 시작일인 \u0026lsquo;2024/01/01\u0026rsquo;부터 종료일인 \u0026lsquo;2024/01/07\u0026rsquo;이 출력된 것을 확인할 수 있습니다.\n또 다른 예시를 보겠습니다.\n\u0026gt;\u0026gt;\u0026gt; pd.date_range(start=\u0026#39;2024-01-01 08:00\u0026#39;, periods = 4, freq = \u0026#39;H\u0026#39;) DatetimeIndex([\u0026#39;2024-01-01 08:00:00\u0026#39;, \u0026#39;2024-01-01 09:00:00\u0026#39;, \u0026#39;2024-01-01 10:00:00\u0026#39;, \u0026#39;2024-01-01 11:00:00\u0026#39;], dtype=\u0026#39;datetime64[ns]\u0026#39;, freq=\u0026#39;H\u0026#39;) 결괏값을 보면 시작일인 \u0026lsquo;2024-01-01\u0026rsquo;의 08시부터 주기인 \u0026lsquo;H\u0026rsquo; (시간 단위)를 토대로 4개의 결괏값이 나온 것을 확인할 수 있습니다.\nfreq(주기)를 설정할 경우, 하기의 링크 내 Offset aliases를 참조하면 여러 가지 형태로 출력이 가능합니다.\n참조: https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases\n","date":"2024년 1월 4일","permalink":"/ko/posts/2/","section":"블로그 글","summary":"\u003cp\u003e본 글은 pandas 라이브러리 내 날짜를 자동으로 생성할 수 있는 date_range() 함수를 설명하기 위해 작성되었습니다.\u003c/p\u003e","title":"파이썬 - pandas 날짜 자동 생성 date_range"},{"content":"","date":null,"permalink":"/ko/tags/sequence-types/","section":"Tags","summary":"","title":"Sequence Types"},{"content":"본 글은 파이썬에서 값이 연속적으로 이어져 있는 자료형인 시퀀스 자료형(sequence types)을 설명하기 위해 작성되었습니다.\n시퀀스 자료형이란? #시퀀스 자료형(sequence types)이란 값이 연속적으로 이어져 있는 자료형을 말합니다.\n시퀀스 자료형은 공통적인 동작과 기능을 제공한다는 점이 가장 큰 특징이라고 할 수 있습니다.\n리스트 [1, 2, 3, 4, 5] [1, 2, 3, 4, 5] 튜플 (1, 2, 3, 4, 5) (1, 2, 3, 4, 5) range range(5) 0, 1, 2, 3, 4 문자열 \u0026lsquo;Hello\u0026rsquo; H e l l o 위와 같이 시퀀스 자료형에는 리스트, 튜플, range, 문자열이 있으며, (bytes, bytearray) 또한 이에 해당됩니다.\n시퀀스 자료형으로 만든 객체를 시퀀스 객체라고 하며, 객체 각각의 값을 요소(element)라고 합니다.\n시퀀스 객체 내 특정 값 확인 #시퀀스 객체 내에 특정한 값이 있는지 확인하기 위해서는 하기와 같이 in이나 not in을 사용할 수 있습니다.\n\u0026gt;\u0026gt;\u0026gt; a = \u0026#34;Hello\u0026#34; \u0026gt;\u0026gt;\u0026gt; \u0026#34;H\u0026#34; in a True \u0026gt;\u0026gt;\u0026gt; \u0026#34;A\u0026#34; in a False # not in 특정 값이 없는지 확인 \u0026gt;\u0026gt;\u0026gt; \u0026#34;ell\u0026#34; not in a False \u0026gt;\u0026gt;\u0026gt; \u0026#34;Python\u0026#34; not in a True in 연산자를 사용했을 때 특정한 값이 있으면 True, 없으면 False가 나오고 반대로 not in 연산자를 사용했을 때 특정한 값이 없으면 True, 있으면 False가 나옵니다.\n시퀀스 객체 연결 #시퀀스 객체는 + 연산자를 사용하여 연결할 수 있습니다.\n\u0026gt;\u0026gt;\u0026gt; a = [0, 1, 2, 3] \u0026gt;\u0026gt;\u0026gt; b = [4, 5, 6] \u0026gt;\u0026gt;\u0026gt; a + b [0, 1, 2, 3, 4, 5, 6] 단, range는 + 연산자로 객체 연결할 수 없습니다.\n\u0026gt;\u0026gt;\u0026gt; range(0, 5) + range(5, 10) TypeError Traceback (most recent call last) \u0026lt;ipython-input-7-88e74efcb3c0\u0026gt; in \u0026lt;cell line: 1\u0026gt;() ----\u0026gt; 1 range(0, 5) + range(5, 10) TypeError: unsupported operand type(s) for +: \u0026#39;range\u0026#39; and \u0026#39;range\u0026#39; 따라서, range를 튜플이나 리스트로 변환하여 연결하는 것이 가능합니다. \u0026gt;\u0026gt;\u0026gt; tuple(range(0, 5)) + tuple(range(5, 10)) (0, 1, 2, 3, 4, 5, 6, 7, 8, 9) \u0026gt;\u0026gt;\u0026gt; list(range(0, 5)) + list(range(5, 10)) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 시퀀스 객체 반복 #시퀀스 객체는 * 연산자를 사용하여 반복할 수 있습니다.\n정수 * 시퀀스 객체 또는 시퀀스 * 정수로 반복 가능합니다.\n\u0026gt;\u0026gt;\u0026gt; [0, 1, 2, 3] * 3 [0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3] 그러나, 시퀀스 객체 연결 방법과 마찬가지로 ranges는 * 연산자를 사용하여 반복할 수 없습니다.\n\u0026gt;\u0026gt;\u0026gt; range(0,10) * 3 TypeError Traceback (most recent call last) \u0026lt;ipython-input-11-824dcf3cff8f\u0026gt; in \u0026lt;cell line: 1\u0026gt;() ----\u0026gt; 1 range(0,10) * 3 TypeError: unsupported operand type(s) for *: \u0026#39;range\u0026#39; and \u0026#39;int\u0026#39; 따라서, 튜플이나 리스트로 변환하여 반복이 가능합니다.\n시퀀스 객체 요소 개수 확인 #시퀀스 객체의 요소 개수는 len 함수를 사용하여 확인이 가능합니다.\n# 리스트 \u0026gt;\u0026gt;\u0026gt; a = [1, 2, 3, 4, 5] \u0026gt;\u0026gt;\u0026gt; len(a) 5 # 튜플 \u0026gt;\u0026gt;\u0026gt; b = (6, 7, 8, 9, 10) \u0026gt;\u0026gt;\u0026gt; len(b) 5 # range len(range(0, 5, 2)) # -\u0026gt; 0에서 5까지 2씩 증가하여 0, 2, 4 3 # 문자열 \u0026gt;\u0026gt;\u0026gt; c = \u0026#34;Hello, World\u0026#34; \u0026gt;\u0026gt;\u0026gt; len(c) 12 ","date":"2024년 1월 2일","permalink":"/ko/posts/1/","section":"블로그 글","summary":"\u003cp\u003e본 글은 파이썬에서 값이 연속적으로 이어져 있는 자료형인 시퀀스 자료형(sequence types)을 설명하기 위해 작성되었습니다.\u003c/p\u003e","title":"파이썬 - 시퀀스 자료형"},{"content":"This is the advanced tag. Just like other listing pages in Congo, you can add custom content to individual taxonomy terms and it will be displayed at the top of the term listing. 🚀\nYou can also use these content pages to define Hugo metadata like titles and descriptions that will be used for SEO and other purposes.\n","date":null,"permalink":"/ko/tags/advanced/","section":"Tags","summary":"This is the advanced tag.","title":"advanced"},{"content":"","date":null,"permalink":"/ko/categories/","section":"Categories","summary":"","title":"Categories"}]