[{"content":"The purpose of this project is to assist these entrepreneurs by providing density and sales forecasts to aid in their business planning.\n","date":"2024年3月7日","permalink":"/ja/projects/convenience_insights/convenience_insights/","section":"Projects","summary":"\u003cp\u003eThe purpose of this project is to assist these entrepreneurs by providing density and sales forecasts to aid in their business planning.\u003c/p\u003e","title":"Convenience Insights"},{"content":"The purpose of this project is to develop a dashboard using Seoul real estate data to visualize real estate market trends and assist users in making real estate investment decisions.\n","date":"2024年2月8日","permalink":"/ja/projects/seoul_real_estate_insight/seoul_real_estate_insight/","section":"Projects","summary":"\u003cp\u003eThe purpose of this project is to develop a dashboard using Seoul real estate data to visualize real estate market trends and assist users in making real estate investment decisions.\u003c/p\u003e","title":"Seoul Real Estate Insight"},{"content":"","date":null,"permalink":"/ja/projects/","section":"Projects","summary":"","title":"Projects"},{"content":"","date":null,"permalink":"/ja/tags/seoul_real_estate_insight/","section":"Tags","summary":"","title":"Seoul_real_estate_insight"},{"content":" ","date":null,"permalink":"/ja/tags/","section":"Tags","summary":" ","title":"Tags"},{"content":"Welcome to my website! I\u0026rsquo;m really happy you stopped by.\n","date":null,"permalink":"/ja/","section":"Welcome to Congo!","summary":"Welcome to my website!","title":"Welcome to Congo!"},{"content":"","date":null,"permalink":"/ja/tags/feature-engineering/","section":"Tags","summary":"","title":"Feature Engineering"},{"content":"","date":null,"permalink":"/ja/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning"},{"content":"","date":null,"permalink":"/ja/tags/python/","section":"Tags","summary":"","title":"Python"},{"content":" 様々なトピックでブログ投稿を掲載しております。 🧑🏻‍💻 ","date":null,"permalink":"/ja/posts/","section":"ブログ投稿","summary":" 様々なトピックでブログ投稿を掲載しております。 🧑🏻‍💻 ","title":"ブログ投稿"},{"content":"","date":null,"permalink":"/ja/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/","section":"Tags","summary":"","title":"機械学習"},{"content":"この投稿では、生データ（raw data）をモデルが理解できる形に変換するプロセスである特徴量エンジニアリング（Feature Engineering）が何であり、なぜ重要なのか、そして基本的な特徴量エンジニアリング技術について説明します。\n特徴量エンジニアリングとは？ #特徴量エンジニアリングは、与えられた生データを機械学習モデルが効果的に機能することができるフィーチャー（Feature）または変数に変換するプロセスです。このプロセスには、不要な情報の削除、有用な情報の抽出と変換、そしてモデルの学習プロセスでより良く機能するようにデータを調整する作業が含まれます。\n特徴量エンジニアリングの重要性 #特徴量エンジニアリングは、機械学習モデルの性能を大幅に向上させることができます。良いフィーチャーはモデルがデータからパターンをよりよく学習することを可能にし、結果として予測の精度を高めます。一方、関連性が低いまたは誤ったフィーチャーはモデルの性能を低下させることがあります。したがって、特徴量エンジニアリングはモデルの性能を最大化するために不可欠なプロセスです。\n特徴量エンジニアリング技術 #特徴量エンジニアリングには様々な技術があり、以下は最も基本的ないくつかの技術です：\n欠損値の処理: データに欠損値が存在する場合、これを処理することが重要です。欠損値を平均値、中央値、最頻値などで置き換えるか、欠損値がある行を削除する方法があります。 カテゴリカルデータの処理: モデルによってはカテゴリカルデータを直接処理できない場合が多いです。ワンホットエンコーディング（One-Hot Encoding）、ラベルエンコーディング（Label Encoding）などの方法を使用してカテゴリカルデータを数値データに変換することができます。 フィーチャースケーリング:: 様々なフィーチャーのスケールを調整して、モデルがフィーチャーを公平に評価できるようにします。標準化（Standardization）と正規化（Normalization）がこれに該当します。 フィーチャー選択（Feature Selection）: モデルの複雑さを減らし、オーバーフィッティングを防ぐために重要なフィーチャーのみを選択します。統計的方法、モデルベースの方法などがあります。 フィーチャー生成（Feature Creation）: 既存のフィーチャーを組み合わせたり変形させて新しいフィーチャーを生成します。これにより、モデルがデータをよりよく理解できるようになります。 エンコーディング変換 #カテゴリカルデータは、テキストで表されたデータカテゴリを意味します。ほとんどの機械学習アルゴリズムは数値データを入力として受け取るため、これらのカテゴリカルデータを適切な数値形式に変換するプロセスが必須です。エンコーディング変換には主に二つの方法が使用されます。\nワンホットエンコーディング（One-Hot Encoding）: 各カテゴリを一つの列に変換し、そのカテゴリに該当する場合のみ1の値を、そうでない場合は0の値を持ちます。この方式はカテゴリ間の順序や重要性を考慮しないため、モデルが各カテゴリを等しく扱うようにします。 ラベルエンコーディング（Label Encoding）: 各カテゴリカルデータに順番に番号を付けて数値に変換します。例えば、「red」、「blue」、「green」をそれぞれ0、1、2に変換することができます。ラベルエンコーディングはカテゴリの数だけ次元を増やさないものの、数値の大小がモデルに影響を与える可能性があるため注意が必要です。 スケーリング #フィーチャースケーリングは、異なる単位または範囲を持つデータを一定の範囲やスケールに統一するプロセスです。これにより、すべてのフィーチャーがモデルに等しく影響を与えるようにします。スケーリングには主に二つの方法が使用されます。\n標準化（Standardization）: データの平均を0、標準偏差を1に調整します。この方式はデータの分布が正規分布に従わない時に有用であり、外れ値に対してもあまり敏感ではありません。 正規化（Normalization）: データの値を0と1の間の範囲に調整します。最も一般的な方法は最小値と最大値を使用することであり、すべてのデータポイントが同じスケールを持つようになります。 ","date":"2024年2月22日","permalink":"/ja/posts/31/","section":"ブログ投稿","summary":"\u003cp\u003eこの投稿では、生データ（raw data）をモデルが理解できる形に変換するプロセスである特徴量エンジニアリング（Feature Engineering）が何であり、なぜ重要なのか、そして基本的な特徴量エンジニアリング技術について説明します。\u003c/p\u003e","title":"機械学習 - 特徴量エンジニアリング（Feature Engineering）"},{"content":"","date":null,"permalink":"/ja/tags/%E7%89%B9%E5%BE%B4%E9%87%8F%E3%82%A8%E3%83%B3%E3%82%B8%E3%83%8B%E3%82%A2%E3%83%AA%E3%83%B3%E3%82%B0/","section":"Tags","summary":"","title":"特徴量エンジニアリング"},{"content":"","date":null,"permalink":"/ja/tags/random-search/","section":"Tags","summary":"","title":"Random Search"},{"content":"","date":null,"permalink":"/ja/tags/%E3%83%A9%E3%83%B3%E3%83%80%E3%83%A0%E3%82%B5%E3%83%BC%E3%83%81/","section":"Tags","summary":"","title":"ランダムサーチ"},{"content":"この投稿では、マシンラーニングでモデルの性能を最大化するためのハイパーパラメータチューニングの一つであるランダムサーチ(Random Search)の概念を説明し、Scikit-learnライブラリを使用した実装例を紹介します。\nランダムサーチとは？ #ランダムサーチは、指定されたパラメータ空間内で無作為に選択された組み合わせを評価することで、ハイパーパラメータの最適な組み合わせを見つける方法です。グリッドサーチ(Grid Search)が指定されたパラメータの全ての組み合わせを体系的に探索するのに対し、ランダムサーチは探索空間からランダムに組み合わせを選択して評価します。特にハイパーパラメータの次元が高い場合や、探索空間が大きい場合に有用で、しばしばより少ない時間で同等またはそれ以上の結果を出すことができます。\n主要パラメータ #Scikit-learnでRandomizedSearchCVクラスを通じてランダムサーチを実装できます。主要なパラメータは以下の通りです：\nestimator: 最適化するモデルを指定します。例えば、RandomForestClassifier()、SVC()などです。 param_distributions: 探索するパラメータ空間を指定します。各パラメータに対して連続分布を指定するか、リストを提供できます。 n_iter: 無作為に選択するパラメータ設定の数を指定します。この値が大きいほど、より多くの組み合わせを探索しますが、計算時間も増加します。 scoring: モデルの性能を評価する基準を指定します。例えば、「accuracy」、「f1」などです。 cv: 交差検証の分割戦略を指定します。整数値を入力すると、その値でk-fold交差検証を実行します。 random_state: 結果の再現性を確保するために、乱数生成器のシード値を指定します。 RandomizedSearchCVの実装 #以下はRandomizedSearchCVを使用して分類器の最適なハイパーパラメータを見つける例です。以下のコードではサポートベクターマシン(SVM)を使用しています。\n\u0026gt;\u0026gt;\u0026gt; from sklearn.model_selection import RandomizedSearchCV \u0026gt;\u0026gt;\u0026gt; from sklearn.svm import SVC \u0026gt;\u0026gt;\u0026gt; from sklearn.datasets import load_iris \u0026gt;\u0026gt;\u0026gt; from sklearn.model_selection import train_test_split \u0026gt;\u0026gt;\u0026gt; import scipy.stats as stats # データをロードする \u0026gt;\u0026gt;\u0026gt; iris = load_iris() \u0026gt;\u0026gt;\u0026gt; X, y = iris.data, iris.target # データセットを訓練セットとテストセットに分割 \u0026gt;\u0026gt;\u0026gt; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # サポートベクターマシンのインスタンス化 \u0026gt;\u0026gt;\u0026gt; svc = SVC() # 探索するハイパーパラメータ空間の定義 \u0026gt;\u0026gt;\u0026gt; param_distributions = { \u0026#39;C\u0026#39;: stats.uniform(0.1, 1000), \u0026#39;gamma\u0026#39;: stats.uniform(0.0001, 0.1), \u0026#39;kernel\u0026#39;: [\u0026#39;linear\u0026#39;, \u0026#39;rbf\u0026#39;] } # RandomizedSearchCVのインスタンス化 \u0026gt;\u0026gt;\u0026gt; random_search.fit(X_train, y_train) # 最適なパラメータと最高の精度を出力 \u0026gt;\u0026gt;\u0026gt; print(\u0026#34;Best parameters:\u0026#34;, random_search.best_params_) \u0026gt;\u0026gt;\u0026gt; print(\u0026#34;Best cross-validation score: {:.2f}\u0026#34;.format(random_search.best_score_)) # テストセットでの性能評価 \u0026gt;\u0026gt;\u0026gt; accuracy = random_search.score(X_test, y_test) \u0026gt;\u0026gt;\u0026gt; print(\u0026#34;Test set accuracy: {:.2f}\u0026#34;.format(accuracy)) このコードは、指定されたC、gamma、kernelハイパーパラメータのランダムな組み合わせを探索し、最適な組み合わせを見つけます。RandomizedSearchCVは特に探索空間が広い場合に、グリッドサーチよりも効果的な方法です。\n","date":"2024年2月20日","permalink":"/ja/posts/28/","section":"ブログ投稿","summary":"\u003cp\u003eこの投稿では、マシンラーニングでモデルの性能を最大化するためのハイパーパラメータチューニングの一つであるランダムサーチ(Random Search)の概念を説明し、Scikit-learnライブラリを使用した実装例を紹介します。\u003c/p\u003e","title":"機械学習 - ランダムサーチ(Random Search)"},{"content":"","date":null,"permalink":"/ja/tags/grid-search/","section":"Tags","summary":"","title":"Grid Search"},{"content":"","date":null,"permalink":"/ja/tags/%E3%82%B0%E3%83%AA%E3%83%83%E3%83%89%E3%82%B5%E3%83%BC%E3%83%81/","section":"Tags","summary":"","title":"グリッドサーチ"},{"content":"機械学習でモデルの最適なパラメータを見つけるための効率的な方法の一つはグリッドサーチ(Grid Search)です。この投稿では、グリッドサーチとは何か、どのように機能するか、そしていつ使用すべきかについて説明します。\nグリッドサーチとは？ #グリッドサーチは、機械学習モデルのハイパーパラメータを最適化するための方法の一つです。この方法は指定されたハイパーパラメータのすべての組み合わせを試し、最も良い性能を発揮するパラメータの組み合わせを見つけます。各パラメータの組み合わせに対してクロスバリデーションを実施し、モデルの性能を評価します。このプロセスを通じて、最適なモデルを選択することができます。\n動作原理 #グリッドサーチは、まずユーザーが指定したハイパーパラメータの範囲やリストを入力として受け取ります。例えば、決定木分類器に対するグリッドサーチを行うとします。ユーザーは、木の深さ(depth)、分割のための最小サンプル数(min_samples_split)などのハイパーパラメータの範囲を設定します。グリッドサーチは、この範囲内のすべての可能な組み合わせに対してモデルを学習させ、クロスバリデーションを使用して各組み合わせの性能を評価します。性能評価方法としては、通常、正確度、精度、再現率、F1スコアなどが使用されます。評価が終わった後、最も性能が良いパラメータの組み合わせが選択されます。\n長所と短所 #長所：\n使いやすく、理解しやすい すべての可能な組み合わせを探索するため、最適な組み合わせを見つける可能性が高い 短所：\n計算コストが非常に高い。パラメータの数と範囲が大きくなるほど、必要な計算量が指数関数的に増加 最適な組み合わせを見つけるのに時間がかかる 利用用途 #グリッドサーチは、パラメータの範囲が比較的小さく、モデルの学習時間が短い場合に適しています。また、最適なハイパーパラメータの組み合わせを見つけることが重要で、計算リソースが十分にある場合に使用すると良いでしょう。しかし、パラメータ空間が非常に大きい場合や、学習時間が非常に長いモデルの場合は、ランダムサーチ(Random Search)、ベイジアン最適化などの他のハイパーパラメータ最適化技術を検討することが望ましいです。\nGridSearchCV #GridSearchCVは、Scikit-learnライブラリのモデル選択モジュールに含まれるクラスで、与えられたモデルのハイパーパラメータ空間を交差検証を通じて探索し、最適なパラメータを見つけるために使用されます。このクラスのコンストラクターに渡すことができる主要なパラメータは以下の通りです:\n主要パラメータ # estimator: 最適化するモデル。例えば、RandomForestClassifier()、SVC()などのscikit-learnの推定器(estimator)オブジェクトがこれに該当します。 param_grid: 探索するパラメータの辞書。例えば、{\u0026rsquo;n_estimators\u0026rsquo;: [100, 200], \u0026lsquo;max_features\u0026rsquo;: [\u0026lsquo;auto\u0026rsquo;, \u0026lsquo;sqrt\u0026rsquo;]}のように設定でき、これはn_estimatorsとmax_featuresパラメータに対してそれぞれ[100, 200]と[\u0026lsquo;auto\u0026rsquo;, \u0026lsquo;sqrt\u0026rsquo;]の値を探索することを意味します。 scoring: モデルの性能を評価する基準。文字列で指定され、例えば「accuracy」、「f1」などが使用されます。scikit-learnで事前定義された他のscoringオプションを使用することもできます。 cv: 交差検証分割戦略。例えば、5は5分割交差検証を意味します。KFold、StratifiedKFoldなどのscikit-learnの分割器(splitter)オブジェクトを直接渡すこともできます。 refit: 最適なパラメータを見つけた後、全データセットに対してモデルを再学習するかどうかを決定します。デフォルト値はTrueで、最適なパラメータで全データセットに対してモデルを学習します。 GridSearchCVの実装 #以下は、Scikit-learnのGridSearchCVを使用して分類器の最適なハイパーパラメータを見つける簡単な例です。ここでは、決定木分類器(DecisionTreeClassifier)を使用します。\n\u0026gt;\u0026gt;\u0026gt; from sklearn.model_selection import GridSearchCV \u0026gt;\u0026gt;\u0026gt; from sklearn.tree import DecisionTreeClassifier \u0026gt;\u0026gt;\u0026gt; from sklearn.datasets import load_iris \u0026gt;\u0026gt;\u0026gt; from sklearn.model_selection import train_test_split # データをロードする \u0026gt;\u0026gt;\u0026gt; iris = load_iris() \u0026gt;\u0026gt;\u0026gt; X = iris.data \u0026gt;\u0026gt;\u0026gt; y = iris.target # 訓練セットとテストセット分割 \u0026gt;\u0026gt;\u0026gt; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # モデル設定 \u0026gt;\u0026gt;\u0026gt; estimator = DecisionTreeClassifier() # 探索するパラメータグリッド設定 \u0026gt;\u0026gt;\u0026gt; param_grid = { \u0026#39;max_depth\u0026#39;: [None, 2, 4, 6, 8], \u0026#39;min_samples_split\u0026#39;: [2, 5, 10], \u0026#39;min_samples_leaf\u0026#39;: [1, 2, 4] } # GridSearchCV設定 \u0026gt;\u0026gt;\u0026gt; grid_search = GridSearchCV(estimator=estimator, param_grid=param_grid, scoring=\u0026#39;accuracy\u0026#39;, cv=5, refit=True) # グリッドサーチ実行 \u0026gt;\u0026gt;\u0026gt; grid_search.fit(X_train, y_train) # 最適パラメータと最高スコア出力 \u0026gt;\u0026gt;\u0026gt; print(\u0026#34;Best parameters:\u0026#34;, grid_search.best_params_) \u0026gt;\u0026gt;\u0026gt; print(\u0026#34;Best score:\u0026#34;, grid_search.best_score_) # テストデータでの性能評価 \u0026gt;\u0026gt;\u0026gt; test_accuracy = grid_search.score(X_test, y_test) \u0026gt;\u0026gt;\u0026gt; print(\u0026#34;Test accuracy:\u0026#34;, test_accuracy) ","date":"2024年2月19日","permalink":"/ja/posts/27/","section":"ブログ投稿","summary":"\u003cp\u003e機械学習でモデルの最適なパラメータを見つけるための効率的な方法の一つはグリッドサーチ(Grid Search)です。この投稿では、グリッドサーチとは何か、どのように機能するか、そしていつ使用すべきかについて説明します。\u003c/p\u003e","title":"機械学習 - グリッドサーチ(Grid Search)"},{"content":"","date":null,"permalink":"/ja/tags/.env/","section":"Tags","summary":"","title":".Env"},{"content":"この投稿では、.envファイルで環境変数を隠す方法を紹介します。\n今回はPython内のpython-dotenvを使用して.envファイルを隠します。\npython-dotenv のインストール #pip install python-dotenv コマンドを使って python-dotenv をインストールします。\n様々な環境変数の整理 #GitHub にアップロードする際に機密情報である API キーなど、様々な環境変数を .env ファイルに整理します。\n.env ファイルのアクティベーション #pythonCopy code from dotenv import load_dotenv import os # .env ファイルのアクティベーション load_dotenv() SERVICE_KEY = os.getenv(\u0026#39;SERVICE_KEY\u0026#39;) .gitignore ファイルの確認 #通常、.gitignore ファイルには既に .env が含まれていますが、GitHub にプッシュする前に再確認します。正しく .env ファイルが設定されている場合は、git add -\u0026gt; commit -\u0026gt; push を進めることができます。\nもし .env ファイル以外のファイルに変数が保存されている場合は、.gitignore にそれを明示する必要があります。\ngit にプッシュする #git にプッシュした後、GitHub を確認します。\n.env ファイルがアップロードされていないことが確認できれば、正しく処理されています。\n","date":"2024年2月1日","permalink":"/ja/posts/12/","section":"ブログ投稿","summary":"\u003cp\u003eこの投稿では、.envファイルで環境変数を隠す方法を紹介します。\u003c/p\u003e","title":".envで環境変数隠す方法"},{"content":"","date":null,"permalink":"/ja/tags/%E7%92%B0%E5%A2%83%E5%A4%89%E6%95%B0/","section":"Tags","summary":"","title":"環境変数"},{"content":"","date":null,"permalink":"/ja/tags/scipy/","section":"Tags","summary":"","title":"Scipy"},{"content":"","date":null,"permalink":"/ja/tags/ttest_rel/","section":"Tags","summary":"","title":"Ttest_rel"},{"content":"","date":null,"permalink":"/ja/tags/%E5%AF%BE%E5%BF%9C%E3%81%AE%E3%81%82%E3%82%8Bt%E6%A4%9C%E5%AE%9A/","section":"Tags","summary":"","title":"対応のあるt検定"},{"content":"","date":null,"permalink":"/ja/tags/%E7%B5%B1%E8%A8%88/","section":"Tags","summary":"","title":"統計"},{"content":"この投稿は、統計学で使用される対応のあるt検定について説明するために書かれました。\n今回もPythonライブラリScipyを利用して対応のあるt検定を行います。\n対応のあるt検定 #対応のあるt検定は、2つの関連するグループ間の平均を比較する統計的技術です。この方法は、通常、同一の被験者グループに対して2つの測定値がある場合に適用されます。対応のあるt検定は、2つの関連するグループ間の平均の差が統計的に有意かどうかを判断するために使用されます。\n1. 仮説設定 # H₀ : 𝜇D = 0 → 帰無仮説 (𝜇𝐷 = 𝜇₁ - 𝜇₂) 実験前後の平均の差は0です。 H₁ : 𝜇D ≠ 0 → 対立仮説 実験前後の平均の差は0ではありません。 2. 正規性検定 #2つのグループのサンプル数が30未満の場合、正規性検定を行う必要があります。\n2つのグループのサンプル数が30以上の場合、中心極限定理により正規性が満たされたと仮定します。\nScipyでの正規性検定はShapiro-Wilk検定を通じて確認可能です。 4. 対応のあるt検定統計量の計算 #2つのグループの平均と標準偏差を使用して、対応のあるt検定統計量を計算します。\n5. 決定/結論 #計算されたT統計量が臨界値を超えた場合、帰無仮説を棄却し、対立仮説を採用します。\nそうでない場合、帰無仮説を棄却しません。\n統計的に有意な差がある場合、2つのグループ間に平均の差が存在すると結論付けます。\nPythonライブラリScipyの利用方法 #以下はPythonのScipyライブラリを利用して対応のあるt検定を行う方法です。\n今回扱うデータには、Aのクラスでは筋トレが集中力を向上させるという話があり、Aが実際に筋トレを始める前と後で比較することにしました。Aは20人に筋トレを行わせた後、トレーニング前と後で集中力測定テストを受けさせた結果です。\n筋トレ前後で集中力に有意な差があるかどうかを対応のあるt検定を通じて調べたいと思います。\n仮説は以下の通りです。\n帰無仮説 : 筋トレ前後のテスト平均は同じです。\n対立仮説 : 筋トレ前後のテスト平均は同じではありません。\n有意水準は0.05に設定します。\nまずはデータを読み込みましょう。\n\u0026gt;\u0026gt;\u0026gt; import pandas as pd \u0026gt;\u0026gt;\u0026gt; from scipy import stats \u0026gt;\u0026gt;\u0026gt; df = pd.read_csv(\u0026#34;./data/ch11_training_rel.csv\u0026#34;) \u0026gt;\u0026gt;\u0026gt; df.head() 전 후 0 59 41 1 52 63 2 55 68 3 61 59 4 59 84 次に、正規性検定を行いましょう。\n\u0026gt;\u0026gt;\u0026gt; a = stats.shapiro(df[\u0026#39;前\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; b = stats.shapiro(df[\u0026#39;後\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; print(a, b) ShapiroResult(statistic=0.9670045375823975, pvalue=0.690794825553894) ShapiroResult(statistic=0.9786625504493713, pvalue=0.9156817197799683) 結果はどちらもp値が0.05より大きく、正規性が満たされています。\n次に、Scipyライブラリ内のttest_relを使ってt統計量とp値を求めます。\n\u0026gt;\u0026gt;\u0026gt; t_score, p_value = stats.ttest_rel(df[\u0026#39;前\u0026#39;], df[\u0026#39;後\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; print(round(t_score, 4), round(p_value, 2)) -2.2042 0.04 p値が有意水準0.05より小さいため、帰無仮説(筋トレ前後の平均が同じである)が棄却されました。したがって、筋トレ前後の平均スコアには有意な差があると言えます。\n","date":"2024年1月19日","permalink":"/ja/posts/10/","section":"ブログ投稿","summary":"\u003cp\u003eこの投稿は、統計学で使用される対応のあるt検定について説明するために書かれました。\u003c/p\u003e","title":"統計 - 対応のあるt検定"},{"content":"","date":null,"permalink":"/ja/tags/ttest_ind/","section":"Tags","summary":"","title":"Ttest_ind"},{"content":"この投稿は、統計学で使用される独立標本 t検定について説明するために書かれました。\n今回もPythonライブラリScipyを利用して独立標本t検定を行います。\n独立標本t検定 #独立標本t検定は、2つの独立した標本間の平均の差が統計的に有意かどうかを検定する統計分析技術です。これは、2つのグループ間の平均差が偶然に起こったものなのか、それとも実際に存在するものなのかを判断するために使用されます。\n独立標本 t検定の主要なステップは次のとおりです。\n1. 仮説の設定 # H₀ : 𝜇₁ = 𝜇₂ → 帰無仮説 2つのグループの平均は同じである。 H₁ : 𝜇₁ ≠ 𝜇₂ → 対立仮説 2つのグループの平均は異なる。 2. 正規性の検定 #2つのグループの標本数が30未満の場合、正規性の検定が必要です。\n2つのグループの標本数が30以上の場合、中心極限定理により正規性が満たされると仮定します。\nScipyでの正規性検定はShapiro-Wilk検定を通じて確認可能です。 3. 等分散性の検定 #2つのグループのデータ数が同じ場合、分散は等しいと仮定します。\n2つのグループのデータ数が異なる場合、分散が等しいかどうかを確認するために等分散性の検定を行うことができます。\nScipyでの等分散性検定はLevene検定を通じて確認可能です。 4. 独立標本 t統計量の計算 #2つのグループの平均と標準偏差を使用して独立標本 t統計量を計算します。\n5. 決定/結論 #計算されたt統計量が臨界値を超えた場合、帰無仮説を棄却し対立仮説を採用します。\nそうでない場合は、帰無仮説を棄却しません。\n統計的に有意な差がある場合、2つのグループ間に平均の差が存在すると結論付けます。\n独立標本t検定は、2つのグループ間の平均の差を比較するのに有用であり、実験群と対照群間の差を調査したり、2つの条件間の効果を確認するなどの状況で適用できます。\nPythonライブラリScipyの利用方法 #次に、PythonのScipyライブラリを利用して独立標本 t検定を行います。\n今回扱うデータには、人文系の学生が多いAクラスでは筋力トレーニングをする学生が急増し、Aはもし筋力トレーニングが集中力を向上させる効果があるなら、自分のクラスと通常から筋力トレーニングをしている体育系の学生が多いBクラスとの間に集中力テストの平均に差が出ないかと考え、Bクラスにも集中力テストを受けさせた結果です。\nAとBのクラスの集中力に有意な差があるかどうかを独立標本 t検定を通じて調べたいと思います。\n仮説は以下の通りです。\n帰無仮説 : AとBのクラスの平均は同じである。\n対立仮説 : AとBのクラスの平均は同じではない。\n有意水準は0.05に設定します。\nまず、データを読み込みましょう。\n\u0026gt;\u0026gt;\u0026gt; import pandas as pd \u0026gt;\u0026gt;\u0026gt; from scipy import stats \u0026gt;\u0026gt;\u0026gt; df = pd.read_csv(\u0026#34;./data/ch11_training_ind.csv\u0026#34;) \u0026gt;\u0026gt;\u0026gt; df.head() A B 0 47 49 1 50 52 2 37 54 3 60 48 4 39 51 次に、正規性検定を行います。\n\u0026gt;\u0026gt;\u0026gt; a = stats.shapiro(df[\u0026#39;A\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; b = stats.shapiro(df[\u0026#39;B\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; print(a, b) ShapiroResult(statistic=0.9685943722724915, pvalue=0.7249553203582764) ShapiroResult(statistic=0.9730021357536316, pvalue=0.8165789842605591) 結果は、どちらもp値が0.05より大きいので正規性が満たされています。\n次に、このデータはデータの個数が同じなので等分散と仮定しますが、各グループのデータ数が異なる場合は分散が等しいかを検定する必要があるため、Levene検定を通じて以下のように確認できます。\n\u0026gt;\u0026gt;\u0026gt; stats.levene(df[\u0026#39;A\u0026#39;], df[\u0026#39;B\u0026#39;]) LeveneResult(statistic=2.061573118077718, pvalue=0.15923550057222613) p値が0.159なので帰無仮説(2つのグループの分散に差がない)が採用されます。\n次に、Scipyライブラリ内の ttest_indを使用してt統計量とp値を求めます。\n\u0026gt;\u0026gt;\u0026gt; t, p = stats.ttest_ind(df[\u0026#39;A\u0026#39;], df[\u0026#39;B\u0026#39;], equal_var=False) # equal_var=False: Welchの方法 \u0026gt;\u0026gt;\u0026gt; t, p (-1.760815724652471, 0.08695731107259362) p値が有意水準0.05より大きいため、帰無仮説（AとBのクラスの平均が同じである）が採用されました。したがって、AのクラスとBのクラスの間には平均スコアに有意な差があるとは言えません。\n","date":"2024年1月15日","permalink":"/ja/posts/9/","section":"ブログ投稿","summary":"\u003cp\u003eこの投稿は、統計学で使用される独立標本 t検定について説明するために書かれました。\u003c/p\u003e","title":"統計 - 独立標本t検定"},{"content":"","date":null,"permalink":"/ja/tags/%E7%8B%AC%E7%AB%8B%E6%A8%99%E6%9C%AC-t%E6%A4%9C%E5%AE%9A/","section":"Tags","summary":"","title":"独立標本 T検定"},{"content":"","date":null,"permalink":"/ja/tags/1%E6%A8%99%E6%9C%AC%E3%81%AEt%E6%A4%9C%E5%AE%9A/","section":"Tags","summary":"","title":"1標本のt検定"},{"content":"この投稿は、統計学で使用される1標本のt検定について説明するために作成されました。\nさらに、PythonのScipyライブラリを利用して単一標本 T検定を行います。\n1標本のt検定とは？ #1標本のt検定　は、統計分析で使用される仮説検定の方法の一つで、一つの標本に対する平均を検定するのに使用されます。主に母集団の平均が特定の値と同じかどうかを確認したい場合に適用されます。\n1標本のt検定は次のような手順で行われます。\n1. 仮説の設定 # H₀ : 𝜇 = 𝜇₀ → 帰無仮説 母平均と標本平均は同じである。 H₁ : 𝜇 ≠ 𝜇₀ → 対立仮説 母平均と標本平均は同じではない。 2. 標本の抽出 #母集団から標本を抽出し、その標本の平均を計算します。\n3. 仮説検定の統計量の計算 #t-統計量を計算します。これは、標本平均と仮説に基づく期待平均間の差を示します。\n4. 決定/結論 #計算されたt-統計量が棄却域に入る場合、帰無仮説を棄却し、対立仮説を採用します。\nそうでない場合は、帰無仮説を棄却しません。\n両側検定の場合、棄却域はt分布の両端に対称的な部分です。 帰無仮説を棄却した場合、標本が母集団と異なると結論付けます。\n逆に棄却できなかった場合、統計的に有意ではないと結論付けます。\nPythonライブラリScipyの利用方法 #次に、PythonのScipyライブラリを利用して単一標本 T検定を行います。\n今回扱うデータには、31本の木の周囲長、高さ、体積が記録されています。\nこの標本の平均が母平均と一致するかどうかを単一標本 T検定で調べたいと思います。仮説は以下のとおりです。\n有意水準は0.05に設定します。\n仮説検定\n帰無仮説 : 平均は75である。\n対立仮説 : 平均は75ではない。\nまず、データを読み込みます。\n\u0026gt;\u0026gt;\u0026gt; import pandas as pd \u0026gt;\u0026gt;\u0026gt; df = pd.read_csv(\u0026#34;./data/trees.csv\u0026#34;) \u0026gt;\u0026gt;\u0026gt; df.head() Girth Height Volume 0 8.3 70 10.3 1 8.6 65 10.3 2 8.8 63 10.2 3 10.5 72 16.4 4 10.7 81 18.8 また、標本平均「Height」の平均を計算しましょう。\n\u0026gt;\u0026gt;\u0026gt; result = df[\u0026#39;Height\u0026#39;].mean() \u0026gt;\u0026gt;\u0026gt; round(result, 2) # (小数点第2位まで四捨五入して計算) 76.0 次に、1標本のt検定のためにScipyライブラリを読み込みます。\n\u0026gt;\u0026gt;\u0026gt; from scipy import stats 単一標本 T検定にはScipy内の ttest_1samp　を使用して検定します。 参照　: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_1samp.html\n次に、仮説検定の統計量を計算しましょう。\n\u0026gt;\u0026gt;\u0026gt; from math import sqrt \u0026gt;\u0026gt;\u0026gt; t_score, p_value = stats.ttest_1samp(df[\u0026#39;Height\u0026#39;], popmean=75) \u0026gt;\u0026gt;\u0026gt; print(round(t_score, 2), round(p_value, 2)) 0.87 0.39 popmeanは帰無仮説で予想された平均と同じです。\n上記の統計量に対するp値を計算した後（小数点第4位まで四捨五入して計算）、有意水準0.05での仮説検定の結果、帰無仮説を棄却するかどうかを確認しましょう。\n\u0026gt;\u0026gt;\u0026gt; print(round(p_value, 4)) 0.3892 \u0026gt;\u0026gt;\u0026gt; if p_value \u0026gt;= 0.05: print(\u0026#34;Accept\u0026#34;) else: print(\u0026#34;Reject\u0026#34;) Accept したがって、標本平均は母平均と同じであるという帰無仮説を棄却できませんでした。\n","date":"2024年1月13日","permalink":"/ja/posts/8/","section":"ブログ投稿","summary":"\u003cp\u003eこの投稿は、統計学で使用される1標本のt検定について説明するために作成されました。\u003c/p\u003e","title":"統計 - 1標本のt検定"},{"content":"","date":null,"permalink":"/ja/tags/%E5%9B%9E%E5%B8%B0%E5%88%86%E6%9E%90/","section":"Tags","summary":"","title":"回帰分析"},{"content":"この投稿は統計学における回帰分析について説明するために作成されました。\n回帰分析とは？ #回帰分析とは、一つまたはそれ以上の独立変数が従属変数に与える影響を推定できる統計技術を指します。\n回帰分析の変数 #従属変数(Dependent Variable) yは影響を受ける変数で、応答変数(Response Variable)、結果変数(Outcome Variable)とも呼ばれます。モデルで予測しようとする変数であり、従属変数は他の変数によって影響を受けます。\n独立変数(Independent Variable) xは影響を与える変数で、説明変数(Explanatory Variable)、予測変数(Predictor Variable)とも呼ばれます。独立変数は従属変数に影響を与える変数であり、予測モデルを構築する際に使用されます。\n変数の数による回帰分析方法 #回帰分析は変数の数に応じてアプローチ方法が異なります。独立変数の数が一つならば単純線形回帰分析で、独立変数の数が二つ以上ならば多重線形回帰分析アプローチが可能です。\n1. 単純線形回帰分析(Simple Linear Regression) #一つの独立変数が従属変数に与える影響を推定できる統計技術を指します。予測値と実際のデータの誤差(=残差)が最も小さい直線を回帰直線として選びます。回帰直線は数多くの直線の中から、データに対して 残差平方和(Residual Sum of Squares, RSS) がより小さい直線を意味します。\nこれは 最小二乗法(Ordinary Least Square) を通じて行われます。\n最小二乗法 #測定値を基に平方和を作り、それが最小となる値を求めて処理する方法、残差平方和が最も小さい線を選びます。\n2. 多重線形回帰分析(Multiple Linear Regression) #二つ以上の独立変数が一つの従属変数に与える影響を推定する統計技術を指します。多重線形回帰分析では 回帰係数の有意性を判断することが重要です。なぜなら、選択された変数の組み合わせでモデルを確認できるように、すべての回帰係数の有意性が統計的に検証される必要があるからです。\n回帰係数の有意性は単純回帰分析の回帰係数有意性検討と同様に 回帰係数 t-統計量 を通じて確認が可能です。\n多重共線性(Multicollinearity) #多重共線性は回帰分析で独立変数間に強い相関関係が現れる現象を意味します。つまり、一つの独立変数を他の独立変数で予測できる場合に発生します。\n多重共線性が発生すると、各独立変数の回帰係数の正確な推定が困難になります。また、各独立変数の回帰係数が従属変数に与える影響力を正しく説明できなくなります。\n多重共線性をチェックする方法\n分散拡大係数 (VIF, Variance Inflation Factor):\n分散拡大係数は、各独立変数の分散がどれだけ増加したかを示し、この値が大きいと多重共線性が増加したと判断します。これは各独立変数を他の独立変数で線形回帰した結果の分散比率で計算されます。一般的に、分散拡大係数が 4以上であれば 多重共線性が存在する と判断され、 10以上であれば 重大な問題がある と解釈されます。\n回帰分析時の検討事項 #回帰分析を実施する場合、検討すべき事項は以下の三つの項目があります。\n1. 回帰係数は有意性 #該当係数の t-統計量のp値が　0.05より小さければ該当回帰係数が統計的に有意であると判断します。つまり、係数が従属変数に対して　有意な影響を与える　ことを意味します。\n2. モデルはどれくらい説明力を持つか？ #モデルがどれくらい説明力を持つか確認するためには　決定係数(𝑅²) を確認する必要があります。\n決定係数(Coefficient of Determination; 𝑅²) #決定係数は0から1の間の値で、1に近いほどモデルが従属変数の変動をよく説明していることを意味します。変動をよく説明するとは、回帰線にどれだけ変動があるか確認が可能であるという意味です。\n高い決定係数はモデルの予測能力が高いことを示します。\n3. モデルはデータによく適合しているか？ #モデルがデータによく適合しているかを判断するためには、残差をグラフに描き回帰診断を行います。残差は実際の値とモデルの予測値との差を意味し、これを視覚的に検討してモデルがデータにどれくらいよく適合しているかを確認します。一般的に、残差は正規分布に従い、特定のパターンや傾向がないことが望ましいです。また、残差の等分散性も確認される必要があります。異常値や影響力のあるデータポイントがあるかも検討し、必要に応じてこれを除去または調整してモデルの安定性を確認します。\nこれらの検討を通じて、回帰分析の信頼性とモデルの適合性を評価することができます。\n","date":"2024年1月12日","permalink":"/ja/posts/7/","section":"ブログ投稿","summary":"\u003cp\u003eこの投稿は統計学における回帰分析について説明するために作成されました。\u003c/p\u003e","title":"統計 - 回帰分析"},{"content":"","date":null,"permalink":"/ja/tags/%E4%BB%AE%E8%AA%AC%E6%A4%9C%E5%AE%9A/","section":"Tags","summary":"","title":"仮説検定"},{"content":"","date":null,"permalink":"/ja/tags/%E5%B8%B0%E7%84%A1%E4%BB%AE%E8%AA%AC/","section":"Tags","summary":"","title":"帰無仮説"},{"content":"","date":null,"permalink":"/ja/tags/%E5%AF%BE%E7%AB%8B%E4%BB%AE%E8%AA%AC/","section":"Tags","summary":"","title":"対立仮説"},{"content":"この投稿は、pandasライブラリを活用して複数のデータを一つに統合する方法について説明するために作成されました。\n統計学での仮説(Hypothesis)は、ある主張や推測を示す命題であり、母数に関する仮定/暫定的結論を意味します。\n仮説の種類 #仮説は以下のように二つの形式で表されます。\n1. 帰無仮説 (Null Hypothesis, H0) #帰無仮説は、元の比較対象として変化や差がないことを示す仮説であり、ある種のデフォルトの仮説です。\n検定方法によって帰無仮説の内容は異なります。例えば、「二つのグループの平均は同じである」という主張が帰無仮説として設定されることがあります。\n2. 対立仮説 (Alternative Hypothesis, H1) #対立仮説は、 帰無仮説に対立する主張であり、標本を通じて確実な根拠を持って証明しようとする仮説です。例えば、「二つのグループの平均は異なる」という主張が対立仮説として設定されることがあります。\n統計的検定を通じて与えられたデータを使用して帰無仮説を棄却するか、または棄却する根拠がないために帰無仮説を棄却できないと決定します。検定結果で対立仮説が真である確実な根拠を発見した場合、帰無仮説を棄却します。\n統計学で主張したいこのような仮説の妥当性を検証するプロセスがまさに仮説検定です。\n仮説検定 #1. 仮説の設定 #仮説検定の最初のステップは、調査したい問題に応じて帰無仮説(H0)と対立仮説(H1)を設定することです。\n2. 標本分析 #次に、全体母集団から一部を代表することができる標本を抽出します。この標本についてデータを収集し分析します。これにより、統計的分析に使用する資料を確保します。\n3. 仮説の妥当性検証 #収集したデータを使用して仮説を検証します。統計的手法を利用して帰無仮説を棄却するか、または棄却する根拠がないために帰無仮説を採用するかを決定します。これは有意水準と検定統計量を考慮して行われます。\n有意水準 (Level of Significance)\n有意水準は、主にα(alpha)で表され、実験または調査で帰無仮説を棄却する基準確率を示します。\n一般的に使用される有意水準は0.05(5%)ですが、実験の性質や研究の特性に応じて0.01や0.10など他の値を使用することがあります。\n検定統計量\n検定統計量は、収集したデータと仮説がどの程度一致するかを測る指標であり、母数推定をするために必要な標本統計量です。検定統計量は仮説検定で重要な役割を果たし、帰無仮説の棄却の有無を決定するために使用されます。\n仮説を検定する過程で、統計的な誤りが生じる可能性が常に存在し、これを仮説検定誤라고 합니다.\n統計的過誤 #1. 第一種過誤 #第一種過誤は、帰無仮説が真のときに帰無仮説を棄却する誤りを指します。第一種の誤りが発生する原因は、統計的検定で有意水準(significance level)を設定することにあり、この水準で帰無仮説を棄却するときに偶然に生じます。\n例) 実際には効果がないのに、効果があると誤って結論づける状況\n2. 第二種過誤 #第二種過誤は、対立仮説が真のときに帰無仮説を採用する誤りを指します。第二種の誤りが発生する原因は、検定力(power)が不足して実際に存在する効果を感知できないときに生じます。\n例) 実際には効果があるのに、統計的検定でその効果を見つけられずに帰無仮説を採用する状況\n","date":"2024年1月11日","permalink":"/ja/posts/6/","section":"ブログ投稿","summary":"\u003cp\u003eこの投稿は、pandasライブラリを活用して複数のデータを一つに統合する方法について説明するために作成されました。\u003c/p\u003e","title":"統計 - 仮説検定 (1)"},{"content":"","date":null,"permalink":"/ja/tags/concat/","section":"Tags","summary":"","title":"Concat"},{"content":"","date":null,"permalink":"/ja/tags/join/","section":"Tags","summary":"","title":"Join"},{"content":"","date":null,"permalink":"/ja/tags/merge/","section":"Tags","summary":"","title":"Merge"},{"content":"","date":null,"permalink":"/ja/tags/pandas/","section":"Tags","summary":"","title":"Pandas"},{"content":"この投稿は、pandasライブラリを活用して複数のデータを一つに統合する方法について説明するために作成されました。\nデータを統合する方法にはいくつかありますが、今回扱うのは concat, join, mergeです。\n説明する前に、例として2つのデータフレームを作成しましょう。\n\u0026gt;\u0026gt;\u0026gt; import pandas as pd \u0026gt;\u0026gt;\u0026gt; df1 = pd.DataFrame({ \u0026#39;Class1\u0026#39; : [95, 92, 98, 100], \u0026#39;Class2\u0026#39; : [91, 93, 97, 99] }) \u0026gt;\u0026gt;\u0026gt; df2 = pd.DataFrame({ \u0026#39;Class1\u0026#39; : [87, 89], \u0026#39;Class2\u0026#39; : [85, 90] }) d1の出力値:\nClass1 Class2 0 87 85 1 89 90 d2の出力値:\nClass1 Class2 0 95 91 1 92 93 2 98 97 3 100 99 1. concat #pandasライブラリの concat 関数はデータフレームを連結するために使用されます。この関数は複数のデータフレームを行または列方向に連結することができます。\ndf1とdf2をresultに連結しましょう。\n\u0026gt;\u0026gt;\u0026gt; result = pd.concat([df1, df2]) \u0026gt;\u0026gt;\u0026gt; result Class1 Class2 0 95 91.0 1 92 93.0 2 98 97.0 3 100 99.0 4 87 85.0 5 89 90.0 6 96 NaN 7 83 NaN **pd.concat([df1, df2])**は df1と df2を行の方向に連結します。つまり、2つのデータフレームが上下に接続されます。\n次に、Class1列のみを持つd3をresultに統合しましょう。\n\u0026gt;\u0026gt;\u0026gt; df3 = pd.DataFrame({ \u0026#39;Class1\u0026#39; : [96, 83] }) \u0026gt;\u0026gt;\u0026gt; pd.concat([result, df3], ignore_index=True) Class1 Class2 0 95 91.0 1 92 93.0 2 98 97.0 3 100 99.0 4 87 85.0 5 89 90.0 6 96 NaN 7 83 NaN d3データは\u0026rsquo;Class2\u0026rsquo;列を持っていないため、空白値が出力されます。\n2. join #pandasライブラリのj join メソッドは、2つのデータフレームを特定の列を基準に結合するために使用されます。一般的にSQLの JOIN 操作と同様の役割を果たします。 concatとは異なり、 joinは横方向に統合されます。\n\u0026gt;\u0026gt;\u0026gt; df4 = pd.DataFrame({ \u0026#39;Class3\u0026#39; : [93, 91, 95, 98] }) \u0026gt;\u0026gt;\u0026gt; df1.join(df4) Class1 Class2 Class3 a 95 91 93 b 92 93 91 c 98 97 95 d 100 99 98 次のようにインデックスを任意に設定して出力することも可能です。\n\u0026gt;\u0026gt;\u0026gt; index_label = [\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;d\u0026#39;] \u0026gt;\u0026gt;\u0026gt; df1a = pd.DataFrame({\u0026#39;Class1\u0026#39;: [95, 92, 98, 100], \u0026#39;Class2\u0026#39;: [91, 93, 97, 99]}, index= index_label) \u0026gt;\u0026gt;\u0026gt; df4a = pd.DataFrame({\u0026#39;Class3\u0026#39;: [93, 91, 95, 98]}, index=index_label) \u0026gt;\u0026gt;\u0026gt; df1a.join(df4a) Class1 Class2 Class3 a 95 91 93 b 92 93 91 c 98 97 95 d 100 99 98 3. merge #pandasライブラリのmerge関数は、2つのデータフレームを特定の列を基準にマージ（統合）するために使用されます。merge関数を使用すると、データフレーム間で共通の列を基準に結合することができます。\n\u0026gt;\u0026gt;\u0026gt; df_A_B = pd.DataFrame({\u0026#39;販売月\u0026#39;: [\u0026#39;1月\u0026#39;, \u0026#39;2月\u0026#39;, \u0026#39;3月\u0026#39;, \u0026#39;4月\u0026#39;], \u0026#39;製品A\u0026#39;: [100, 150, 200, 130], \u0026#39;製品B\u0026#39;: [90, 110, 140, 170]}) \u0026gt;\u0026gt;\u0026gt; df_C_D = pd.DataFrame({\u0026#39;販売月\u0026#39;: [\u0026#39;1月\u0026#39;, \u0026#39;2月\u0026#39;, \u0026#39;3月\u0026#39;, \u0026#39;4月\u0026#39;], \u0026#39;製品C\u0026#39;: [112, 141, 203, 134], \u0026#39;製品D\u0026#39;: [90, 110, 140, 170]}) df_A_B\n販売月 製品A 製品B 0 1月 100 90 1 2月 150 110 2 3月 200 140 3 4月 130 170 df_C_D\n販売月 製品C 製品D 0 1月 112 90 1 2月 141 110 2 3月 203 140 3 4月 134 170 mergeを使って　*\u0026lsquo;販売月\u0026rsquo;*列を基準に2つのデータフレームをマージします。結果として \u0026lsquo;販売月\u0026rsquo; 列を基準に2つのデータフレームが結合され、共通の列を中心にデータが整理されます。\n\u0026gt;\u0026gt;\u0026gt; df_A_B.merge(df_C_D) 販売月 製品A 製品B 製品C 製品D 0 1月 100 90 112 90 1 2月 150 110 141 110 2 3月 200 140 203 140 3 4月 130 170 134 170 mergeメソッドを使用して2つのデータフレームを結合する4つの異なる方法を実装してみましょう。\n\u0026gt;\u0026gt;\u0026gt; df_left = pd.DataFrame({\u0026#39;key\u0026#39;:[\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;C\u0026#39;], \u0026#39;left\u0026#39;: [1, 2, 3]}) \u0026gt;\u0026gt;\u0026gt; df_right = pd.DataFrame({\u0026#39;key\u0026#39;:[\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;D\u0026#39;], \u0026#39;right\u0026#39;: [4, 5, 6]}) 1.\n\u0026gt;\u0026gt;\u0026gt; df_left.merge(df_right, how=\u0026#39;left\u0026#39;, on = \u0026#39;key\u0026#39;) key left right 0 A 1 4.0 1 B 2 5.0 2 C 3 NaN \u0026lsquo;key\u0026rsquo;列を基準にdf_leftとdf_rightを左結合（left join）します。左結合は左データフレーム(df_left)の全ての行を保持し、右データフレーム(df_right)の該当するキー値がある行を追加します。該当するキー値が右データフレームにない場合はNaNで埋められます。\n2.\n\u0026gt;\u0026gt;\u0026gt; df_left.merge(df_right, how=\u0026#39;right\u0026#39;, on = \u0026#39;key\u0026#39;) key left right 0 A 1.0 4 1 B 2.0 5 2 D NaN 6 \u0026lsquo;key\u0026rsquo;列を基準にdf_leftとdf_rightを右結合（right join）します。右結合は右データフレーム(df_right)の全ての行を保持し、左データフレーム(df_left)の該当するキー値がある行を追加します。該当するキー値が左データフレームにない場合はNaNで埋められます。\n3.\n\u0026gt;\u0026gt;\u0026gt; df_left.merge(df_right, how=\u0026#39;outer\u0026#39;, on = \u0026#39;key\u0026#39;) key left right 0 A 1.0 4.0 1 B 2.0 5.0 2 D 3.0 NaN 3 D NaN 6.0 \u0026lsquo;key\u0026rsquo;列を基準にdf_leftとdf_rightを外部結合（outer join）します。外部結合は両方のデータフレームの全ての行を含み、一方のデータフレームにのみ該当する場合はNaNで埋められます。\n4.\n\u0026gt;\u0026gt;\u0026gt; df_left.merge(df_right, how=\u0026#39;inner\u0026#39;, on = \u0026#39;key\u0026#39;) key left right 0 A 1 4 1 B 2 5 \u0026lsquo;key\u0026rsquo;列を基準にdf_leftとdf_rightを内部結合（inner join）します。内部結合は両方のデータフレームに共通して存在する行のみを含みます。つまり、両方のデータフレームで同じ\u0026rsquo;key\u0026rsquo;値を持つ行を結合します。\n","date":"2024年1月8日","permalink":"/ja/posts/5/","section":"ブログ投稿","summary":"\u003cp\u003eこの投稿は、pandasライブラリを活用して複数のデータを一つに統合する方法について説明するために作成されました。\u003c/p\u003e","title":"Python - pandasデータ統合 (concat, join, merge)"},{"content":"","date":null,"permalink":"/ja/tags/matplotlib/","section":"Tags","summary":"","title":"Matplotlib"},{"content":"この記事は、Python内のmatplotlibライブラリを使用してグラフにテキストを追加する方法を説明するために作成されました。\nmatplotlibを活用してグラフを出力する際、以下のようにグラフ上にテキストを追加してみましょう。 まず、以下のように任意の月別売上数量のデータを持って実装しましょう。\n\u0026gt;\u0026gt;\u0026gt; import calendar \u0026gt;\u0026gt;\u0026gt; month_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] \u0026gt;\u0026gt;\u0026gt; sold_list = [300, 400, 550, 900, 600, 960, 900, 910, 800, 700, 550, 450] \u0026gt;\u0026gt;\u0026gt; fig, ax = plt.subplots() \u0026gt;\u0026gt;\u0026gt; barcharts = ax.bar(month_list, sold_list) # calendar.month_name[1:13] → xlabelに1月から12月まで出力 \u0026gt;\u0026gt;\u0026gt; ax.set_xticks(month_list, calendar.month_name[1:13], rotation=90) \u0026gt;\u0026gt;\u0026gt; print(barcharts) コードを実行すると、以下のようなグラフが出力されます。\n次に、各バー上に該当するy値を追加しましょう。\n各バーの値を得た後、テキストとして追加するためには、y値を出力するget_height()とバーにテキストを入力するax.text()を使用します。\n参照:\nget_height()\nhttps://matplotlib.org/stable/api/_as_gen/matplotlib.patches.Rectangle.html\nax.text()\nhttps://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.text.html#matplotlib.axes.Axes.text\n完成したコードは以下の通りです。\n\u0026gt;\u0026gt;\u0026gt; import calendar \u0026gt;\u0026gt;\u0026gt; month_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] \u0026gt;\u0026gt;\u0026gt; sold_list = [300, 400, 550, 900, 600, 960, 900, 910, 800, 700, 550, 450] \u0026gt;\u0026gt;\u0026gt; fig, ax = plt.subplots() \u0026gt;\u0026gt;\u0026gt; barcharts = ax.bar(month_list, sold_list) \u0026gt;\u0026gt;\u0026gt; ax.set_xticks(month_list, calendar.month_name[1:13], rotation=90) \u0026gt;\u0026gt;\u0026gt; print(barcharts) \u0026gt;\u0026gt;\u0026gt; for rect in barcharts: height = rect.get_height() ax.text(rect.get_x() + rect.get_width()/2., 1.002*height,\u0026#39;%d\u0026#39; % int(height), ha=\u0026#39;center\u0026#39;, va=\u0026#39;bottom\u0026#39;) \u0026gt;\u0026gt;\u0026gt; plt.show() rect.get_x() + rect.get_width()/2.\n各バーのX位置と幅の中間点を計算します。これはバーの横軸中心を表します。\n1.002 * height\nheightは現在のバーの高さであり、1.002*heightはバーの高さよりも少し上にテキストを配置するための補正値です。\n\u0026rsquo;% d\u0026rsquo; % int(height)\n文字列フォーマッティング、つまり、各バーごとの高さ値を追加（%の後に来るd(整数)を追加、int(height)はバーの高さを整数に変換）\nha=\u0026lsquo;center\u0026rsquo;\n水平（x軸）の配置を中央に合わせます。\nva=\u0026lsquo;bottom\u0026rsquo;\n垂直（y軸）の配置を下に合わせます。\nしたがって、コードを実行すると以下のように出力されます。 ","date":"2024年1月5日","permalink":"/ja/posts/4/","section":"ブログ投稿","summary":"\u003cp\u003eこの記事は、Python内のmatplotlibライブラリを使用してグラフにテキストを追加する方法を説明するために作成されました。\u003c/p\u003e","title":"Python - Matplotlib テキスト追加"},{"content":"","date":null,"permalink":"/ja/tags/date_range/","section":"Tags","summary":"","title":"Date_range"},{"content":"","date":null,"permalink":"/ja/tags/iloc/","section":"Tags","summary":"","title":"Iloc"},{"content":"","date":null,"permalink":"/ja/tags/loc/","section":"Tags","summary":"","title":"Loc"},{"content":"この投稿は、Pythonの pandas ライブラリを活用してDataFrameを扱う際に必要な .loc() と .iloc() のそれぞれの特徴と違いを説明するために作成されました。\nまず、説明のために seaborn を活用して例のデータ(iris)を取得しましょう。\n\u0026gt;\u0026gt;\u0026gt; import seaborn as sns \u0026gt;\u0026gt;\u0026gt; iris = sns.load_dataset(\u0026#39;iris\u0026#39;) \u0026gt;\u0026gt;\u0026gt; iris.head() irisデータの最初の5行 1. loc #locはラベル(Label)を基にデータを選択するメソッドです。行と列の名前を使用してデータにアクセスします。つまり、行と列の名前を明示的に指定してデータを選択します。\n# 列名が\u0026#39;species\u0026#39;で、その値が\u0026#39;virginica\u0026#39;であるデータを選択 \u0026gt;\u0026gt;\u0026gt; iris.loc[iris[\u0026#39;species\u0026#39;] == \u0026#39;virginica\u0026#39;] speciesの中で\u0026rsquo;virginica\u0026rsquo;の値を持つ最初の5行 2. iloc #ilocは整数(Integer)基準のインデックスを使用してデータを選択するメソッドです。行と列の整数の位置(インデックス)を利用してデータにアクセスします。つまり、データの位置を整数で明示して選択します。\n# 最初の行と2番目の列にあるデータを選択 \u0026gt;\u0026gt;\u0026gt; iris.iloc[0, 1] 3.5 最初の行と2番目の列にある\u0026rsquo;sepal_width\u0026rsquo;のデータ 3. locとilocの違い # インデックスのタイプ:\nlocはラベル(Label)を使用するため、行と列の名前が文字列や他のデータタイプである可能性があります。\nilocは整数(Integer)を使用するため、行と列のインデックスは整数である必要があります。\n使用方法:\nlocは明示的にラベルを使用してデータを選択することに重点を置きます。\nilocは整数位置(インデックス)を使用してデータを選択することに重点を置きます。\n例:\nlocの例: df.loc[\u0026lsquo;A\u0026rsquo;, \u0026lsquo;column_name\u0026rsquo;]\nilocの例: df.iloc[0, 1]\nどのメソッドを使用するかは、データフレームの構造やユーザーの目的によって異なります。 locはラベルが明確に定義されている場合に有用であり、 ilocは整数基準のインデックスが使用される場合に有用です。\n","date":"2024年1月4日","permalink":"/ja/posts/3/","section":"ブログ投稿","summary":"\u003cp\u003eこの投稿は、Pythonの \u003cem\u003e\u003cstrong\u003epandas\u003c/strong\u003e\u003c/em\u003e ライブラリを活用してDataFrameを扱う際に必要な \u003cstrong\u003e.loc()\u003c/strong\u003e と \u003cstrong\u003e.iloc()\u003c/strong\u003e のそれぞれの特徴と違いを説明するために作成されました。\u003c/p\u003e","title":"Python - pandas loc vs iloc"},{"content":"この投稿は、pandasライブラリ内で日付を自動的に生成できるdate_range()関数について説明するために作成されました。\nデータのインデックスに日付を一つ一つ入力する代わりに、pandasのdate_range()を活用すると、値が多い時に便利です。\ndate_range()は以下のように使用します。\n\u0026gt;\u0026gt;\u0026gt; pd.date_range(start=\u0026#39;日付\u0026#39;, end=\u0026#39;日付\u0026#39;, freq=\u0026#39;周期\u0026#39;) 例を挙げて説明します。\n\u0026gt;\u0026gt;\u0026gt; pd.date_range(start=\u0026#39;2024/01/01\u0026#39;, end=\u0026#39;2024/01/07\u0026#39;) DatetimeIndex([\u0026#39;2024-01-01\u0026#39;, \u0026#39;2024-01-02\u0026#39;, \u0026#39;2024-01-03\u0026#39;, \u0026#39;2024-01-04\u0026#39;, \u0026#39;2024-01-05\u0026#39;, \u0026#39;2024-01-06\u0026#39;, \u0026#39;2024-01-07\u0026#39;], dtype=\u0026#39;datetime64[ns]\u0026#39;, freq=\u0026#39;D\u0026#39;) 出力された結果として、開始日である'2024/01/01\u0026rsquo;から終了日である'2024/01/07\u0026rsquo;までが出力されたことが確認できます。\nもう一つの例を見てみましょう。\n\u0026gt;\u0026gt;\u0026gt; pd.date_range(start=\u0026#39;2024-01-01 08:00\u0026#39;, periods = 4, freq = \u0026#39;H\u0026#39;) DatetimeIndex([\u0026#39;2024-01-01 08:00:00\u0026#39;, \u0026#39;2024-01-01 09:00:00\u0026#39;, \u0026#39;2024-01-01 10:00:00\u0026#39;, \u0026#39;2024-01-01 11:00:00\u0026#39;], dtype=\u0026#39;datetime64[ns]\u0026#39;, freq=\u0026#39;H\u0026#39;) 結果を見ると、開始日である'2024-01-01\u0026rsquo;の08時から周期である\u0026rsquo;H\u0026rsquo;（時間単位）を基にした4つの結果が出たことが確認できます。\nfreq（周期）を設定する場合、以下のリンク内のOffset aliasesを参照すると、様々な形で出力が可能です。\n参照: https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases\n","date":"2024年1月4日","permalink":"/ja/posts/2/","section":"ブログ投稿","summary":"\u003cp\u003eこの投稿は、pandasライブラリ内で日付を自動的に生成できるdate_range()関数について説明するために作成されました。\u003c/p\u003e","title":"Python - pandasのdate_range"},{"content":"この投稿は、Pythonで値が連続しているデータタイプであるシーケンス(sequence types)について説明するために作成されました。\nシーケンスとは？ #シーケンス型(sequence types)とは、値が連続しているデータタイプを指します。 シーケンス型の最大の特徴は、共通の動作と機能を提供する点です。\nリスト [1, 2, 3, 4, 5] [1, 2, 3, 4, 5] タプル (1, 2, 3, 4, 5) (1, 2, 3, 4, 5) range range(5) 0, 1, 2, 3, 4 文字列 \u0026lsquo;Hello\u0026rsquo; H e l l o このように、シーケンスにはリスト、タプル、range、文字列があり、(bytes, bytearray)もこれに該当します。\nシーケンスで作成されたオブジェクトをシーケンスオブジェクトと呼び、各々の値を要素(element)と言います。\nシーケンスオブジェクト内の特定の値の確認 #シーケンスオブジェクト内に特定の値があるか確認するためには、以下のようにinまたはnot inを使用できます。\n\u0026gt;\u0026gt;\u0026gt; a = \u0026#34;Hello\u0026#34; \u0026gt;\u0026gt;\u0026gt; \u0026#34;H\u0026#34; in a True \u0026gt;\u0026gt;\u0026gt; \u0026#34;A\u0026#34; in a False # not in 特定の値がないか確認 \u0026gt;\u0026gt;\u0026gt; \u0026#34;ell\u0026#34; not in a False \u0026gt;\u0026gt;\u0026gt; \u0026#34;Python\u0026#34; not in a True in 演算子を使用した場合、特定の値があれば True、なければ Falseが返され、逆に not in 演算子を使用した場合、特定の値がなければ True、あれば Falseが返されます。\nシーケンスオブジェクトの連結 #シーケンスオブジェクトは + 演算子を使用して連結できます。\n\u0026gt;\u0026gt;\u0026gt; a = [0, 1, 2, 3] \u0026gt;\u0026gt;\u0026gt; b = [4, 5, 6] \u0026gt;\u0026gt;\u0026gt; a + b [0, 1, 2, 3, 4, 5, 6] ただし、 range는 + 演算子でオブジェクトを連結できません。\n\u0026gt;\u0026gt;\u0026gt; range(0, 5) + range(5, 10) TypeError Traceback (most recent call last) \u0026lt;ipython-input-7-88e74efcb3c0\u0026gt; in \u0026lt;cell line: 1\u0026gt;() ----\u0026gt; 1 range(0, 5) + range(5, 10) TypeError: unsupported operand type(s) for +: \u0026#39;range\u0026#39; and \u0026#39;range\u0026#39; したがって、rangeをタプルやリストに変換して連結することが可能です。\n\u0026gt;\u0026gt;\u0026gt; tuple(range(0, 5)) + tuple(range(5, 10)) (0, 1, 2, 3, 4, 5, 6, 7, 8, 9) \u0026gt;\u0026gt;\u0026gt; list(range(0, 5)) + list(range(5, 10)) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] シーケンスオブジェクトの繰り返し #シーケンスオブジェクトは*演算子を使用して繰り返すことができます。 整数 * シーケンスオブジェクトまたはシーケンス * 整数で繰り返し可能です。\n\u0026gt;\u0026gt;\u0026gt; [0, 1, 2, 3] * 3 [0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3] しかし、シーケンスオブジェクトの連結方法と同様に、rangesは * 演算子を使用して繰り返すことはできません。\n\u0026gt;\u0026gt;\u0026gt; range(0,10) * 3 TypeError Traceback (most recent call last) \u0026lt;ipython-input-11-824dcf3cff8f\u0026gt; in \u0026lt;cell line: 1\u0026gt;() ----\u0026gt; 1 range(0,10) * 3 TypeError: unsupported operand type(s) for *: \u0026#39;range\u0026#39; and \u0026#39;int\u0026#39; したがって、タプルやリストに変換して繰り返しは可能です。\nシーケンスオブジェクトの要素数の確認 #シーケンスオブジェクトの要素数は len 関数を使用して確認できます。\n# リスト \u0026gt;\u0026gt;\u0026gt; a = [1, 2, 3, 4, 5] \u0026gt;\u0026gt;\u0026gt; len(a) 5 # タプル \u0026gt;\u0026gt;\u0026gt; b = (6, 7, 8, 9, 10) \u0026gt;\u0026gt;\u0026gt; len(b) 5 # range len(range(0, 5, 2)) # -\u0026gt; 0から5まで2ずつ増やして 0, 2, 4 3 # 文字列 \u0026gt;\u0026gt;\u0026gt; c = \u0026#34;Hello, World\u0026#34; \u0026gt;\u0026gt;\u0026gt; len(c) 12 ","date":"2024年1月2日","permalink":"/ja/posts/1/","section":"ブログ投稿","summary":"\u003cp\u003eこの投稿は、Pythonで値が連続しているデータタイプであるシーケンス(sequence types)について説明するために作成されました。\u003c/p\u003e","title":"Python - シーケンス"},{"content":"","date":null,"permalink":"/ja/tags/sequence-types/","section":"Tags","summary":"","title":"Sequence Types"},{"content":"これは高度なタグです。Congoの他のリスティングページと同様に、個々のTaxonomy Termにカスタムコンテンツを追加することができ、Term Listの上部に表示されます。 🚀\nまた、これらのコンテンツページを使用して、SEOやその他の目的で使用されるタイトルや説明文などのHugoのメタデータを定義することもできます。\n","date":null,"permalink":"/ja/tags/advanced/","section":"Tags","summary":"これは高度なタグです。Congoの他のリスティングページと同様に、個々のTaxonomy Termにカスタムコンテンツを追加することができ、Term Listの上部に表示されます。 🚀","title":"advanced"},{"content":"","date":null,"permalink":"/ja/categories/","section":"Categories","summary":"","title":"Categories"}]